# Script dir = /Users/lmthang/nmt.matlab/scripts
cd /Users/lmthang/nmt.matlab/scripts/../code

## trainLSTM('', '', '', '', '', '', '', '../output/gradcheck', 'isGradCheck', 1, 'isResume', 0, 'feedInput', 1)
[?1h=
                                                                   < M A T L A B (R) >
                                                         Copyright 1984-2013 The MathWorks, Inc.
                                                            R2013a (8.1.0.604) 64-bit (maci64)
                                                                    February 15, 2013

 
To get started, type one of these: helpwin, helpdesk, or demo.
For product information, visit www.mathworks.com.
 

  Student License -- for use in conjunction with courses offered at a
  degree-granting institution.  Professional and commercial use prohibited.

## Bilingual setting
# Init LSTM parameters using dataType=double, initRange=0.1
  Model size = 104, individual sizes:  W_src{1}=32 W_tgt{1}=48 W_emb_src=8 W_emb_tgt=8 W_soft=8
# assert = 0
# attnFunc = 0
# attnOpt = 0
# batchSize = 10
# dataType = double
# debug = 0
# decode = 1
# dropout = 1
# epochFraction = 1
# feedInput = 1
# finetuneEpoch = 5
# finetuneRate = 0.5
# gpuDevice = 0
# initRange = 0.1
# isBi = 1
# isClip = 1
# isGradCheck = 1
# isProfile = 0
# isResume = 0
# isReverse = 0
# learningRate = 1
# loadModel = 
# logFreq = 10
# lstmOpt = 0
# lstmSize = 2
# maxGradNorm = 5
# maxLenRatio = 1.5
# maxSentLen = 7
# minLenRatio = 0.5
# numEpoches = 10
# numLayers = 1
# onlyCPU = 0
# outDir = ../output/gradcheck
# posWin = 1
# seed = 0
# shuffle = 1
# sortBatch = 1
# srcLang = 
# srcVocabFile = 
# testPrefix = 
# tgtLang = 
# tgtVocabFile = 
# trainPrefix = 
# validPrefix = 
# chunkSize = 12800
# baseIndex = 0
# clipForward = 50
# clipBackward = 1000
# nonlinear_gate_f = sigmoid
# nonlinear_gate_f_prime = sigmoidPrime
# nonlinear_f = tanh
# nonlinear_f_prime = tanhPrime
# beamSize = 12
# stackSize = 100
# unkPenalty = 0
# lengthReward = 0
# isGPU = 0
# batchId = 1
# maxRelDist = 2
# logId = 3
# srcSos = 3
# srcEos = 4
# srcVocabSize = 4
# nullPosId = 0
# tgtSos = 3
# tgtEos = 4
# tgtVocabSize = 4
# modelFile = ../output/gradcheck/model.mat
# modelRecentFile = ../output/gradcheck/modelRecent.mat
# softmaxSize = 2
# lr = 1
# epoch = 1
# bestCostValid = 100000
# testPerplexity = 100000
# curTestPerpWord = 100000
# startIter = 0
# iter = 0
# epochBatchCount = 0
# finetuneCount = 0
# modelSize = 104
  src input 1: y y x y
  src mask: 1  1  1  1  1
  tgt input 1: <t_sos> a b a
  tgt output 1: a b a <t_eos>
  tgt mask: 1  1  1  1
# W_src{1}, [8 4]
 -0.000013	 -0.000013	diff=1.00581e-10
 -0.000002	 -0.000002	diff=1.58468e-11
 -0.000010	 -0.000010	diff=1.1217e-09
 -0.000001	 -0.000001	diff=1.23089e-10
 -0.000000	 -0.000000	diff=1.04141e-12
  0.000000	  0.000000	diff=9.07881e-13
 -0.003613	 -0.003613	diff=8.10743e-08
  0.000713	  0.000713	diff=9.21717e-09
  0.000015	  0.000015	diff=1.37375e-10
  0.000003	  0.000003	diff=2.07755e-11
  0.000011	  0.000011	diff=1.43185e-09
  0.000001	  0.000001	diff=1.86931e-10
  0.000000	  0.000000	diff=2.33636e-12
 -0.000000	 -0.000000	diff=1.13417e-12
  0.004176	  0.004176	diff=1.0838e-07
 -0.000891	 -0.000891	diff=1.45165e-08
  0.000001	  0.000001	diff=3.86068e-12
  0.000000	  0.000000	diff=5.75087e-13
  0.000000	  0.000000	diff=3.83232e-12
  0.000000	  0.000000	diff=7.18741e-13
 -0.000000	 -0.000000	diff=3.80584e-13
 -0.000000	 -0.000000	diff=1.08735e-12
  0.000155	  0.000154	diff=2.82143e-07
 -0.000021	 -0.000021	diff=1.65579e-10
 -0.000000	 -0.000000	diff=1.37378e-13
 -0.000000	 -0.000000	diff=4.1579e-13
 -0.000000	 -0.000000	diff=8.26237e-13
 -0.000000	 -0.000000	diff=9.4915e-13
  0.000000	  0.000000	diff=2.77459e-13
  0.000000	  0.000000	diff=5.61059e-13
 -0.000120	 -0.000120	diff=1.68958e-09
  0.000016	  0.000016	diff=3.36779e-08
  local_diff=5.34021e-07
# W_tgt{1}, [8 6]
 -0.000023	 -0.000023	diff=8.22795e-11
  0.000002	  0.000002	diff=4.21865e-11
  0.000027	  0.000027	diff=1.70418e-09
 -0.000000	 -0.000000	diff=4.36351e-10
  0.000018	  0.000018	diff=8.6079e-11
  0.000001	  0.000001	diff=1.0676e-11
  0.019842	  0.019842	diff=1.67844e-07
 -0.001313	 -0.001313	diff=3.03045e-08
  0.000041	  0.000041	diff=2.24695e-11
  0.000002	  0.000002	diff=4.93694e-11
  0.000000	  0.000000	diff=1.03452e-08
  0.000005	  0.000005	diff=1.76185e-10
  0.000055	  0.000055	diff=2.0443e-11
  0.000005	  0.000005	diff=6.52866e-11
 -0.011633	 -0.011633	diff=3.68551e-07
 -0.001429	 -0.001429	diff=8.41353e-09
  0.000000	  0.000000	diff=3.61114e-12
 -0.000000	 -0.000000	diff=5.59428e-13
 -0.000001	 -0.000001	diff=1.66645e-12
  0.000000	  0.000000	diff=5.90792e-13
 -0.000000	 -0.000000	diff=5.34451e-13
 -0.000000	 -0.000000	diff=1.84839e-13
 -0.000167	 -0.000167	diff=8.2321e-07
 -0.000041	 -0.000041	diff=1.27712e-09
 -0.000000	 -0.000000	diff=3.83453e-13
  0.000000	  0.000000	diff=3.47029e-13
  0.000000	  0.000000	diff=2.38324e-13
 -0.000000	 -0.000000	diff=4.6071e-13
  0.000000	  0.000000	diff=2.05234e-13
  0.000000	  0.000000	diff=1.718e-13
  0.000055	  0.000055	diff=4.22464e-09
  0.000031	  0.000031	diff=2.84871e-08
  0.000001	  0.000001	diff=9.52438e-13
  0.000000	  0.000000	diff=3.51754e-13
  0.000000	  0.000000	diff=7.37492e-12
  0.000000	  0.000000	diff=3.04127e-13
  0.000002	  0.000002	diff=1.7391e-13
  0.000000	  0.000000	diff=5.4158e-13
  0.000041	  0.000042	diff=1.62139e-06
 -0.000089	 -0.000089	diff=2.78163e-09
 -0.000001	 -0.000001	diff=5.47438e-13
 -0.000000	 -0.000000	diff=4.30218e-13
 -0.000000	 -0.000000	diff=5.80738e-13
 -0.000000	 -0.000000	diff=4.95699e-13
 -0.000001	 -0.000001	diff=2.21791e-13
 -0.000000	 -0.000000	diff=8.2266e-13
 -0.000100	 -0.000100	diff=1.49232e-08
  0.000069	  0.000068	diff=1.43545e-08
  local_diff=3.09882e-06
# W_emb_src, [2 4]
 -0.001162	 -0.001162	diff=2.73404e-07
  0.001998	  0.001997	diff=7.75039e-07
 -0.001487	 -0.001487	diff=3.43119e-07
  0.002311	  0.002310	diff=8.76973e-07
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  local_diff=2.26853e-06
# W_emb_tgt, [2 4]
 -0.009062	 -0.009061	diff=8.53796e-07
 -0.003136	 -0.003135	diff=3.85396e-07
 -0.004123	 -0.004123	diff=8.56602e-07
 -0.001353	 -0.001353	diff=2.27634e-07
  0.004611	  0.004612	diff=4.17262e-07
  0.001362	  0.001362	diff=2.80845e-07
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  local_diff=3.02154e-06
# W_soft, [4 2]
 -0.014903	 -0.014903	diff=9.79672e-08
 -0.003292	 -0.003292	diff=9.79819e-08
  0.005365	  0.005365	diff=9.79731e-08
  0.012831	  0.012831	diff=9.79869e-08
  0.010052	  0.010052	diff=5.11384e-08
  0.001880	  0.001880	diff=5.11442e-08
 -0.001340	 -0.001340	diff=5.11415e-08
 -0.010593	 -0.010593	diff=5.11469e-08
  local_diff=5.9648e-07
# Num params=104, abs_diff=9.5194e-06
Elapsed time is 0.612559 seconds.
[?1l>
## trainLSTM('', '', '', '', '', '', '', '../output/gradcheck', 'isGradCheck', 1, 'isResume', 0, 'feedInput', 1, 'numLayers', 2)
[?1h=
                                                                   < M A T L A B (R) >
                                                         Copyright 1984-2013 The MathWorks, Inc.
                                                            R2013a (8.1.0.604) 64-bit (maci64)
                                                                    February 15, 2013

 
To get started, type one of these: helpwin, helpdesk, or demo.
For product information, visit www.mathworks.com.
 

  Student License -- for use in conjunction with courses offered at a
  degree-granting institution.  Professional and commercial use prohibited.

## Bilingual setting
# Init LSTM parameters using dataType=double, initRange=0.1
  Model size = 168, individual sizes:  W_src{1}=32 W_src{2}=32 W_tgt{1}=48 W_tgt{2}=32 W_emb_src=8 W_emb_tgt=8 W_soft=8
# assert = 0
# attnFunc = 0
# attnOpt = 0
# batchSize = 10
# dataType = double
# debug = 0
# decode = 1
# dropout = 1
# epochFraction = 1
# feedInput = 1
# finetuneEpoch = 5
# finetuneRate = 0.5
# gpuDevice = 0
# initRange = 0.1
# isBi = 1
# isClip = 1
# isGradCheck = 1
# isProfile = 0
# isResume = 0
# isReverse = 0
# learningRate = 1
# loadModel = 
# logFreq = 10
# lstmOpt = 0
# lstmSize = 2
# maxGradNorm = 5
# maxLenRatio = 1.5
# maxSentLen = 7
# minLenRatio = 0.5
# numEpoches = 10
# numLayers = 2
# onlyCPU = 0
# outDir = ../output/gradcheck
# posWin = 1
# seed = 0
# shuffle = 1
# sortBatch = 1
# srcLang = 
# srcVocabFile = 
# testPrefix = 
# tgtLang = 
# tgtVocabFile = 
# trainPrefix = 
# validPrefix = 
# chunkSize = 12800
# baseIndex = 0
# clipForward = 50
# clipBackward = 1000
# nonlinear_gate_f = sigmoid
# nonlinear_gate_f_prime = sigmoidPrime
# nonlinear_f = tanh
# nonlinear_f_prime = tanhPrime
# beamSize = 12
# stackSize = 100
# unkPenalty = 0
# lengthReward = 0
# isGPU = 0
# batchId = 1
# maxRelDist = 2
# logId = 3
# srcSos = 3
# srcEos = 4
# srcVocabSize = 4
# nullPosId = 0
# tgtSos = 3
# tgtEos = 4
# tgtVocabSize = 4
# modelFile = ../output/gradcheck/model.mat
# modelRecentFile = ../output/gradcheck/modelRecent.mat
# softmaxSize = 2
# lr = 1
# epoch = 1
# bestCostValid = 100000
# testPerplexity = 100000
# curTestPerpWord = 100000
# startIter = 0
# iter = 0
# epochBatchCount = 0
# finetuneCount = 0
# modelSize = 168
  src input 1: <s_sos> x y y
  src mask: 0  1  1  1  1
  tgt input 1: <t_sos> b b <t_eos> <t_eos>
  tgt output 1: b b <t_eos> <t_eos> <t_eos>
  tgt mask: 1  1  1  0  0
# W_src{1}, [8 4]
  0.000000	  0.000000	diff=2.37356e-13
  0.000000	  0.000000	diff=5.07596e-13
 -0.000000	 -0.000000	diff=3.89347e-13
  0.000000	  0.000000	diff=2.09694e-13
  0.000000	  0.000000	diff=6.7354e-13
 -0.000000	 -0.000000	diff=3.01608e-13
 -0.000098	 -0.000098	diff=1.71358e-10
 -0.000027	 -0.000027	diff=2.02507e-10
 -0.000000	 -0.000000	diff=4.03735e-13
  0.000000	  0.000000	diff=4.93459e-13
 -0.000000	 -0.000000	diff=1.05589e-12
  0.000000	  0.000000	diff=1.31381e-12
 -0.000000	 -0.000000	diff=5.29186e-13
 -0.000000	 -0.000000	diff=6.33429e-14
 -0.000071	 -0.000071	diff=2.73825e-10
 -0.000016	 -0.000016	diff=2.06476e-10
 -0.000000	 -0.000000	diff=3.1992e-13
  0.000000	  0.000000	diff=3.49978e-14
 -0.000000	 -0.000000	diff=3.31659e-13
  0.000000	  0.000000	diff=5.91489e-13
 -0.000000	 -0.000000	diff=7.90748e-13
 -0.000000	 -0.000000	diff=2.27064e-14
 -0.000000	 -0.000000	diff=1.57994e-10
 -0.000000	 -0.000000	diff=1.19818e-13
  0.000000	  0.000000	diff=3.78066e-14
 -0.000000	 -0.000000	diff=3.79243e-13
  0.000000	  0.000000	diff=3.76031e-13
 -0.000000	 -0.000000	diff=5.89753e-13
  0.000000	  0.000000	diff=5.50325e-13
  0.000000	  0.000000	diff=3.55559e-13
  0.000001	  0.000001	diff=2.3666e-12
  0.000000	  0.000000	diff=4.45165e-10
  local_diff=1.47037e-09
# W_src{2}, [8 4]
 -0.000000	 -0.000000	diff=7.25711e-14
 -0.000000	 -0.000000	diff=9.41711e-14
 -0.000000	 -0.000000	diff=2.72845e-13
 -0.000000	 -0.000000	diff=4.41692e-14
  0.000000	  0.000000	diff=4.86349e-13
 -0.000000	 -0.000000	diff=4.3558e-13
  0.000000	  0.000000	diff=8.2258e-13
 -0.000001	 -0.000001	diff=3.2864e-12
  0.000000	  0.000000	diff=3.94709e-14
  0.000000	  0.000000	diff=1.44032e-13
  0.000000	  0.000000	diff=2.51421e-13
  0.000000	  0.000000	diff=1.28913e-12
 -0.000000	 -0.000000	diff=4.95312e-13
  0.000000	  0.000000	diff=3.3129e-13
  0.000010	  0.000010	diff=7.19492e-13
 -0.000035	 -0.000035	diff=8.99509e-12
 -0.000000	 -0.000000	diff=4.46887e-13
 -0.000000	 -0.000000	diff=2.6235e-13
 -0.000000	 -0.000000	diff=5.15374e-13
 -0.000000	 -0.000000	diff=8.96287e-14
  0.000000	  0.000000	diff=1.77944e-13
  0.000000	 -0.000000	diff=1.78689e-13
 -0.000000	 -0.000000	diff=2.93359e-11
  0.000000	  0.000000	diff=3.58466e-14
  0.000000	  0.000000	diff=9.04837e-13
  0.000000	  0.000000	diff=9.27368e-13
  0.000000	  0.000000	diff=3.49249e-13
  0.000000	  0.000000	diff=3.77428e-13
  0.000000	 -0.000000	diff=2.36505e-13
  0.000000	  0.000000	diff=2.4404e-13
  0.000000	  0.000000	diff=2.02618e-13
 -0.000000	 -0.000000	diff=4.59638e-10
  local_diff=5.11703e-10
# W_tgt{1}, [8 6]
 -0.000000	 -0.000000	diff=3.35442e-13
  0.000000	  0.000000	diff=3.94848e-13
 -0.000000	 -0.000000	diff=8.35513e-13
  0.000000	  0.000000	diff=3.19567e-12
 -0.000000	 -0.000000	diff=3.12538e-13
  0.000000	  0.000000	diff=2.25089e-13
  0.000018	  0.000018	diff=1.56162e-10
 -0.000180	 -0.000180	diff=6.4791e-10
  0.000000	  0.000000	diff=5.97299e-13
 -0.000000	 -0.000000	diff=1.17176e-12
  0.000000	  0.000000	diff=7.39324e-13
 -0.000000	 -0.000000	diff=6.04085e-12
  0.000000	  0.000000	diff=3.74592e-13
 -0.000001	 -0.000001	diff=1.50315e-12
 -0.000019	 -0.000019	diff=5.46561e-10
  0.000267	  0.000267	diff=1.72464e-09
 -0.000000	 -0.000000	diff=7.72659e-13
  0.000000	  0.000000	diff=5.256e-13
  0.000000	  0.000000	diff=1.40037e-13
  0.000000	  0.000000	diff=6.0965e-13
 -0.000000	 -0.000000	diff=1.20597e-13
  0.000000	  0.000000	diff=1.34371e-13
 -0.000000	 -0.000000	diff=8.68219e-14
 -0.000000	 -0.000000	diff=3.16904e-13
 -0.000000	 -0.000000	diff=2.54767e-13
  0.000000	  0.000000	diff=1.13838e-13
  0.000000	  0.000000	diff=3.30865e-13
 -0.000000	 -0.000000	diff=3.9686e-13
 -0.000000	 -0.000000	diff=1.03145e-13
  0.000000	  0.000000	diff=4.09281e-13
 -0.000000	 -0.000000	diff=8.13412e-13
  0.000000	  0.000000	diff=2.02349e-12
  0.000000	  0.000000	diff=4.83992e-13
 -0.000000	 -0.000000	diff=3.21717e-13
  0.000000	  0.000000	diff=3.58882e-13
  0.000000	  0.000000	diff=1.14669e-13
  0.000000	  0.000000	diff=5.65705e-13
 -0.000000	 -0.000000	diff=8.01985e-14
  0.000000	  0.000000	diff=7.736e-10
  0.000000	  0.000000	diff=1.464e-11
  0.000000	  0.000000	diff=3.80087e-14
 -0.000000	 -0.000000	diff=3.63436e-13
 -0.000000	 -0.000000	diff=1.01133e-12
 -0.000000	 -0.000000	diff=7.74789e-14
  0.000000	  0.000000	diff=8.27628e-13
 -0.000000	 -0.000000	diff=5.33543e-13
  0.000001	  0.000001	diff=4.38372e-11
  0.000002	  0.000002	diff=3.02569e-09
  local_diff=6.96069e-09
# W_tgt{2}, [8 4]
  0.000000	  0.000000	diff=1.92292e-13
 -0.000000	 -0.000000	diff=6.25853e-14
 -0.000000	 -0.000000	diff=2.1148e-13
 -0.000000	 -0.000000	diff=5.64859e-13
  0.000000	  0.000000	diff=8.46261e-14
 -0.000000	 -0.000000	diff=5.17635e-13
  0.000012	  0.000012	diff=2.91316e-10
 -0.000062	 -0.000062	diff=4.91028e-11
  0.000000	  0.000000	diff=3.36282e-13
 -0.000000	 -0.000000	diff=6.32542e-13
  0.000000	  0.000000	diff=4.91868e-13
 -0.000000	 -0.000000	diff=1.5889e-13
  0.000000	  0.000000	diff=6.1663e-13
 -0.000000	 -0.000000	diff=1.73656e-14
  0.000019	  0.000019	diff=7.04199e-09
 -0.000193	 -0.000193	diff=7.89954e-10
  0.000000	  0.000000	diff=8.50074e-14
  0.000000	  0.000000	diff=3.42618e-13
 -0.000000	 -0.000000	diff=1.87641e-13
 -0.000000	 -0.000000	diff=6.48417e-14
 -0.000000	 -0.000000	diff=4.69917e-13
  0.000000	  0.000000	diff=1.03154e-12
  0.000000	  0.000000	diff=1.33143e-09
  0.000000	  0.000000	diff=2.00005e-11
  0.000000	  0.000000	diff=2.25047e-13
  0.000000	  0.000000	diff=1.90105e-13
  0.000000	  0.000000	diff=3.55687e-13
  0.000000	  0.000000	diff=4.70236e-13
  0.000000	  0.000000	diff=5.76478e-13
  0.000000	  0.000000	diff=3.9976e-13
  0.000002	  0.000002	diff=3.88208e-12
 -0.000000	 -0.000000	diff=4.67449e-09
  local_diff=1.42105e-08
# W_emb_src, [2 4]
  0.000077	  0.000077	diff=2.28501e-08
 -0.000079	 -0.000079	diff=6.2331e-08
  0.000060	  0.000060	diff=2.10513e-08
 -0.000053	 -0.000053	diff=5.22341e-08
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  local_diff=1.58466e-07
# W_emb_tgt, [2 4]
  0.000078	  0.000078	diff=1.34663e-08
  0.000007	  0.000007	diff=5.13426e-09
 -0.000015	 -0.000015	diff=2.08447e-08
  0.000013	  0.000013	diff=8.10949e-09
  0.000152	  0.000152	diff=3.17758e-08
 -0.000016	 -0.000016	diff=8.84772e-09
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  local_diff=8.81784e-08
# W_soft, [4 2]
 -0.000066	 -0.000066	diff=2.39247e-12
 -0.000008	 -0.000008	diff=2.44774e-12
  0.000061	  0.000061	diff=3.34885e-12
  0.000013	  0.000013	diff=2.46908e-12
 -0.000231	 -0.000231	diff=3.58424e-11
 -0.000009	 -0.000009	diff=3.57434e-11
  0.000264	  0.000264	diff=3.59311e-11
 -0.000024	 -0.000024	diff=3.53022e-11
  local_diff=1.53477e-10
# Num params=168, abs_diff=2.69951e-07
Elapsed time is 1.557497 seconds.
[?1l>
## trainLSTM('', '', '', '', '', '', '', '../output/gradcheck', 'isGradCheck', 1, 'isResume', 0, 'feedInput', 1, 'numLayers', 2, 'dropout', 0.8)
[?1h=
                                                                   < M A T L A B (R) >
                                                         Copyright 1984-2013 The MathWorks, Inc.
                                                            R2013a (8.1.0.604) 64-bit (maci64)
                                                                    February 15, 2013

 
To get started, type one of these: helpwin, helpdesk, or demo.
For product information, visit www.mathworks.com.
 

  Student License -- for use in conjunction with courses offered at a
  degree-granting institution.  Professional and commercial use prohibited.

## Bilingual setting
# Init LSTM parameters using dataType=double, initRange=0.1
  Model size = 168, individual sizes:  W_src{1}=32 W_src{2}=32 W_tgt{1}=48 W_tgt{2}=32 W_emb_src=8 W_emb_tgt=8 W_soft=8
# assert = 0
# attnFunc = 0
# attnOpt = 0
# batchSize = 10
# dataType = double
# debug = 0
# decode = 1
# dropout = 0.8
# epochFraction = 1
# feedInput = 1
# finetuneEpoch = 5
# finetuneRate = 0.5
# gpuDevice = 0
# initRange = 0.1
# isBi = 1
# isClip = 1
# isGradCheck = 1
# isProfile = 0
# isResume = 0
# isReverse = 0
# learningRate = 1
# loadModel = 
# logFreq = 10
# lstmOpt = 0
# lstmSize = 2
# maxGradNorm = 5
# maxLenRatio = 1.5
# maxSentLen = 7
# minLenRatio = 0.5
# numEpoches = 10
# numLayers = 2
# onlyCPU = 0
# outDir = ../output/gradcheck
# posWin = 1
# seed = 0
# shuffle = 1
# sortBatch = 1
# srcLang = 
# srcVocabFile = 
# testPrefix = 
# tgtLang = 
# tgtVocabFile = 
# trainPrefix = 
# validPrefix = 
# chunkSize = 12800
# baseIndex = 0
# clipForward = 50
# clipBackward = 1000
# nonlinear_gate_f = sigmoid
# nonlinear_gate_f_prime = sigmoidPrime
# nonlinear_f = tanh
# nonlinear_f_prime = tanhPrime
# beamSize = 12
# stackSize = 100
# unkPenalty = 0
# lengthReward = 0
# isGPU = 0
# batchId = 1
# maxRelDist = 2
# logId = 3
# srcSos = 3
# srcEos = 4
# srcVocabSize = 4
# nullPosId = 0
# tgtSos = 3
# tgtEos = 4
# tgtVocabSize = 4
# modelFile = ../output/gradcheck/model.mat
# modelRecentFile = ../output/gradcheck/modelRecent.mat
# softmaxSize = 2
# lr = 1
# epoch = 1
# bestCostValid = 100000
# testPerplexity = 100000
# curTestPerpWord = 100000
# startIter = 0
# iter = 0
# epochBatchCount = 0
# finetuneCount = 0
# modelSize = 168
  src input 1: <s_sos> x y y
  src mask: 0  1  1  1  1
  tgt input 1: <t_sos> b b <t_eos> <t_eos>
  tgt output 1: b b <t_eos> <t_eos> <t_eos>
  tgt mask: 1  1  1  0  0
# W_src{1}, [8 4]
  0.000000	  0.000000	diff=4.50022e-13
  0.000000	  0.000000	diff=3.70299e-13
  0.000000	  0.000000	diff=3.80875e-13
  0.000000	  0.000000	diff=3.99373e-14
  0.000000	  0.000000	diff=7.13939e-13
 -0.000000	 -0.000000	diff=4.4373e-13
 -0.000126	 -0.000126	diff=4.99498e-10
 -0.000035	 -0.000035	diff=2.83971e-10
 -0.000000	 -0.000000	diff=2.37622e-13
  0.000000	  0.000000	diff=6.06675e-13
 -0.000000	 -0.000000	diff=1.68961e-12
  0.000000	  0.000000	diff=1.35066e-12
 -0.000000	 -0.000000	diff=2.34582e-13
 -0.000000	 -0.000000	diff=5.55439e-13
 -0.000076	 -0.000076	diff=6.82937e-10
 -0.000023	 -0.000023	diff=2.56518e-10
 -0.000000	 -0.000000	diff=8.42783e-14
  0.000000	  0.000000	diff=1.12563e-13
 -0.000000	 -0.000000	diff=3.91041e-13
  0.000000	  0.000000	diff=4.03977e-13
 -0.000000	 -0.000000	diff=8.34396e-14
 -0.000000	 -0.000000	diff=6.422e-13
  0.000000	  0.000000	diff=4.11999e-10
 -0.000000	 -0.000000	diff=1.54629e-12
  0.000000	  0.000000	diff=2.90295e-13
 -0.000000	 -0.000000	diff=1.29958e-13
  0.000000	  0.000000	diff=5.31788e-13
 -0.000000	 -0.000000	diff=4.09935e-13
  0.000000	  0.000000	diff=6.56176e-13
  0.000000	  0.000000	diff=5.51095e-13
  0.000001	  0.000001	diff=3.57575e-12
  0.000000	  0.000000	diff=3.295e-10
  local_diff=2.4809e-09
# W_src{2}, [8 4]
 -0.000000	 -0.000000	diff=4.86688e-13
 -0.000000	 -0.000000	diff=4.25432e-14
 -0.000000	 -0.000000	diff=2.67191e-13
 -0.000000	 -0.000000	diff=2.14032e-13
  0.000000	  0.000000	diff=1.74025e-13
 -0.000000	 -0.000000	diff=1.94983e-13
  0.000002	  0.000002	diff=1.39909e-12
 -0.000010	 -0.000010	diff=5.38959e-12
  0.000000	  0.000000	diff=7.75021e-13
  0.000000	  0.000000	diff=9.37984e-13
  0.000000	  0.000000	diff=1.3982e-13
  0.000000	  0.000000	diff=5.72212e-13
 -0.000000	 -0.000000	diff=7.80079e-14
  0.000000	  0.000000	diff=6.22788e-13
  0.000013	  0.000013	diff=3.27014e-12
 -0.000037	 -0.000037	diff=1.3136e-11
 -0.000000	 -0.000000	diff=6.84717e-13
 -0.000000	 -0.000000	diff=6.95833e-13
 -0.000000	 -0.000000	diff=4.82485e-13
 -0.000000	 -0.000000	diff=4.3266e-13
  0.000000	  0.000000	diff=4.17671e-13
  0.000000	 -0.000000	diff=2.33146e-13
  0.000000	  0.000000	diff=1.56425e-10
  0.000000	  0.000000	diff=1.04659e-13
  0.000000	  0.000000	diff=1.3877e-13
  0.000000	  0.000000	diff=1.16207e-13
  0.000000	  0.000000	diff=3.19836e-13
  0.000000	  0.000000	diff=7.67281e-13
  0.000000	 -0.000000	diff=3.80023e-13
  0.000000	  0.000000	diff=3.94946e-13
  0.000000	  0.000000	diff=1.42127e-14
 -0.000000	 -0.000000	diff=1.61812e-10
  local_diff=3.5112e-10
# W_tgt{1}, [8 6]
 -0.000000	 -0.000000	diff=2.49779e-13
  0.000000	  0.000000	diff=2.91339e-13
 -0.000000	 -0.000000	diff=7.47188e-13
  0.000000	  0.000000	diff=5.57496e-12
 -0.000000	 -0.000000	diff=1.09904e-12
  0.000001	  0.000001	diff=2.31986e-13
  0.000034	  0.000034	diff=1.85303e-10
 -0.000223	 -0.000223	diff=1.48637e-09
  0.000000	  0.000000	diff=2.14712e-13
 -0.000001	 -0.000001	diff=9.90151e-13
  0.000000	  0.000000	diff=1.84494e-12
 -0.000000	 -0.000000	diff=7.11377e-12
  0.000000	  0.000000	diff=2.77917e-13
 -0.000001	 -0.000001	diff=7.97686e-13
 -0.000025	 -0.000025	diff=5.78503e-10
  0.000209	  0.000209	diff=2.1592e-09
  0.000000	  0.000000	diff=1.70135e-13
  0.000000	  0.000000	diff=1.27117e-13
  0.000000	  0.000000	diff=3.66828e-13
  0.000000	  0.000000	diff=3.02834e-13
  0.000000	  0.000000	diff=6.78474e-13
  0.000000	  0.000000	diff=3.8408e-14
 -0.000000	 -0.000000	diff=9.91711e-14
 -0.000000	 -0.000000	diff=6.37686e-13
  0.000000	  0.000000	diff=1.04146e-12
 -0.000000	 -0.000000	diff=7.84685e-13
  0.000000	  0.000000	diff=1.16384e-15
  0.000000	  0.000000	diff=1.88798e-13
  0.000000	  0.000000	diff=7.66524e-13
  0.000000	  0.000000	diff=6.70968e-14
 -0.000000	 -0.000000	diff=5.60973e-13
  0.000000	  0.000000	diff=3.1706e-12
 -0.000000	 -0.000000	diff=3.26674e-13
 -0.000000	 -0.000000	diff=2.13518e-14
 -0.000000	 -0.000000	diff=1.64235e-13
  0.000000	  0.000000	diff=2.23502e-13
 -0.000000	 -0.000000	diff=1.60861e-13
 -0.000000	 -0.000000	diff=9.96673e-14
  0.000000	  0.000000	diff=2.02144e-09
  0.000001	  0.000001	diff=2.67663e-11
  0.000000	  0.000000	diff=6.95348e-13
 -0.000000	 -0.000000	diff=2.99291e-13
 -0.000000	 -0.000000	diff=2.26233e-13
 -0.000000	 -0.000000	diff=9.57078e-14
  0.000000	  0.000000	diff=3.11986e-13
 -0.000000	 -0.000000	diff=1.14376e-13
  0.000000	  0.000000	diff=6.38723e-11
  0.000003	  0.000003	diff=2.472e-09
  local_diff=9.02462e-09
# W_tgt{2}, [8 4]
  0.000000	  0.000000	diff=3.49132e-13
 -0.000000	 -0.000000	diff=5.41798e-13
 -0.000000	 -0.000000	diff=9.85302e-13
 -0.000000	 -0.000000	diff=9.17212e-13
  0.000000	  0.000000	diff=1.53024e-13
 -0.000000	 -0.000000	diff=7.73917e-13
  0.000026	  0.000026	diff=1.01983e-09
 -0.000135	 -0.000135	diff=2.68099e-10
  0.000000	  0.000000	diff=4.87922e-13
 -0.000000	 -0.000000	diff=9.34949e-13
  0.000000	  0.000000	diff=4.47553e-13
 -0.000000	 -0.000000	diff=3.93564e-13
  0.000000	  0.000000	diff=4.40486e-13
 -0.000000	 -0.000000	diff=1.83329e-13
  0.000042	  0.000042	diff=1.11818e-08
 -0.000238	 -0.000238	diff=5.86472e-10
  0.000000	  0.000000	diff=1.94164e-13
  0.000000	  0.000000	diff=3.98178e-13
 -0.000000	 -0.000000	diff=2.66335e-13
 -0.000000	 -0.000000	diff=6.16989e-13
 -0.000000	 -0.000000	diff=1.29279e-13
  0.000000	  0.000000	diff=2.0376e-13
  0.000000	  0.000000	diff=1.40829e-09
  0.000000	  0.000000	diff=1.78205e-11
  0.000000	  0.000000	diff=3.05454e-13
  0.000000	  0.000000	diff=1.76299e-13
  0.000000	  0.000000	diff=9.44136e-13
  0.000000	  0.000000	diff=1.49885e-13
  0.000000	  0.000000	diff=2.66131e-13
  0.000000	  0.000000	diff=2.68875e-13
  0.000002	  0.000002	diff=6.76649e-12
  0.000001	  0.000001	diff=6.27277e-09
  local_diff=2.07724e-08
# W_emb_src, [2 4]
  0.000086	  0.000086	diff=3.56917e-08
 -0.000080	 -0.000080	diff=8.6878e-08
  0.000092	  0.000092	diff=4.07016e-08
 -0.000076	 -0.000076	diff=8.65711e-08
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  local_diff=2.49842e-07
# W_emb_tgt, [2 4]
  0.000114	  0.000114	diff=1.6979e-08
  0.000004	  0.000004	diff=5.50205e-09
 -0.000010	 -0.000010	diff=2.68157e-08
  0.000005	  0.000005	diff=3.08463e-09
  0.000168	  0.000168	diff=5.79362e-08
 -0.000017	 -0.000017	diff=1.19392e-08
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  local_diff=1.22257e-07
# W_soft, [4 2]
 -0.000104	 -0.000104	diff=9.31198e-12
 -0.000010	 -0.000010	diff=8.68877e-12
  0.000089	  0.000089	diff=8.54213e-12
  0.000025	  0.000025	diff=8.98425e-12
 -0.000339	 -0.000339	diff=6.57343e-11
  0.000031	  0.000031	diff=6.5345e-11
  0.000300	  0.000300	diff=6.59517e-11
  0.000008	  0.000008	diff=6.58698e-11
  local_diff=2.98428e-10
# Num params=168, abs_diff=4.05027e-07
Elapsed time is 1.332976 seconds.
[?1l>
## trainLSTM('', '', '', '', '', '', '', '../output/gradcheck', 'isGradCheck', 1, 'isResume', 0, 'feedInput', 1, 'numLayers', 2, 'dropout', 0.8, 'isReverse', 1, 'attnFunc', 1, 'attnOpt', 1)
[?1h=
                                                                   < M A T L A B (R) >
                                                         Copyright 1984-2013 The MathWorks, Inc.
                                                            R2013a (8.1.0.604) 64-bit (maci64)
                                                                    February 15, 2013

 
To get started, type one of these: helpwin, helpdesk, or demo.
For product information, visit www.mathworks.com.
 

  Student License -- for use in conjunction with courses offered at a
  degree-granting institution.  Professional and commercial use prohibited.

## Bilingual setting
# Init LSTM parameters using dataType=double, initRange=0.1
  Model size = 176, individual sizes:  W_src{1}=32 W_src{2}=32 W_tgt{1}=48 W_tgt{2}=32 W_emb_src=8 W_emb_tgt=8 W_h=8 W_soft=8
# assert = 0
# attnFunc = 1
# attnOpt = 1
# batchSize = 10
# dataType = double
# debug = 0
# decode = 1
# dropout = 0.8
# epochFraction = 1
# feedInput = 1
# finetuneEpoch = 5
# finetuneRate = 0.5
# gpuDevice = 0
# initRange = 0.1
# isBi = 1
# isClip = 1
# isGradCheck = 1
# isProfile = 0
# isResume = 0
# isReverse = 1
# learningRate = 1
# loadModel = 
# logFreq = 10
# lstmOpt = 0
# lstmSize = 2
# maxGradNorm = 5
# maxLenRatio = 1.5
# maxSentLen = 7
# minLenRatio = 0.5
# numEpoches = 10
# numLayers = 2
# onlyCPU = 0
# outDir = ../output/gradcheck
# posWin = 1
# seed = 0
# shuffle = 1
# sortBatch = 1
# srcLang = 
# srcVocabFile = 
# testPrefix = 
# tgtLang = 
# tgtVocabFile = 
# trainPrefix = 
# validPrefix = 
# chunkSize = 12800
# baseIndex = 0
# clipForward = 50
# clipBackward = 1000
# nonlinear_gate_f = sigmoid
# nonlinear_gate_f_prime = sigmoidPrime
# nonlinear_f = tanh
# nonlinear_f_prime = tanhPrime
# beamSize = 12
# stackSize = 100
# unkPenalty = 0
# lengthReward = 0
# isGPU = 0
# batchId = 1
# maxRelDist = 2
# logId = 3
# srcSos = 3
# srcEos = 4
# srcVocabSize = 4
# nullPosId = 0
# tgtSos = 3
# tgtEos = 4
# tgtVocabSize = 4
# modelFile = ../output/gradcheck/model.mat
# modelRecentFile = ../output/gradcheck/modelRecent.mat
# softmaxSize = 2
# lr = 1
# epoch = 1
# bestCostValid = 100000
# testPerplexity = 100000
# curTestPerpWord = 100000
# startIter = 0
# iter = 0
# epochBatchCount = 0
# finetuneCount = 0
# modelSize = 176
  src input 1: <s_sos> y x x
  src mask: 0  1  1  1  1
  tgt input 1: <t_sos> a b <t_eos> <t_eos>
  tgt output 1: a b <t_eos> <t_eos> <t_eos>
  tgt mask: 1  1  1  0  0
# W_src{1}, [8 4]
 -0.000000	 -0.000000	diff=1.5598e-12
  0.000000	  0.000000	diff=1.83591e-12
 -0.000000	 -0.000000	diff=5.19631e-13
  0.000000	  0.000000	diff=9.78332e-13
 -0.000000	 -0.000000	diff=1.01286e-12
 -0.000000	 -0.000000	diff=1.98518e-12
 -0.000005	 -0.000005	diff=1.19439e-09
 -0.000004	 -0.000004	diff=1.5982e-09
 -0.000000	 -0.000000	diff=1.23072e-12
  0.000000	  0.000000	diff=3.52436e-12
 -0.000000	 -0.000000	diff=2.23224e-13
  0.000000	  0.000000	diff=6.60179e-13
 -0.000000	 -0.000000	diff=3.34314e-13
 -0.000000	 -0.000000	diff=3.62964e-12
 -0.000004	 -0.000004	diff=6.36141e-10
 -0.000003	 -0.000003	diff=1.96696e-09
  0.000000	  0.000000	diff=2.68464e-13
  0.000000	  0.000000	diff=6.65299e-13
 -0.000000	 -0.000000	diff=5.50286e-13
  0.000000	  0.000000	diff=2.03013e-13
 -0.000000	 -0.000000	diff=4.00111e-13
 -0.000000	 -0.000000	diff=1.26701e-13
 -0.000000	 -0.000000	diff=6.74013e-11
 -0.000000	 -0.000000	diff=1.02954e-11
 -0.000000	 -0.000000	diff=4.62313e-14
 -0.000000	 -0.000000	diff=3.4469e-13
  0.000000	  0.000000	diff=6.23055e-13
 -0.000000	 -0.000000	diff=5.00093e-13
  0.000000	  0.000000	diff=8.35191e-13
  0.000000	  0.000000	diff=4.73165e-13
  0.000000	  0.000000	diff=7.51535e-12
  0.000000	  0.000000	diff=1.44645e-10
  local_diff=5.64809e-09
# W_src{2}, [8 4]
 -0.000000	 -0.000000	diff=5.09494e-13
 -0.000000	 -0.000000	diff=3.62773e-13
 -0.000000	 -0.000000	diff=9.54545e-13
 -0.000000	 -0.000000	diff=4.54457e-13
 -0.000000	 -0.000000	diff=7.61357e-13
  0.000000	  0.000000	diff=5.11057e-13
 -0.000000	 -0.000000	diff=3.03711e-09
  0.000000	  0.000000	diff=2.79204e-09
  0.000000	  0.000000	diff=1.2295e-13
  0.000000	  0.000000	diff=2.95302e-13
  0.000000	  0.000000	diff=7.18647e-14
  0.000000	  0.000000	diff=5.87087e-13
  0.000000	  0.000000	diff=3.95194e-13
 -0.000000	 -0.000000	diff=1.96515e-13
  0.000001	  0.000001	diff=6.62753e-09
 -0.000002	 -0.000002	diff=6.79838e-09
 -0.000000	 -0.000000	diff=6.73311e-13
 -0.000000	 -0.000000	diff=6.50061e-13
  0.000000	 -0.000000	diff=8.98412e-14
 -0.000000	 -0.000000	diff=1.01509e-13
 -0.000000	 -0.000000	diff=5.41988e-13
  0.000000	  0.000000	diff=9.72959e-13
 -0.000000	 -0.000000	diff=6.55241e-11
  0.000000	  0.000000	diff=6.27262e-11
 -0.000000	  0.000000	diff=7.59901e-13
  0.000000	  0.000000	diff=3.78778e-15
  0.000000	  0.000000	diff=1.23435e-13
  0.000000	  0.000000	diff=2.292e-14
  0.000000	  0.000000	diff=2.3267e-13
 -0.000000	 -0.000000	diff=7.85591e-13
  0.000000	  0.000000	diff=8.7776e-11
 -0.000000	 -0.000000	diff=1.8971e-11
  local_diff=1.95002e-08
# W_tgt{1}, [8 6]
 -0.000000	 -0.000000	diff=6.26856e-13
  0.000000	  0.000000	diff=3.32873e-13
 -0.000000	 -0.000000	diff=5.12257e-13
  0.000000	  0.000000	diff=4.05323e-13
 -0.000000	 -0.000000	diff=5.08563e-13
  0.000000	  0.000000	diff=3.03378e-13
  0.000001	  0.000001	diff=6.48329e-12
 -0.000007	 -0.000007	diff=6.10862e-11
  0.000000	  0.000000	diff=2.12075e-13
 -0.000000	 -0.000000	diff=4.1157e-13
 -0.000000	 -0.000000	diff=1.14227e-13
 -0.000000	 -0.000000	diff=4.62645e-14
  0.000000	  0.000000	diff=7.80676e-13
 -0.000000	 -0.000000	diff=1.7951e-14
 -0.000001	 -0.000001	diff=1.31194e-11
  0.000021	  0.000021	diff=1.45207e-10
  0.000000	  0.000000	diff=7.64479e-16
  0.000000	  0.000000	diff=4.51798e-14
  0.000000	 -0.000000	diff=2.67019e-15
  0.000000	  0.000000	diff=2.50182e-13
 -0.000000	 -0.000000	diff=7.09404e-13
  0.000000	  0.000000	diff=2.25508e-13
  0.000000	  0.000000	diff=5.73743e-13
 -0.000000	 -0.000000	diff=7.91033e-14
  0.000000	  0.000000	diff=5.01612e-15
  0.000000	 -0.000000	diff=9.69634e-14
 -0.000000	  0.000000	diff=7.12247e-13
 -0.000000	  0.000000	diff=1.05171e-12
 -0.000000	  0.000000	diff=7.14214e-13
  0.000000	  0.000000	diff=1.38291e-13
  0.000000	  0.000000	diff=7.39393e-13
 -0.000000	 -0.000000	diff=2.05944e-13
  0.000000	  0.000000	diff=1.49535e-13
  0.000000	  0.000000	diff=6.84271e-14
  0.000000	  0.000000	diff=2.28195e-13
  0.000000	  0.000000	diff=7.72973e-15
  0.000000	  0.000000	diff=1.08441e-12
  0.000000	  0.000000	diff=1.6758e-13
 -0.000000	 -0.000000	diff=1.24296e-11
 -0.000000	 -0.000000	diff=1.68175e-12
 -0.000000	 -0.000000	diff=8.56865e-13
 -0.000000	 -0.000000	diff=4.61473e-13
  0.000000	  0.000000	diff=3.89902e-13
 -0.000000	 -0.000000	diff=3.78733e-13
  0.000000	  0.000000	diff=4.05268e-13
 -0.000000	 -0.000000	diff=1.36078e-13
 -0.000000	 -0.000000	diff=1.18261e-12
  0.000000	  0.000000	diff=4.77178e-10
  local_diff=7.32525e-10
# W_tgt{2}, [8 4]
 -0.000000	 -0.000000	diff=6.1669e-15
 -0.000000	 -0.000000	diff=1.25728e-13
  0.000000	  0.000000	diff=3.48778e-13
 -0.000000	 -0.000000	diff=7.52668e-13
 -0.000000	 -0.000000	diff=1.18867e-12
 -0.000000	 -0.000000	diff=4.35997e-13
 -0.000000	 -0.000000	diff=3.05642e-13
 -0.000005	 -0.000005	diff=4.6174e-12
 -0.000000	 -0.000000	diff=2.79035e-13
 -0.000000	 -0.000000	diff=7.23714e-13
 -0.000000	 -0.000000	diff=8.89231e-15
  0.000000	  0.000000	diff=5.15174e-13
 -0.000000	 -0.000000	diff=1.47692e-13
 -0.000000	 -0.000000	diff=1.30966e-13
 -0.000000	 -0.000000	diff=6.67823e-13
 -0.000009	 -0.000009	diff=6.59024e-11
  0.000000	 -0.000000	diff=3.72394e-15
  0.000000	  0.000000	diff=6.41518e-13
  0.000000	  0.000000	diff=1.22134e-13
 -0.000000	 -0.000000	diff=2.84323e-13
  0.000000	  0.000000	diff=5.98169e-13
  0.000000	  0.000000	diff=2.3277e-13
  0.000000	  0.000000	diff=7.97911e-13
  0.000000	  0.000000	diff=1.46235e-12
 -0.000000	  0.000000	diff=7.47401e-13
 -0.000000	 -0.000000	diff=4.0464e-14
  0.000000	 -0.000000	diff=9.38321e-14
  0.000000	  0.000000	diff=1.71244e-13
  0.000000	 -0.000000	diff=7.984e-14
  0.000000	  0.000000	diff=5.2857e-13
 -0.000000	 -0.000000	diff=2.51705e-13
  0.000000	  0.000000	diff=1.79486e-10
  local_diff=2.61699e-10
# W_emb_src, [2 4]
  0.000004	  0.000004	diff=2.02172e-10
 -0.000001	 -0.000001	diff=3.042e-09
  0.000001	  0.000001	diff=2.83339e-10
  0.000001	  0.000001	diff=1.10861e-09
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  local_diff=4.63612e-09
# W_emb_tgt, [2 4]
  0.000002	  0.000002	diff=1.17525e-09
 -0.000001	 -0.000001	diff=3.12443e-10
  0.000005	  0.000005	diff=1.07371e-09
 -0.000001	 -0.000001	diff=3.14938e-10
  0.000008	  0.000008	diff=1.72009e-09
 -0.000001	 -0.000001	diff=3.94204e-10
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  local_diff=4.99063e-09
# W_h, [2 4]
  0.000005	  0.000005	diff=1.10838e-12
  0.000000	  0.000000	diff=4.51328e-14
 -0.000007	 -0.000007	diff=3.95799e-13
 -0.000001	 -0.000001	diff=9.31476e-13
  0.000001	  0.000001	diff=5.9363e-13
  0.000006	  0.000006	diff=2.50651e-12
  0.000010	  0.000010	diff=2.02189e-11
 -0.000001	 -0.000001	diff=9.41717e-12
  local_diff=3.5217e-11
# W_soft, [4 2]
  0.000000	  0.000000	diff=1.05994e-12
 -0.000007	 -0.000007	diff=1.15998e-13
  0.000008	  0.000008	diff=1.05759e-13
 -0.000001	 -0.000001	diff=1.27636e-13
  0.000000	  0.000000	diff=5.43184e-13
 -0.000008	 -0.000008	diff=4.9301e-13
  0.000010	  0.000010	diff=7.64531e-13
 -0.000001	 -0.000001	diff=3.30904e-13
  local_diff=3.54096e-12
# Num params=176, abs_diff=3.58081e-08
Elapsed time is 2.112437 seconds.
[?1l>
## trainLSTM('', '', '', '', '', '', '', '../output/gradcheck', 'isGradCheck', 1, 'isResume', 0, 'feedInput', 1, 'numLayers', 2, 'dropout', 0.8, 'isReverse', 1, 'attnFunc', 2, 'attnOpt', 1)
[?1h=
                                                                   < M A T L A B (R) >
                                                         Copyright 1984-2013 The MathWorks, Inc.
                                                            R2013a (8.1.0.604) 64-bit (maci64)
                                                                    February 15, 2013

 
To get started, type one of these: helpwin, helpdesk, or demo.
For product information, visit www.mathworks.com.
 

  Student License -- for use in conjunction with courses offered at a
  degree-granting institution.  Professional and commercial use prohibited.

## Bilingual setting
# Init LSTM parameters using dataType=double, initRange=0.1
  Model size = 176, individual sizes:  W_src{1}=32 W_src{2}=32 W_tgt{1}=48 W_tgt{2}=32 W_emb_src=8 W_emb_tgt=8 W_h=8 W_soft=8
# assert = 0
# attnFunc = 2
# attnOpt = 1
# batchSize = 10
# dataType = double
# debug = 0
# decode = 1
# dropout = 0.8
# epochFraction = 1
# feedInput = 1
# finetuneEpoch = 5
# finetuneRate = 0.5
# gpuDevice = 0
# initRange = 0.1
# isBi = 1
# isClip = 1
# isGradCheck = 1
# isProfile = 0
# isResume = 0
# isReverse = 1
# learningRate = 1
# loadModel = 
# logFreq = 10
# lstmOpt = 0
# lstmSize = 2
# maxGradNorm = 5
# maxLenRatio = 1.5
# maxSentLen = 7
# minLenRatio = 0.5
# numEpoches = 10
# numLayers = 2
# onlyCPU = 0
# outDir = ../output/gradcheck
# posWin = 1
# seed = 0
# shuffle = 1
# sortBatch = 1
# srcLang = 
# srcVocabFile = 
# testPrefix = 
# tgtLang = 
# tgtVocabFile = 
# trainPrefix = 
# validPrefix = 
# chunkSize = 12800
# baseIndex = 0
# clipForward = 50
# clipBackward = 1000
# nonlinear_gate_f = sigmoid
# nonlinear_gate_f_prime = sigmoidPrime
# nonlinear_f = tanh
# nonlinear_f_prime = tanhPrime
# beamSize = 12
# stackSize = 100
# unkPenalty = 0
# lengthReward = 0
# isGPU = 0
# batchId = 1
# maxRelDist = 2
# logId = 3
# srcSos = 3
# srcEos = 4
# srcVocabSize = 4
# nullPosId = 0
# tgtSos = 3
# tgtEos = 4
# tgtVocabSize = 4
# modelFile = ../output/gradcheck/model.mat
# modelRecentFile = ../output/gradcheck/modelRecent.mat
# softmaxSize = 2
# lr = 1
# epoch = 1
# bestCostValid = 100000
# testPerplexity = 100000
# curTestPerpWord = 100000
# startIter = 0
# iter = 0
# epochBatchCount = 0
# finetuneCount = 0
# modelSize = 176
  src input 1: <s_sos> y x x
  src mask: 0  1  1  1  1
  tgt input 1: <t_sos> a b <t_eos> <t_eos>
  tgt output 1: a b <t_eos> <t_eos> <t_eos>
  tgt mask: 1  1  1  0  0
# W_src{1}, [8 4]
 -0.000000	 -0.000000	diff=1.26853e-12
  0.000000	  0.000000	diff=1.89747e-12
 -0.000000	 -0.000000	diff=9.30036e-13
  0.000000	  0.000000	diff=5.72552e-13
 -0.000000	 -0.000000	diff=1.65705e-12
 -0.000000	 -0.000000	diff=1.15027e-12
 -0.000001	 -0.000001	diff=1.15985e-09
 -0.000004	 -0.000004	diff=1.59769e-09
 -0.000000	 -0.000000	diff=5.00179e-14
  0.000000	  0.000000	diff=3.25916e-12
 -0.000000	 -0.000000	diff=4.19068e-13
  0.000000	  0.000000	diff=3.81499e-13
 -0.000000	 -0.000000	diff=1.99585e-13
 -0.000000	 -0.000000	diff=3.7525e-12
 -0.000002	 -0.000002	diff=6.12934e-10
 -0.000004	 -0.000004	diff=1.96778e-09
  0.000000	  0.000000	diff=6.27668e-13
  0.000000	  0.000000	diff=9.79365e-13
 -0.000000	 -0.000000	diff=5.19258e-13
  0.000000	  0.000000	diff=2.5498e-14
 -0.000000	 -0.000000	diff=3.15993e-13
 -0.000000	 -0.000000	diff=5.64645e-13
 -0.000000	 -0.000000	diff=5.11436e-11
 -0.000000	 -0.000000	diff=9.69521e-12
 -0.000000	 -0.000000	diff=3.42563e-13
 -0.000000	 -0.000000	diff=5.31911e-13
  0.000000	  0.000000	diff=7.55015e-13
 -0.000000	 -0.000000	diff=3.75978e-13
  0.000000	  0.000000	diff=3.88145e-13
  0.000000	  0.000000	diff=1.83575e-13
  0.000000	  0.000000	diff=8.3749e-12
  0.000000	  0.000000	diff=1.58881e-10
  local_diff=5.58749e-09
# W_src{2}, [8 4]
 -0.000000	 -0.000000	diff=1.13403e-12
  0.000000	  0.000000	diff=5.22755e-13
 -0.000000	 -0.000000	diff=7.5553e-13
 -0.000000	 -0.000000	diff=1.79e-13
 -0.000000	 -0.000000	diff=1.75895e-12
  0.000000	  0.000000	diff=6.61097e-13
  0.000000	  0.000000	diff=3.03747e-09
  0.000001	  0.000001	diff=2.7929e-09
  0.000000	  0.000000	diff=4.27724e-14
  0.000000	  0.000000	diff=7.37492e-13
  0.000000	  0.000000	diff=2.41151e-13
  0.000000	  0.000000	diff=8.61245e-13
  0.000000	  0.000000	diff=8.20046e-14
 -0.000000	 -0.000000	diff=7.12318e-13
  0.000001	  0.000001	diff=6.62676e-09
 -0.000000	 -0.000000	diff=6.79828e-09
 -0.000000	 -0.000000	diff=5.3305e-13
 -0.000000	 -0.000000	diff=6.37735e-13
 -0.000000	 -0.000000	diff=1.12231e-12
 -0.000000	 -0.000000	diff=9.65508e-13
 -0.000000	 -0.000000	diff=1.01529e-12
  0.000000	  0.000000	diff=6.00752e-14
 -0.000000	 -0.000000	diff=7.6426e-11
  0.000000	  0.000000	diff=6.34664e-11
 -0.000000	  0.000000	diff=9.6121e-13
  0.000000	  0.000000	diff=4.2121e-13
  0.000000	  0.000000	diff=4.12541e-13
  0.000000	  0.000000	diff=1.99723e-13
  0.000000	  0.000000	diff=5.69055e-13
 -0.000000	 -0.000000	diff=6.49821e-13
  0.000000	  0.000000	diff=8.80403e-11
 -0.000000	 -0.000000	diff=2.32327e-11
  local_diff=1.95218e-08
# W_tgt{1}, [8 6]
 -0.000000	 -0.000000	diff=6.27344e-13
  0.000000	  0.000000	diff=3.45178e-13
 -0.000000	 -0.000000	diff=5.13035e-13
  0.000000	  0.000000	diff=4.03299e-13
 -0.000000	 -0.000000	diff=2.03007e-13
  0.000000	  0.000000	diff=1.12807e-12
  0.000001	  0.000001	diff=5.77612e-12
 -0.000007	 -0.000007	diff=6.1619e-11
  0.000000	  0.000000	diff=2.11837e-13
 -0.000000	 -0.000000	diff=3.17509e-13
 -0.000000	 -0.000000	diff=8.23338e-13
 -0.000000	 -0.000000	diff=5.76446e-14
  0.000000	  0.000000	diff=7.79159e-13
 -0.000000	 -0.000000	diff=1.35227e-12
 -0.000001	 -0.000001	diff=1.31202e-11
  0.000021	  0.000021	diff=1.45809e-10
  0.000000	  0.000000	diff=1.33583e-15
  0.000000	  0.000000	diff=5.01089e-14
  0.000000	 -0.000000	diff=1.83561e-15
  0.000000	  0.000000	diff=2.60209e-13
 -0.000000	 -0.000000	diff=7.10424e-13
  0.000000	  0.000000	diff=2.37335e-13
  0.000000	  0.000000	diff=7.26867e-13
 -0.000000	 -0.000000	diff=2.71566e-13
  0.000000	  0.000000	diff=5.80086e-15
  0.000000	 -0.000000	diff=9.12846e-14
  0.000000	  0.000000	diff=3.23838e-15
  0.000000	  0.000000	diff=3.32946e-13
  0.000000	  0.000000	diff=4.94568e-15
  0.000000	  0.000000	diff=1.74542e-13
  0.000000	  0.000000	diff=1.32655e-12
 -0.000000	 -0.000000	diff=4.30784e-13
  0.000000	  0.000000	diff=8.60046e-13
  0.000000	  0.000000	diff=6.73768e-14
  0.000000	  0.000000	diff=1.64926e-12
  0.000000	  0.000000	diff=7.03402e-13
  0.000000	  0.000000	diff=3.73832e-13
  0.000000	  0.000000	diff=5.43852e-13
 -0.000000	 -0.000000	diff=1.24347e-11
 -0.000000	 -0.000000	diff=1.45079e-12
 -0.000000	 -0.000000	diff=1.46289e-13
 -0.000000	 -0.000000	diff=4.5974e-13
  0.000000	  0.000000	diff=3.89856e-13
 -0.000000	 -0.000000	diff=1.04191e-12
  0.000000	  0.000000	diff=1.11576e-12
 -0.000000	 -0.000000	diff=8.44952e-13
 -0.000000	 -0.000000	diff=1.91115e-12
  0.000000	  0.000000	diff=4.75661e-10
  local_diff=7.37369e-10
# W_tgt{2}, [8 4]
 -0.000000	 -0.000000	diff=1.41493e-12
 -0.000000	 -0.000000	diff=1.29582e-12
  0.000000	  0.000000	diff=1.05933e-12
 -0.000000	 -0.000000	diff=4.21304e-14
 -0.000000	 -0.000000	diff=1.18868e-12
 -0.000000	 -0.000000	diff=1.14641e-12
 -0.000000	 -0.000000	diff=7.20908e-13
 -0.000005	 -0.000005	diff=5.25477e-12
 -0.000000	 -0.000000	diff=4.31499e-13
 -0.000000	 -0.000000	diff=1.06529e-14
 -0.000000	 -0.000000	diff=7.19436e-13
  0.000000	  0.000000	diff=5.14255e-13
 -0.000000	 -0.000000	diff=1.47678e-13
 -0.000000	 -0.000000	diff=1.2811e-13
 -0.000000	 -0.000000	diff=9.6039e-13
 -0.000009	 -0.000009	diff=6.60416e-11
 -0.000000	 -0.000000	diff=7.06819e-13
  0.000000	  0.000000	diff=6.41519e-13
  0.000000	  0.000000	diff=1.22134e-13
 -0.000000	 -0.000000	diff=9.9486e-13
 -0.000000	  0.000000	diff=8.22917e-13
  0.000000	  0.000000	diff=4.77771e-13
  0.000000	  0.000000	diff=1.50875e-12
  0.000000	  0.000000	diff=2.23206e-12
 -0.000000	  0.000000	diff=7.47401e-13
 -0.000000	 -0.000000	diff=7.51045e-13
 -0.000000	 -0.000000	diff=6.1671e-13
  0.000000	  0.000000	diff=8.81805e-13
  0.000000	 -0.000000	diff=7.98399e-14
  0.000000	  0.000000	diff=1.81927e-13
 -0.000000	 -0.000000	diff=9.83274e-13
  0.000000	  0.000000	diff=1.7868e-10
  local_diff=2.71506e-10
# W_emb_src, [2 4]
  0.000001	  0.000001	diff=1.4071e-09
  0.000003	  0.000003	diff=6.37005e-09
 -0.000001	 -0.000001	diff=1.41613e-09
  0.000002	  0.000002	diff=2.13325e-09
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  local_diff=1.13265e-08
# W_emb_tgt, [2 4]
  0.000002	  0.000002	diff=1.17511e-09
 -0.000001	 -0.000001	diff=3.12484e-10
  0.000005	  0.000005	diff=1.0729e-09
 -0.000001	 -0.000001	diff=3.14212e-10
  0.000008	  0.000008	diff=1.71919e-09
 -0.000001	 -0.000001	diff=3.95614e-10
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  local_diff=4.9895e-09
# W_h, [2 4]
  0.000006	  0.000006	diff=6.8639e-13
 -0.000001	 -0.000001	diff=4.76547e-13
 -0.000008	 -0.000008	diff=6.12672e-13
 -0.000000	 -0.000000	diff=9.23256e-13
  0.000001	  0.000001	diff=7.98344e-13
  0.000006	  0.000006	diff=4.21413e-12
  0.000010	  0.000010	diff=1.98399e-11
 -0.000001	 -0.000001	diff=8.65859e-12
  local_diff=3.62098e-11
# W_soft, [4 2]
  0.000000	  0.000000	diff=2.61591e-13
 -0.000008	 -0.000008	diff=2.15846e-13
  0.000008	  0.000008	diff=2.56495e-14
 -0.000001	 -0.000001	diff=6.39148e-13
  0.000000	  0.000000	diff=8.53785e-13
 -0.000009	 -0.000009	diff=5.09233e-13
  0.000010	  0.000010	diff=1.13551e-12
 -0.000001	 -0.000001	diff=3.43641e-13
  local_diff=3.9844e-12
# Num params=176, abs_diff=4.24744e-08
Elapsed time is 2.751424 seconds.
[?1l>
## trainLSTM('', '', '', '', '', '', '', '../output/gradcheck', 'isGradCheck', 1, 'isResume', 0, 'feedInput', 1, 'numLayers', 2, 'dropout', 0.8, 'isReverse', 1, 'attnFunc', 4, 'attnOpt', 1)
[?1h=
                                                                   < M A T L A B (R) >
                                                         Copyright 1984-2013 The MathWorks, Inc.
                                                            R2013a (8.1.0.604) 64-bit (maci64)
                                                                    February 15, 2013

 
To get started, type one of these: helpwin, helpdesk, or demo.
For product information, visit www.mathworks.com.
 

  Student License -- for use in conjunction with courses offered at a
  degree-granting institution.  Professional and commercial use prohibited.

## Bilingual setting
# Init LSTM parameters using dataType=double, initRange=0.1
  Model size = 182, individual sizes:  W_src{1}=32 W_src{2}=32 W_tgt{1}=48 W_tgt{2}=32 W_emb_src=8 W_emb_tgt=8 W_pos=4 v_pos=2 W_h=8 W_soft=8
# assert = 0
# attnFunc = 4
# attnOpt = 1
# batchSize = 10
# dataType = double
# debug = 0
# decode = 1
# dropout = 0.8
# epochFraction = 1
# feedInput = 1
# finetuneEpoch = 5
# finetuneRate = 0.5
# gpuDevice = 0
# initRange = 0.1
# isBi = 1
# isClip = 1
# isGradCheck = 1
# isProfile = 0
# isResume = 0
# isReverse = 1
# learningRate = 1
# loadModel = 
# logFreq = 10
# lstmOpt = 0
# lstmSize = 2
# maxGradNorm = 5
# maxLenRatio = 1.5
# maxSentLen = 7
# minLenRatio = 0.5
# numEpoches = 10
# numLayers = 2
# onlyCPU = 0
# outDir = ../output/gradcheck
# posWin = 1
# seed = 0
# shuffle = 1
# sortBatch = 1
# srcLang = 
# srcVocabFile = 
# testPrefix = 
# tgtLang = 
# tgtVocabFile = 
# trainPrefix = 
# validPrefix = 
# chunkSize = 12800
# baseIndex = 0
# clipForward = 50
# clipBackward = 1000
# nonlinear_gate_f = sigmoid
# nonlinear_gate_f_prime = sigmoidPrime
# nonlinear_f = tanh
# nonlinear_f_prime = tanhPrime
# beamSize = 12
# stackSize = 100
# unkPenalty = 0
# lengthReward = 0
# isGPU = 0
# batchId = 1
# maxRelDist = 2
# distSigma = 0.5
# logId = 3
# srcSos = 3
# srcEos = 4
# srcVocabSize = 4
# nullPosId = 0
# tgtSos = 3
# tgtEos = 4
# tgtVocabSize = 4
# modelFile = ../output/gradcheck/model.mat
# modelRecentFile = ../output/gradcheck/modelRecent.mat
# softmaxSize = 2
# lr = 1
# epoch = 1
# bestCostValid = 100000
# testPerplexity = 100000
# curTestPerpWord = 100000
# startIter = 0
# iter = 0
# epochBatchCount = 0
# finetuneCount = 0
# modelSize = 182
  src input 1: x x x x
  src mask: 1  1  1  1  1
  tgt input 1: <t_sos> a b <t_eos> <t_eos>
  tgt output 1: a b <t_eos> <t_eos> <t_eos>
  tgt mask: 1  1  1  0  0
# W_src{1}, [8 4]
  0.000000	  0.000000	diff=1.55877e-12
  0.000000	  0.000000	diff=1.30842e-13
  0.000000	  0.000000	diff=8.16017e-13
  0.000000	  0.000000	diff=3.67257e-13
  0.000000	  0.000000	diff=6.7008e-13
 -0.000000	 -0.000000	diff=4.95035e-13
 -0.000004	 -0.000004	diff=8.07875e-10
 -0.000003	 -0.000003	diff=6.79952e-10
 -0.000000	 -0.000000	diff=1.76472e-12
  0.000000	  0.000000	diff=2.07146e-12
 -0.000000	 -0.000000	diff=1.16352e-12
  0.000000	  0.000000	diff=5.91519e-13
 -0.000000	 -0.000000	diff=1.99662e-12
 -0.000000	 -0.000000	diff=1.81344e-12
 -0.000001	 -0.000001	diff=9.24075e-10
 -0.000002	 -0.000002	diff=1.19874e-09
 -0.000000	 -0.000000	diff=1.52759e-12
  0.000000	  0.000000	diff=3.95232e-13
 -0.000000	 -0.000000	diff=1.20979e-12
  0.000000	  0.000000	diff=1.11181e-12
 -0.000000	 -0.000000	diff=4.56225e-13
 -0.000000	 -0.000000	diff=1.41829e-12
  0.000000	  0.000000	diff=6.44703e-11
 -0.000000	 -0.000000	diff=4.02894e-12
  0.000000	  0.000000	diff=7.99479e-13
 -0.000000	 -0.000000	diff=7.71153e-13
  0.000000	  0.000000	diff=2.73521e-13
 -0.000000	 -0.000000	diff=1.17905e-12
  0.000000	  0.000000	diff=7.683e-13
  0.000000	  0.000000	diff=4.45192e-13
  0.000000	  0.000000	diff=8.64027e-13
  0.000000	  0.000000	diff=6.60092e-11
  local_diff=3.76981e-09
# W_src{2}, [8 4]
 -0.000000	 -0.000000	diff=2.27466e-13
 -0.000000	 -0.000000	diff=4.10123e-13
 -0.000000	 -0.000000	diff=6.31193e-13
 -0.000000	 -0.000000	diff=9.30301e-13
  0.000000	  0.000000	diff=4.87188e-13
  0.000000	  0.000000	diff=6.31753e-13
  0.000000	  0.000000	diff=1.87511e-10
 -0.000001	 -0.000001	diff=3.95452e-10
  0.000000	  0.000000	diff=4.21131e-13
  0.000000	  0.000000	diff=1.42536e-12
  0.000000	  0.000000	diff=1.04499e-12
  0.000000	  0.000000	diff=8.58656e-13
 -0.000000	 -0.000000	diff=1.16969e-12
 -0.000000	 -0.000000	diff=1.05228e-12
  0.000002	  0.000002	diff=2.70512e-09
 -0.000001	 -0.000001	diff=3.74959e-09
 -0.000000	 -0.000000	diff=9.28083e-13
 -0.000000	 -0.000000	diff=3.93373e-14
 -0.000000	 -0.000000	diff=1.32132e-12
 -0.000000	 -0.000000	diff=7.21647e-13
  0.000000	  0.000000	diff=2.82533e-13
  0.000000	 -0.000000	diff=3.67119e-15
  0.000000	  0.000000	diff=4.28808e-11
 -0.000000	 -0.000000	diff=9.26046e-12
  0.000000	  0.000000	diff=6.18887e-13
  0.000000	  0.000000	diff=4.2833e-13
 -0.000000	  0.000000	diff=7.94523e-13
  0.000000	  0.000000	diff=9.11792e-13
 -0.000000	 -0.000000	diff=1.07298e-12
 -0.000000	  0.000000	diff=7.11634e-13
 -0.000000	 -0.000000	diff=1.07062e-11
 -0.000000	 -0.000000	diff=1.75297e-11
  local_diff=7.13517e-09
# W_tgt{1}, [8 6]
 -0.000000	 -0.000000	diff=1.30361e-12
  0.000000	  0.000000	diff=3.03324e-13
  0.000000	  0.000000	diff=9.3205e-13
  0.000000	  0.000000	diff=2.755e-13
  0.000000	  0.000000	diff=1.38828e-12
  0.000000	  0.000000	diff=4.41583e-13
 -0.000001	 -0.000001	diff=1.61674e-11
 -0.000020	 -0.000020	diff=1.03957e-10
 -0.000000	 -0.000000	diff=5.66988e-14
 -0.000000	 -0.000000	diff=7.00392e-13
 -0.000000	 -0.000000	diff=6.89462e-13
 -0.000000	 -0.000000	diff=2.24619e-12
 -0.000000	 -0.000000	diff=7.65289e-13
 -0.000000	 -0.000000	diff=8.27268e-13
  0.000001	  0.000001	diff=4.89857e-11
  0.000037	  0.000037	diff=2.54768e-10
  0.000000	 -0.000000	diff=9.3274e-17
 -0.000000	  0.000000	diff=7.23516e-13
  0.000000	 -0.000000	diff=2.13714e-14
 -0.000000	 -0.000000	diff=5.81188e-13
  0.000000	 -0.000000	diff=1.41293e-14
  0.000000	 -0.000000	diff=9.54344e-14
  0.000000	  0.000000	diff=1.94971e-13
  0.000000	  0.000000	diff=2.1505e-13
  0.000000	  0.000000	diff=2.75877e-14
  0.000000	  0.000000	diff=2.32869e-14
  0.000000	  0.000000	diff=4.40847e-14
  0.000000	  0.000000	diff=7.68421e-13
  0.000000	  0.000000	diff=6.59804e-14
  0.000000	  0.000000	diff=4.56802e-13
 -0.000000	 -0.000000	diff=2.96472e-13
 -0.000000	 -0.000000	diff=4.42676e-14
 -0.000000	 -0.000000	diff=1.17561e-12
  0.000000	  0.000000	diff=1.34363e-12
  0.000000	  0.000000	diff=4.2752e-13
 -0.000000	 -0.000000	diff=5.8053e-13
 -0.000000	 -0.000000	diff=8.09007e-14
  0.000000	  0.000000	diff=9.38381e-13
 -0.000000	 -0.000000	diff=3.27266e-11
  0.000000	  0.000000	diff=4.58269e-12
 -0.000000	 -0.000000	diff=1.08913e-12
 -0.000000	 -0.000000	diff=5.72859e-13
 -0.000000	 -0.000000	diff=1.51546e-12
 -0.000000	 -0.000000	diff=1.73423e-13
 -0.000000	 -0.000000	diff=8.24609e-13
 -0.000000	 -0.000000	diff=5.98038e-13
 -0.000000	 -0.000000	diff=1.03233e-12
  0.000001	  0.000001	diff=9.17244e-10
  local_diff=1.40229e-09
# W_tgt{2}, [8 4]
 -0.000000	 -0.000000	diff=3.7589e-13
 -0.000000	 -0.000000	diff=8.17744e-13
 -0.000000	 -0.000000	diff=7.42787e-13
 -0.000000	 -0.000000	diff=3.31013e-13
 -0.000000	 -0.000000	diff=1.88675e-13
 -0.000000	 -0.000000	diff=4.34368e-13
 -0.000000	 -0.000000	diff=8.063e-13
 -0.000001	 -0.000001	diff=2.9415e-12
  0.000000	  0.000000	diff=1.28743e-13
 -0.000000	 -0.000000	diff=3.80849e-13
  0.000000	  0.000000	diff=9.82502e-13
 -0.000000	 -0.000000	diff=6.55045e-13
  0.000000	  0.000000	diff=5.64755e-13
 -0.000000	 -0.000000	diff=1.22611e-12
  0.000009	  0.000009	diff=3.93889e-11
 -0.000022	 -0.000022	diff=1.35311e-10
 -0.000000	  0.000000	diff=1.43671e-12
  0.000000	  0.000000	diff=7.00665e-13
 -0.000000	 -0.000000	diff=1.00901e-12
 -0.000000	 -0.000000	diff=9.81917e-13
 -0.000000	 -0.000000	diff=7.74495e-13
  0.000000	  0.000000	diff=1.21975e-12
  0.000000	  0.000000	diff=7.06914e-11
  0.000000	  0.000000	diff=7.77717e-13
  0.000000	  0.000000	diff=2.55365e-13
  0.000000	  0.000000	diff=2.10134e-13
  0.000000	  0.000000	diff=4.26597e-13
  0.000000	  0.000000	diff=3.50347e-13
  0.000000	  0.000000	diff=1.82186e-13
  0.000000	  0.000000	diff=1.10487e-13
 -0.000000	 -0.000000	diff=5.07284e-13
  0.000000	  0.000000	diff=6.54767e-10
  local_diff=9.19678e-10
# W_emb_src, [2 4]
  0.000003	  0.000003	diff=1.44443e-09
  0.000002	  0.000002	diff=2.40503e-09
  0.000002	  0.000002	diff=8.64159e-10
  0.000004	  0.000004	diff=2.70527e-09
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  local_diff=7.4189e-09
# W_emb_tgt, [2 4]
  0.000020	  0.000020	diff=3.65399e-09
 -0.000000	 -0.000000	diff=5.42392e-10
  0.000009	  0.000009	diff=2.13843e-09
 -0.000001	 -0.000001	diff=4.59631e-10
  0.000018	  0.000018	diff=3.44125e-09
 -0.000001	 -0.000001	diff=5.352e-10
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  local_diff=1.07709e-08
# W_pos, [2 2]
  0.000000	 -0.000000	diff=4.23807e-16
  0.000000	 -0.000000	diff=5.3652e-16
  0.000000	 -0.000000	diff=1.55331e-16
  0.000000	 -0.000000	diff=1.96641e-16
  local_diff=1.3123e-15
# v_pos, [1 2]
  0.000000	  0.000000	diff=1.62939e-15
  0.000000	  0.000000	diff=6.73002e-15
  local_diff=8.35941e-15
# W_h, [2 4]
  0.000000	  0.000000	diff=3.3926e-13
 -0.000001	 -0.000001	diff=9.56741e-13
  0.000000	  0.000000	diff=3.36891e-13
 -0.000000	 -0.000000	diff=4.52117e-14
 -0.000001	 -0.000001	diff=6.62112e-13
  0.000001	  0.000001	diff=2.73314e-12
 -0.000014	 -0.000014	diff=6.76858e-11
  0.000016	  0.000016	diff=1.28148e-10
  local_diff=2.00907e-10
# W_soft, [4 2]
  0.000008	  0.000008	diff=1.00863e-12
  0.000001	  0.000001	diff=1.74127e-13
 -0.000009	 -0.000009	diff=1.29469e-12
 -0.000000	 -0.000000	diff=2.4386e-15
 -0.000026	 -0.000026	diff=3.9336e-14
 -0.000007	 -0.000007	diff=3.23666e-13
  0.000029	  0.000029	diff=1.73664e-13
  0.000005	  0.000005	diff=5.21204e-13
  local_diff=3.53775e-12
# Num params=182, abs_diff=3.16212e-08
Elapsed time is 2.881268 seconds.
[?1l>
## trainLSTM('', '', '', '', '', '', '', '../output/gradcheck', 'isGradCheck', 1, 'isResume', 0, 'feedInput', 1, 'numLayers', 2, 'dropout', 0.8, 'isReverse', 1, 'attnFunc', 1, 'attnOpt', 2)
[?1h=
                                                                   < M A T L A B (R) >
                                                         Copyright 1984-2013 The MathWorks, Inc.
                                                            R2013a (8.1.0.604) 64-bit (maci64)
                                                                    February 15, 2013

 
To get started, type one of these: helpwin, helpdesk, or demo.
For product information, visit www.mathworks.com.
 

  Student License -- for use in conjunction with courses offered at a
  degree-granting institution.  Professional and commercial use prohibited.

## Bilingual setting
# Init LSTM parameters using dataType=double, initRange=0.1
  Model size = 180, individual sizes:  W_src{1}=32 W_src{2}=32 W_tgt{1}=48 W_tgt{2}=32 W_emb_src=8 W_emb_tgt=8 W_a=4 W_h=8 W_soft=8
# assert = 0
# attnFunc = 1
# attnOpt = 2
# batchSize = 10
# dataType = double
# debug = 0
# decode = 1
# dropout = 0.8
# epochFraction = 1
# feedInput = 1
# finetuneEpoch = 5
# finetuneRate = 0.5
# gpuDevice = 0
# initRange = 0.1
# isBi = 1
# isClip = 1
# isGradCheck = 1
# isProfile = 0
# isResume = 0
# isReverse = 1
# learningRate = 1
# loadModel = 
# logFreq = 10
# lstmOpt = 0
# lstmSize = 2
# maxGradNorm = 5
# maxLenRatio = 1.5
# maxSentLen = 7
# minLenRatio = 0.5
# numEpoches = 10
# numLayers = 2
# onlyCPU = 0
# outDir = ../output/gradcheck
# posWin = 1
# seed = 0
# shuffle = 1
# sortBatch = 1
# srcLang = 
# srcVocabFile = 
# testPrefix = 
# tgtLang = 
# tgtVocabFile = 
# trainPrefix = 
# validPrefix = 
# chunkSize = 12800
# baseIndex = 0
# clipForward = 50
# clipBackward = 1000
# nonlinear_gate_f = sigmoid
# nonlinear_gate_f_prime = sigmoidPrime
# nonlinear_f = tanh
# nonlinear_f_prime = tanhPrime
# beamSize = 12
# stackSize = 100
# unkPenalty = 0
# lengthReward = 0
# isGPU = 0
# batchId = 1
# maxRelDist = 2
# logId = 3
# srcSos = 3
# srcEos = 4
# srcVocabSize = 4
# nullPosId = 0
# tgtSos = 3
# tgtEos = 4
# tgtVocabSize = 4
# modelFile = ../output/gradcheck/model.mat
# modelRecentFile = ../output/gradcheck/modelRecent.mat
# softmaxSize = 2
# lr = 1
# epoch = 1
# bestCostValid = 100000
# testPerplexity = 100000
# curTestPerpWord = 100000
# startIter = 0
# iter = 0
# epochBatchCount = 0
# finetuneCount = 0
# modelSize = 180
  src input 1: <s_sos> <s_sos> x y
  src mask: 0  0  1  1  1
  tgt input 1: <t_sos> a <t_eos> <t_eos> <t_eos>
  tgt output 1: a <t_eos> <t_eos> <t_eos> <t_eos>
  tgt mask: 1  1  0  0  0
# W_src{1}, [8 4]
  0.000000	  0.000000	diff=2.11283e-13
  0.000000	  0.000000	diff=2.10311e-13
  0.000000	  0.000000	diff=9.8182e-13
  0.000000	  0.000000	diff=9.03827e-13
  0.000000	  0.000000	diff=4.75751e-13
  0.000000	  0.000000	diff=1.04801e-12
 -0.000002	 -0.000002	diff=3.26045e-10
 -0.000002	 -0.000002	diff=8.82101e-10
  0.000000	  0.000000	diff=1.5323e-13
  0.000000	  0.000000	diff=5.37654e-12
  0.000000	  0.000000	diff=1.48729e-13
  0.000000	  0.000000	diff=9.9394e-13
  0.000000	  0.000000	diff=3.38012e-13
 -0.000000	 -0.000000	diff=4.59557e-12
  0.000001	  0.000001	diff=1.59822e-10
 -0.000002	 -0.000002	diff=2.02936e-09
 -0.000000	 -0.000000	diff=2.71999e-13
  0.000000	  0.000000	diff=5.49455e-13
 -0.000000	 -0.000000	diff=2.6215e-13
  0.000000	  0.000000	diff=3.65491e-13
 -0.000000	 -0.000000	diff=3.97708e-13
 -0.000000	 -0.000000	diff=1.06781e-12
  0.000000	  0.000000	diff=4.38816e-11
 -0.000000	 -0.000000	diff=1.14458e-11
  0.000000	  0.000000	diff=9.27808e-13
 -0.000000	 -0.000000	diff=1.43988e-14
 -0.000000	 -0.000000	diff=5.43032e-13
 -0.000000	 -0.000000	diff=8.91583e-14
  0.000000	  0.000000	diff=1.02609e-12
  0.000000	  0.000000	diff=1.66717e-13
 -0.000000	 -0.000000	diff=2.19906e-12
  0.000000	  0.000000	diff=4.99581e-11
  local_diff=3.52594e-09
# W_src{2}, [8 4]
  0.000000	  0.000000	diff=1.92476e-13
 -0.000000	 -0.000000	diff=5.5535e-14
  0.000000	  0.000000	diff=5.44537e-13
 -0.000000	 -0.000000	diff=5.53463e-13
 -0.000000	 -0.000000	diff=6.36951e-13
 -0.000000	 -0.000000	diff=3.49103e-13
 -0.000001	 -0.000001	diff=5.69839e-10
 -0.000002	 -0.000002	diff=4.86847e-10
 -0.000000	 -0.000000	diff=4.98875e-13
  0.000000	  0.000000	diff=1.67219e-15
 -0.000000	 -0.000000	diff=2.26101e-13
  0.000000	  0.000000	diff=2.93556e-13
  0.000000	  0.000000	diff=2.68106e-13
  0.000000	  0.000000	diff=1.63891e-13
 -0.000003	 -0.000003	diff=9.07809e-09
 -0.000006	 -0.000006	diff=8.08591e-09
  0.000000	  0.000000	diff=1.0876e-12
 -0.000000	 -0.000000	diff=4.39231e-13
  0.000000	  0.000000	diff=1.17612e-12
  0.000000	 -0.000000	diff=1.58664e-13
  0.000000	 -0.000000	diff=1.44034e-12
  0.000000	 -0.000000	diff=3.83697e-13
 -0.000000	 -0.000000	diff=2.97849e-11
 -0.000000	 -0.000000	diff=1.93822e-11
  0.000000	 -0.000000	diff=3.833e-13
  0.000000	  0.000000	diff=5.83027e-14
  0.000000	 -0.000000	diff=3.06328e-13
  0.000000	  0.000000	diff=1.96446e-13
  0.000000	  0.000000	diff=1.39683e-12
  0.000000	  0.000000	diff=9.46837e-13
 -0.000000	 -0.000000	diff=4.73542e-11
  0.000000	  0.000000	diff=1.12142e-10
  local_diff=1.84411e-08
# W_tgt{1}, [8 6]
  0.000000	  0.000000	diff=1.45554e-12
  0.000000	  0.000000	diff=8.04841e-14
  0.000000	  0.000000	diff=1.48225e-13
  0.000000	  0.000000	diff=6.45711e-13
  0.000000	  0.000000	diff=1.26888e-12
  0.000000	  0.000000	diff=5.42293e-13
 -0.000002	 -0.000002	diff=1.68896e-11
 -0.000025	 -0.000025	diff=8.60559e-11
 -0.000000	 -0.000000	diff=4.51956e-13
 -0.000000	 -0.000000	diff=7.27629e-13
 -0.000000	 -0.000000	diff=6.43595e-13
 -0.000000	 -0.000000	diff=2.88992e-12
 -0.000000	 -0.000000	diff=9.40351e-13
 -0.000000	 -0.000000	diff=2.41856e-13
  0.000005	  0.000005	diff=6.0157e-11
  0.000035	  0.000035	diff=8.18742e-11
  0.000000	 -0.000000	diff=7.37537e-13
  0.000000	  0.000000	diff=1.23765e-13
  0.000000	  0.000000	diff=4.72412e-14
  0.000000	  0.000000	diff=1.73071e-14
  0.000000	  0.000000	diff=6.98809e-13
  0.000000	  0.000000	diff=1.87228e-13
 -0.000000	 -0.000000	diff=2.57438e-13
 -0.000000	 -0.000000	diff=4.33974e-13
  0.000000	 -0.000000	diff=3.06119e-15
  0.000000	 -0.000000	diff=2.89033e-14
  0.000000	  0.000000	diff=1.41776e-12
  0.000000	 -0.000000	diff=8.24666e-13
  0.000000	  0.000000	diff=2.58243e-16
  0.000000	 -0.000000	diff=7.81117e-13
 -0.000000	 -0.000000	diff=4.98252e-13
  0.000000	  0.000000	diff=2.68078e-13
  0.000000	  0.000000	diff=8.18701e-13
  0.000000	  0.000000	diff=1.49504e-13
 -0.000000	 -0.000000	diff=5.59646e-13
 -0.000000	 -0.000000	diff=2.0631e-13
 -0.000000	 -0.000000	diff=4.66229e-13
  0.000000	  0.000000	diff=3.99123e-13
  0.000000	  0.000000	diff=7.67705e-11
  0.000000	  0.000000	diff=1.86573e-12
  0.000000	  0.000000	diff=5.47068e-13
 -0.000000	 -0.000000	diff=1.27456e-12
 -0.000000	 -0.000000	diff=5.6121e-14
 -0.000000	 -0.000000	diff=2.62384e-13
 -0.000000	 -0.000000	diff=2.22845e-13
 -0.000000	 -0.000000	diff=5.29232e-13
  0.000000	  0.000000	diff=4.51629e-13
  0.000001	  0.000001	diff=1.41778e-09
  local_diff=1.76369e-09
# W_tgt{2}, [8 4]
 -0.000000	 -0.000000	diff=9.42829e-13
 -0.000000	 -0.000000	diff=1.11681e-13
  0.000000	  0.000000	diff=7.68942e-14
  0.000000	  0.000000	diff=3.8855e-13
 -0.000000	 -0.000000	diff=7.16532e-13
  0.000000	  0.000000	diff=7.78989e-14
 -0.000001	 -0.000001	diff=4.26598e-13
  0.000001	  0.000001	diff=2.92048e-12
 -0.000000	 -0.000000	diff=1.29253e-13
 -0.000000	 -0.000000	diff=4.61014e-14
 -0.000000	 -0.000000	diff=5.83721e-13
 -0.000000	 -0.000000	diff=4.6681e-13
 -0.000000	 -0.000000	diff=1.88715e-14
 -0.000000	 -0.000000	diff=4.00114e-13
 -0.000021	 -0.000021	diff=9.42734e-12
 -0.000024	 -0.000024	diff=2.0935e-10
  0.000000	  0.000000	diff=1.51765e-13
  0.000000	  0.000000	diff=5.36739e-13
  0.000000	  0.000000	diff=3.51479e-13
  0.000000	  0.000000	diff=1.92979e-13
  0.000000	  0.000000	diff=4.11475e-13
  0.000000	  0.000000	diff=1.09855e-12
  0.000000	  0.000000	diff=4.94754e-12
  0.000000	  0.000000	diff=1.45742e-14
  0.000000	  0.000000	diff=6.25299e-14
  0.000000	  0.000000	diff=2.16625e-13
  0.000000	  0.000000	diff=4.78359e-13
  0.000000	  0.000000	diff=6.02323e-13
  0.000000	  0.000000	diff=2.10076e-13
  0.000000	  0.000000	diff=5.82815e-13
  0.000000	  0.000000	diff=3.71257e-13
  0.000001	  0.000001	diff=1.18486e-09
  local_diff=1.42117e-09
# W_emb_src, [2 4]
  0.000000	  0.000000	diff=4.01786e-10
  0.000006	  0.000006	diff=7.99019e-09
  0.000002	  0.000002	diff=7.86899e-10
  0.000007	  0.000007	diff=7.7789e-09
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  local_diff=1.69578e-08
# W_emb_tgt, [2 4]
  0.000025	  0.000025	diff=6.50347e-09
 -0.000000	 -0.000000	diff=3.13787e-10
  0.000020	  0.000020	diff=7.63658e-10
  0.000000	  0.000000	diff=6.86142e-10
  0.000028	  0.000028	diff=2.16072e-09
  0.000001	  0.000001	diff=1.12469e-09
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  local_diff=1.15525e-08
# W_a, [2 2]
  0.000000	  0.000000	diff=3.27516e-17
  0.000000	 -0.000000	diff=3.82285e-17
  0.000000	  0.000000	diff=5.50711e-17
  0.000000	 -0.000000	diff=6.43517e-17
  local_diff=1.90403e-16
# W_h, [2 4]
 -0.000001	 -0.000001	diff=4.97137e-13
 -0.000000	 -0.000000	diff=5.27495e-13
 -0.000001	 -0.000001	diff=3.21824e-13
  0.000002	  0.000002	diff=3.69881e-13
  0.000002	  0.000002	diff=6.21023e-12
 -0.000005	 -0.000005	diff=7.5769e-13
  0.000035	  0.000035	diff=3.26743e-10
 -0.000031	 -0.000031	diff=7.19417e-11
  local_diff=4.07369e-10
# W_soft, [4 2]
 -0.000021	 -0.000021	diff=2.96595e-13
 -0.000003	 -0.000003	diff=7.89498e-13
  0.000024	  0.000024	diff=1.52917e-12
 -0.000000	 -0.000000	diff=2.2691e-13
 -0.000001	 -0.000001	diff=6.82123e-13
  0.000001	  0.000001	diff=7.68445e-13
 -0.000000	 -0.000000	diff=8.28854e-14
 -0.000000	 -0.000000	diff=1.30872e-12
  local_diff=5.68434e-12
# Num params=180, abs_diff=5.40752e-08
Elapsed time is 2.097748 seconds.
[?1l>
## trainLSTM('', '', '', '', '', '', '', '../output/gradcheck', 'isGradCheck', 1, 'isResume', 0, 'feedInput', 1, 'numLayers', 2, 'dropout', 0.8, 'isReverse', 1, 'attnFunc', 1, 'attnOpt', 3)
[?1h=
                                                                   < M A T L A B (R) >
                                                         Copyright 1984-2013 The MathWorks, Inc.
                                                            R2013a (8.1.0.604) 64-bit (maci64)
                                                                    February 15, 2013

 
To get started, type one of these: helpwin, helpdesk, or demo.
For product information, visit www.mathworks.com.
 

  Student License -- for use in conjunction with courses offered at a
  degree-granting institution.  Professional and commercial use prohibited.

## Bilingual setting
# Init LSTM parameters using dataType=double, initRange=0.1
  Model size = 182, individual sizes:  W_src{1}=32 W_src{2}=32 W_tgt{1}=48 W_tgt{2}=32 W_emb_src=8 W_emb_tgt=8 W_a=4 v_a=2 W_h=8 W_soft=8
# assert = 0
# attnFunc = 1
# attnOpt = 3
# batchSize = 10
# dataType = double
# debug = 0
# decode = 1
# dropout = 0.8
# epochFraction = 1
# feedInput = 1
# finetuneEpoch = 5
# finetuneRate = 0.5
# gpuDevice = 0
# initRange = 0.1
# isBi = 1
# isClip = 1
# isGradCheck = 1
# isProfile = 0
# isResume = 0
# isReverse = 1
# learningRate = 1
# loadModel = 
# logFreq = 10
# lstmOpt = 0
# lstmSize = 2
# maxGradNorm = 5
# maxLenRatio = 1.5
# maxSentLen = 7
# minLenRatio = 0.5
# numEpoches = 10
# numLayers = 2
# onlyCPU = 0
# outDir = ../output/gradcheck
# posWin = 1
# seed = 0
# shuffle = 1
# sortBatch = 1
# srcLang = 
# srcVocabFile = 
# testPrefix = 
# tgtLang = 
# tgtVocabFile = 
# trainPrefix = 
# validPrefix = 
# chunkSize = 12800
# baseIndex = 0
# clipForward = 50
# clipBackward = 1000
# nonlinear_gate_f = sigmoid
# nonlinear_gate_f_prime = sigmoidPrime
# nonlinear_f = tanh
# nonlinear_f_prime = tanhPrime
# beamSize = 12
# stackSize = 100
# unkPenalty = 0
# lengthReward = 0
# isGPU = 0
# batchId = 1
# maxRelDist = 2
# logId = 3
# srcSos = 3
# srcEos = 4
# srcVocabSize = 4
# nullPosId = 0
# tgtSos = 3
# tgtEos = 4
# tgtVocabSize = 4
# modelFile = ../output/gradcheck/model.mat
# modelRecentFile = ../output/gradcheck/modelRecent.mat
# softmaxSize = 2
# lr = 1
# epoch = 1
# bestCostValid = 100000
# testPerplexity = 100000
# curTestPerpWord = 100000
# startIter = 0
# iter = 0
# epochBatchCount = 0
# finetuneCount = 0
# modelSize = 182
  src input 1: x x x x
  src mask: 1  1  1  1  1
  tgt input 1: <t_sos> a b <t_eos> <t_eos>
  tgt output 1: a b <t_eos> <t_eos> <t_eos>
  tgt mask: 1  1  1  0  0
# W_src{1}, [8 4]
  0.000000	  0.000000	diff=3.89083e-13
  0.000000	  0.000000	diff=1.11933e-12
  0.000000	  0.000000	diff=2.2195e-14
  0.000000	  0.000000	diff=1.42963e-14
  0.000000	  0.000000	diff=2.81252e-13
  0.000000	  0.000000	diff=1.32728e-14
 -0.000001	 -0.000001	diff=8.1916e-10
 -0.000003	 -0.000003	diff=6.78229e-10
  0.000000	  0.000000	diff=1.11587e-12
  0.000000	  0.000000	diff=2.20322e-12
 -0.000000	 -0.000000	diff=9.67593e-15
  0.000000	  0.000000	diff=5.45434e-13
 -0.000000	 -0.000000	diff=1.26492e-12
 -0.000000	 -0.000000	diff=1.50451e-12
  0.000001	  0.000001	diff=9.41161e-10
 -0.000002	 -0.000002	diff=1.19636e-09
 -0.000000	 -0.000000	diff=1.22174e-12
  0.000000	  0.000000	diff=7.82054e-13
 -0.000000	 -0.000000	diff=1.89729e-13
  0.000000	  0.000000	diff=9.07092e-13
 -0.000000	 -0.000000	diff=6.80857e-14
 -0.000000	 -0.000000	diff=1.7734e-13
  0.000000	  0.000000	diff=4.63239e-11
 -0.000000	 -0.000000	diff=3.41518e-12
  0.000000	  0.000000	diff=2.39005e-13
 -0.000000	 -0.000000	diff=1.02153e-13
  0.000000	  0.000000	diff=1.17666e-12
 -0.000000	 -0.000000	diff=2.20144e-13
  0.000000	  0.000000	diff=3.97374e-13
  0.000000	  0.000000	diff=4.32425e-13
  0.000000	  0.000000	diff=4.46065e-13
  0.000000	  0.000000	diff=7.35874e-11
  local_diff=3.77308e-09
# W_src{2}, [8 4]
 -0.000000	 -0.000000	diff=1.25497e-13
 -0.000000	 -0.000000	diff=2.20409e-13
 -0.000000	 -0.000000	diff=1.05074e-12
 -0.000000	 -0.000000	diff=2.06619e-13
  0.000000	  0.000000	diff=8.09356e-13
  0.000000	  0.000000	diff=5.37109e-14
  0.000000	  0.000000	diff=1.89046e-10
 -0.000001	 -0.000001	diff=3.95545e-10
  0.000000	  0.000000	diff=9.4394e-14
  0.000000	  0.000000	diff=3.17757e-13
 -0.000000	 -0.000000	diff=6.08125e-13
  0.000000	  0.000000	diff=2.16467e-13
 -0.000000	 -0.000000	diff=1.1918e-13
 -0.000000	 -0.000000	diff=2.42291e-13
  0.000001	  0.000001	diff=2.70601e-09
 -0.000000	 -0.000000	diff=3.75113e-09
 -0.000000	 -0.000000	diff=1.18832e-12
 -0.000000	 -0.000000	diff=3.12791e-13
  0.000000	  0.000000	diff=4.01378e-13
 -0.000000	 -0.000000	diff=9.57755e-13
  0.000000	  0.000000	diff=7.50398e-13
  0.000000	  0.000000	diff=5.46866e-13
  0.000000	  0.000000	diff=4.38248e-11
 -0.000000	 -0.000000	diff=8.96963e-12
  0.000000	  0.000000	diff=3.42245e-13
  0.000000	  0.000000	diff=6.60313e-13
 -0.000000	 -0.000000	diff=2.89283e-13
  0.000000	  0.000000	diff=5.95415e-13
 -0.000000	 -0.000000	diff=5.58223e-13
 -0.000000	 -0.000000	diff=6.74306e-13
 -0.000000	 -0.000000	diff=1.07553e-11
 -0.000000	 -0.000000	diff=6.2467e-12
  local_diff=7.12287e-09
# W_tgt{1}, [8 6]
 -0.000000	 -0.000000	diff=1.12477e-13
  0.000000	  0.000000	diff=2.94612e-13
  0.000000	  0.000000	diff=2.22319e-13
  0.000000	  0.000000	diff=2.95628e-13
  0.000000	  0.000000	diff=6.85497e-13
  0.000000	  0.000000	diff=4.02195e-13
 -0.000001	 -0.000001	diff=1.68746e-11
 -0.000020	 -0.000020	diff=1.03174e-10
 -0.000000	 -0.000000	diff=6.45456e-13
 -0.000000	 -0.000000	diff=6.8529e-13
 -0.000000	 -0.000000	diff=2.27633e-14
 -0.000000	 -0.000000	diff=2.94165e-12
 -0.000000	 -0.000000	diff=7.53056e-13
 -0.000000	 -0.000000	diff=1.44247e-13
  0.000001	  0.000001	diff=4.82796e-11
  0.000037	  0.000037	diff=2.53507e-10
  0.000000	  0.000000	diff=7.17061e-16
  0.000000	  0.000000	diff=1.077e-14
  0.000000	 -0.000000	diff=2.11124e-14
  0.000000	 -0.000000	diff=1.14129e-13
  0.000000	 -0.000000	diff=1.31149e-14
  0.000000	 -0.000000	diff=9.13359e-14
  0.000000	  0.000000	diff=9.67332e-15
  0.000000	  0.000000	diff=1.02842e-12
  0.000000	  0.000000	diff=3.0903e-14
  0.000000	  0.000000	diff=6.78966e-13
  0.000000	  0.000000	diff=4.31662e-14
  0.000000	  0.000000	diff=1.34071e-13
  0.000000	  0.000000	diff=6.92145e-14
  0.000000	  0.000000	diff=2.28617e-13
 -0.000000	 -0.000000	diff=6.18858e-15
 -0.000000	 -0.000000	diff=1.21311e-12
 -0.000000	 -0.000000	diff=2.45467e-13
  0.000000	  0.000000	diff=7.77046e-14
  0.000000	  0.000000	diff=2.83007e-13
 -0.000000	 -0.000000	diff=1.30455e-13
 -0.000000	 -0.000000	diff=6.2968e-13
  0.000000	  0.000000	diff=2.27493e-13
 -0.000000	 -0.000000	diff=3.27386e-11
  0.000000	  0.000000	diff=3.60548e-12
 -0.000000	 -0.000000	diff=3.32002e-13
 -0.000000	 -0.000000	diff=5.70022e-13
 -0.000000	 -0.000000	diff=8.04958e-13
 -0.000000	 -0.000000	diff=5.35031e-13
 -0.000000	 -0.000000	diff=5.96465e-13
 -0.000000	 -0.000000	diff=1.15736e-13
 -0.000000	 -0.000000	diff=2.8985e-13
  0.000001	  0.000001	diff=9.1715e-10
  local_diff=1.39107e-09
# W_tgt{2}, [8 4]
 -0.000000	 -0.000000	diff=3.75736e-13
 -0.000000	 -0.000000	diff=1.07229e-13
 -0.000000	 -0.000000	diff=6.78159e-13
 -0.000000	 -0.000000	diff=3.80231e-13
 -0.000000	 -0.000000	diff=1.88354e-13
 -0.000000	 -0.000000	diff=4.34426e-13
 -0.000000	 -0.000000	diff=2.43519e-12
 -0.000001	 -0.000001	diff=1.69095e-12
  0.000000	  0.000000	diff=1.27911e-13
 -0.000000	 -0.000000	diff=3.31082e-13
  0.000000	  0.000000	diff=2.71845e-13
 -0.000000	 -0.000000	diff=7.68852e-13
  0.000000	  0.000000	diff=5.63742e-13
 -0.000000	 -0.000000	diff=5.14276e-13
  0.000009	  0.000009	diff=3.90126e-11
 -0.000022	 -0.000022	diff=1.37057e-10
  0.000000	  0.000000	diff=1.56297e-14
  0.000000	  0.000000	diff=9.83761e-15
 -0.000000	 -0.000000	diff=4.12079e-13
 -0.000000	 -0.000000	diff=2.71396e-13
 -0.000000	 -0.000000	diff=7.74497e-13
  0.000000	  0.000000	diff=2.01289e-13
  0.000000	  0.000000	diff=7.07502e-11
  0.000000	  0.000000	diff=4.08899e-13
 -0.000000	  0.000000	diff=9.65913e-13
  0.000000	  0.000000	diff=2.10248e-13
  0.000000	  0.000000	diff=2.83943e-13
  0.000000	  0.000000	diff=3.50428e-13
  0.000000	  0.000000	diff=1.82186e-13
  0.000000	  0.000000	diff=1.10638e-13
 -0.000000	 -0.000000	diff=2.77819e-13
  0.000000	  0.000000	diff=6.54759e-10
  local_diff=9.14921e-10
# W_emb_src, [2 4]
 -0.000001	 -0.000001	diff=7.68591e-12
  0.000006	  0.000006	diff=5.7019e-09
  0.000001	  0.000001	diff=4.28558e-10
  0.000006	  0.000006	diff=4.42522e-09
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  local_diff=1.05634e-08
# W_emb_tgt, [2 4]
  0.000020	  0.000020	diff=3.65451e-09
 -0.000000	 -0.000000	diff=5.42355e-10
  0.000009	  0.000009	diff=2.13743e-09
 -0.000001	 -0.000001	diff=4.58211e-10
  0.000018	  0.000018	diff=3.44188e-09
 -0.000001	 -0.000001	diff=5.34481e-10
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  local_diff=1.07689e-08
# W_a, [2 2]
  0.000000	 -0.000000	diff=7.22066e-25
  0.000000	 -0.000000	diff=2.18478e-24
  0.000000	 -0.000000	diff=1.39975e-24
  0.000000	 -0.000000	diff=7.22448e-24
  local_diff=1.15311e-23
# v_a, [1 2]
  0.000000	  0.000000	diff=1.45651e-12
 -0.000000	 -0.000000	diff=4.08018e-13
  local_diff=1.86453e-12
# W_h, [2 4]
  0.000000	  0.000000	diff=2.9641e-13
 -0.000001	 -0.000001	diff=3.17873e-13
  0.000001	  0.000001	diff=5.12031e-13
 -0.000000	 -0.000000	diff=3.14627e-13
 -0.000001	 -0.000001	diff=1.18023e-12
  0.000001	  0.000001	diff=3.09933e-12
 -0.000014	 -0.000014	diff=6.79475e-11
  0.000016	  0.000016	diff=1.2746e-10
  local_diff=2.01128e-10
# W_soft, [4 2]
  0.000008	  0.000008	diff=5.93487e-14
  0.000000	  0.000000	diff=5.49712e-13
 -0.000009	 -0.000009	diff=5.84191e-13
  0.000000	  0.000000	diff=3.46531e-13
 -0.000026	 -0.000026	diff=4.31667e-14
 -0.000008	 -0.000008	diff=5.18947e-13
  0.000029	  0.000029	diff=7.93649e-14
  0.000005	  0.000005	diff=1.55398e-13
  local_diff=2.33666e-12
# Num params=182, abs_diff=3.47395e-08
Elapsed time is 2.245570 seconds.
[?1l>
## trainLSTM('', '', '', '', '', '', '', '../output/gradcheck', 'isGradCheck', 1, 'initRange', 10, 'isResume', 0, 'feedInput', 1)
[?1h=
                                                                   < M A T L A B (R) >
                                                         Copyright 1984-2013 The MathWorks, Inc.
                                                            R2013a (8.1.0.604) 64-bit (maci64)
                                                                    February 15, 2013

 
To get started, type one of these: helpwin, helpdesk, or demo.
For product information, visit www.mathworks.com.
 

  Student License -- for use in conjunction with courses offered at a
  degree-granting institution.  Professional and commercial use prohibited.

## Bilingual setting
# Init LSTM parameters using dataType=double, initRange=10
  Model size = 104, individual sizes:  W_src{1}=32 W_tgt{1}=48 W_emb_src=8 W_emb_tgt=8 W_soft=8
# assert = 0
# attnFunc = 0
# attnOpt = 0
# batchSize = 10
# dataType = double
# debug = 0
# decode = 1
# dropout = 1
# epochFraction = 1
# feedInput = 1
# finetuneEpoch = 5
# finetuneRate = 0.5
# gpuDevice = 0
# initRange = 10
# isBi = 1
# isClip = 1
# isGradCheck = 1
# isProfile = 0
# isResume = 0
# isReverse = 0
# learningRate = 1
# loadModel = 
# logFreq = 10
# lstmOpt = 0
# lstmSize = 2
# maxGradNorm = 5
# maxLenRatio = 1.5
# maxSentLen = 7
# minLenRatio = 0.5
# numEpoches = 10
# numLayers = 1
# onlyCPU = 0
# outDir = ../output/gradcheck
# posWin = 1
# seed = 0
# shuffle = 1
# sortBatch = 1
# srcLang = 
# srcVocabFile = 
# testPrefix = 
# tgtLang = 
# tgtVocabFile = 
# trainPrefix = 
# validPrefix = 
# chunkSize = 12800
# baseIndex = 0
# clipForward = 50
# clipBackward = 1000
# nonlinear_gate_f = sigmoid
# nonlinear_gate_f_prime = sigmoidPrime
# nonlinear_f = tanh
# nonlinear_f_prime = tanhPrime
# beamSize = 12
# stackSize = 100
# unkPenalty = 0
# lengthReward = 0
# isGPU = 0
# batchId = 1
# maxRelDist = 2
# logId = 3
# srcSos = 3
# srcEos = 4
# srcVocabSize = 4
# nullPosId = 0
# tgtSos = 3
# tgtEos = 4
# tgtVocabSize = 4
# modelFile = ../output/gradcheck/model.mat
# modelRecentFile = ../output/gradcheck/modelRecent.mat
# softmaxSize = 2
# lr = 1
# epoch = 1
# bestCostValid = 100000
# testPerplexity = 100000
# curTestPerpWord = 100000
# startIter = 0
# iter = 0
# epochBatchCount = 0
# finetuneCount = 0
# modelSize = 104
  src input 1: y y x y
  src mask: 1  1  1  1  1
  tgt input 1: <t_sos> a b a
  tgt output 1: a b a <t_eos>
  tgt mask: 1  1  1  1
# W_src{1}, [8 4]
 -0.000013	 -0.000013	diff=4.63718e-07
  0.000001	  0.000001	diff=3.65501e-08
 -0.000932	 -0.000900	diff=3.1885e-05
  0.000000	  0.000000	diff=5.99269e-12
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  0.000012	  0.000012	diff=3.89478e-07
 -0.000001	 -0.000001	diff=3.07312e-08
  0.000818	  0.000845	diff=2.66832e-05
 -0.000000	 -0.000000	diff=3.9331e-12
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  0.000001	  0.000001	diff=5.53295e-09
 -0.000000	 -0.000000	diff=2.5681e-11
  0.000099	  0.000099	diff=3.74947e-07
 -0.000000	 -0.000000	diff=2.84401e-12
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
 -0.000002	 -0.000002	diff=9.24172e-09
  0.000000	  0.000000	diff=2.51403e-11
 -0.000129	 -0.000129	diff=6.36201e-07
  0.000000	  0.000000	diff=2.67178e-12
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  local_diff=6.05147e-05
# W_tgt{1}, [8 6]
 -0.063025	 -0.053372	diff=0.00965252
  0.000000	  0.000000	diff=1.28429e-08
  0.000000	 -0.000000	diff=1.50853e-15
 -0.000000	 -0.000000	diff=2.483e-12
-31.263908	-32.483123	diff=1.21921
 -0.010618	 -0.011040	diff=0.000421337
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  0.048896	  0.062674	diff=0.0137782
  0.000000	  0.000000	diff=1.72803e-08
  0.000000	  0.000000	diff=8.37179e-16
  0.000000	  0.000000	diff=9.17082e-13
 39.771604	 38.144140	diff=1.62746
  0.013573	  0.012957	diff=0.000615584
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  0.023973	  0.024038	diff=6.48058e-05
  0.000000	 -0.000000	diff=3.79277e-51
  0.000000	  0.000000	diff=4.41704e-17
  0.000000	 -0.000000	diff=2.09651e-15
 -0.685378	 -0.683481	diff=0.00189718
  0.000137	  0.000137	diff=1.83166e-07
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
 -0.019028	 -0.018988	diff=4.01812e-05
  0.000000	  0.000000	diff=2.58066e-51
  0.000000	 -0.000000	diff=4.51516e-20
  0.000000	  0.000000	diff=2.24548e-18
  0.554215	  0.555460	diff=0.00124536
  0.000000	  0.000000	diff=6.82596e-09
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  0.023973	  0.024038	diff=6.48058e-05
  0.000000	  0.000000	diff=2.55251e-10
  0.000000	  0.000000	diff=4.41704e-17
  0.000000	 -0.000000	diff=2.09651e-15
 -0.685378	 -0.683481	diff=0.00189718
  0.000137	  0.000137	diff=1.83166e-07
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
 -0.019028	 -0.018988	diff=4.01812e-05
 -0.000000	 -0.000000	diff=2.87921e-10
  0.000000	 -0.000000	diff=4.51516e-20
  0.000000	  0.000000	diff=2.24548e-18
  0.554215	  0.555460	diff=0.00124536
  0.000000	  0.000000	diff=6.82596e-09
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  local_diff=2.87764
# W_emb_src, [2 4]
 -0.000003	 -0.000003	diff=1.14337e-07
 -0.000003	 -0.000003	diff=9.65271e-08
 -0.000990	 -0.000953	diff=3.72485e-05
 -0.000900	 -0.000869	diff=3.17681e-05
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  local_diff=6.92275e-05
# W_emb_tgt, [2 4]
-27.956638	-28.915513	diff=0.958875
-24.302396	-25.019891	diff=0.717496
 -0.000005	 -0.000005	diff=1.23032e-07
 -0.000005	 -0.000005	diff=1.29694e-07
  0.000001	  0.000001	diff=2.66556e-08
 -0.000000	 -0.000000	diff=8.71782e-09
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  local_diff=1.67637
# W_soft, [4 2]
 -3.427182	 -3.427237	diff=5.54155e-05
 -2.145279	 -2.146908	diff=0.00162899
  0.126818	  0.126673	diff=0.000145468
  5.449159	  5.447473	diff=0.00168599
  2.285337	  2.285337	diff=3.86105e-08
  1.413201	  1.412786	diff=0.000414105
 -0.000600	 -0.000601	diff=1.90223e-06
 -3.697104	 -3.697522	diff=0.000418031
  local_diff=0.00434995
# Num params=104, abs_diff=4.55849
Elapsed time is 0.607274 seconds.
[?1l>
## trainLSTM('', '', '', '', '', '', '', '../output/gradcheck', 'isGradCheck', 1, 'initRange', 10, 'isResume', 0, 'feedInput', 1, 'numLayers', 2)
[?1h=
                                                                   < M A T L A B (R) >
                                                         Copyright 1984-2013 The MathWorks, Inc.
                                                            R2013a (8.1.0.604) 64-bit (maci64)
                                                                    February 15, 2013

 
To get started, type one of these: helpwin, helpdesk, or demo.
For product information, visit www.mathworks.com.
 

  Student License -- for use in conjunction with courses offered at a
  degree-granting institution.  Professional and commercial use prohibited.

## Bilingual setting
# Init LSTM parameters using dataType=double, initRange=10
  Model size = 168, individual sizes:  W_src{1}=32 W_src{2}=32 W_tgt{1}=48 W_tgt{2}=32 W_emb_src=8 W_emb_tgt=8 W_soft=8
# assert = 0
# attnFunc = 0
# attnOpt = 0
# batchSize = 10
# dataType = double
# debug = 0
# decode = 1
# dropout = 1
# epochFraction = 1
# feedInput = 1
# finetuneEpoch = 5
# finetuneRate = 0.5
# gpuDevice = 0
# initRange = 10
# isBi = 1
# isClip = 1
# isGradCheck = 1
# isProfile = 0
# isResume = 0
# isReverse = 0
# learningRate = 1
# loadModel = 
# logFreq = 10
# lstmOpt = 0
# lstmSize = 2
# maxGradNorm = 5
# maxLenRatio = 1.5
# maxSentLen = 7
# minLenRatio = 0.5
# numEpoches = 10
# numLayers = 2
# onlyCPU = 0
# outDir = ../output/gradcheck
# posWin = 1
# seed = 0
# shuffle = 1
# sortBatch = 1
# srcLang = 
# srcVocabFile = 
# testPrefix = 
# tgtLang = 
# tgtVocabFile = 
# trainPrefix = 
# validPrefix = 
# chunkSize = 12800
# baseIndex = 0
# clipForward = 50
# clipBackward = 1000
# nonlinear_gate_f = sigmoid
# nonlinear_gate_f_prime = sigmoidPrime
# nonlinear_f = tanh
# nonlinear_f_prime = tanhPrime
# beamSize = 12
# stackSize = 100
# unkPenalty = 0
# lengthReward = 0
# isGPU = 0
# batchId = 1
# maxRelDist = 2
# logId = 3
# srcSos = 3
# srcEos = 4
# srcVocabSize = 4
# nullPosId = 0
# tgtSos = 3
# tgtEos = 4
# tgtVocabSize = 4
# modelFile = ../output/gradcheck/model.mat
# modelRecentFile = ../output/gradcheck/modelRecent.mat
# softmaxSize = 2
# lr = 1
# epoch = 1
# bestCostValid = 100000
# testPerplexity = 100000
# curTestPerpWord = 100000
# startIter = 0
# iter = 0
# epochBatchCount = 0
# finetuneCount = 0
# modelSize = 168
  src input 1: <s_sos> x y y
  src mask: 0  1  1  1  1
  tgt input 1: <t_sos> b b <t_eos> <t_eos>
  tgt output 1: b b <t_eos> <t_eos> <t_eos>
  tgt mask: 1  1  1  0  0
# W_src{1}, [8 4]
 -0.000645	 -0.000655	diff=9.57798e-06
 -0.000000	 -0.000000	diff=9.14617e-13
  0.000020	  0.000019	diff=2.86991e-07
  0.000000	 -0.000000	diff=4.7598e-16
-12.880180	-12.036708	diff=0.843472
 -0.000000	 -0.000000	diff=4.27527e-10
  0.000000	  0.000000	diff=1.13196e-12
 -0.000000	 -0.000000	diff=6.82978e-13
  0.000219	  0.000218	diff=1.07071e-06
  0.000000	  0.000000	diff=1.68232e-13
 -0.000006	 -0.000006	diff=3.12514e-08
  0.000000	  0.000000	diff=1.58105e-16
  3.844936	  3.998194	diff=0.153258
  0.000000	  0.000000	diff=4.66287e-11
  0.000000	  0.000000	diff=1.98321e-12
  0.000000	  0.000000	diff=1.18925e-12
 -0.000042	 -0.000042	diff=2.38479e-07
  0.000000	  0.000000	diff=1.00509e-13
  0.000005	  0.000005	diff=1.92753e-08
  0.000000	  0.000000	diff=3.02454e-17
  0.427493	  0.429236	diff=0.00174272
  0.000000	  0.000000	diff=1.20405e-11
  0.000000	  0.000000	diff=1.87317e-13
  0.000000	  0.000000	diff=7.12328e-13
  0.000000	 -0.000000	diff=3.66484e-15
  0.000000	 -0.000000	diff=9.15927e-27
  0.000000	 -0.000000	diff=3.32337e-17
  0.000000	 -0.000000	diff=7.69988e-27
 -0.000000	 -0.000000	diff=2.56314e-13
  0.000000	 -0.000000	diff=7.75586e-20
  0.000000	  0.000000	diff=4.93975e-22
  0.000000	 -0.000000	diff=1.66729e-27
  local_diff=0.998483
# W_src{2}, [8 4]
  0.060915	  0.059955	diff=0.000960268
 -0.105433	 -0.105172	diff=0.00026134
  0.026054	  0.026043	diff=1.08659e-05
  0.097445	  0.097690	diff=0.000245299
 -0.717873	 -0.715295	diff=0.00257756
  0.304524	  0.303461	diff=0.00106279
 -0.417615	 -0.416720	diff=0.000895252
  0.014483	  0.014604	diff=0.000121373
  0.000000	  0.000000	diff=1.47533e-12
  0.000000	  0.000000	diff=6.74657e-13
 -0.000000	 -0.000000	diff=6.01845e-13
  0.000000	  0.000000	diff=1.12799e-13
  0.000000	  0.000000	diff=9.99918e-13
  0.000000	  0.000000	diff=7.96308e-13
  0.000000	  0.000000	diff=1.33443e-12
 -0.000000	  0.000000	diff=7.38353e-13
  0.060181	  0.060104	diff=7.68896e-05
 -0.157522	 -0.157616	diff=9.45347e-05
  0.042343	  0.042319	diff=2.43084e-05
 -0.038686	 -0.038684	diff=2.3279e-06
  0.060124	  0.060122	diff=1.77464e-06
 -0.051238	 -0.051260	diff=2.20286e-05
 -0.736389	 -0.733520	diff=0.00286856
 -0.002836	 -0.002831	diff=5.27755e-06
  0.043146	  0.043124	diff=2.16903e-05
  0.353888	  0.353337	diff=0.000551438
 -0.118440	 -0.118594	diff=0.000153526
  0.090253	  0.090250	diff=2.50515e-06
 -0.117548	 -0.117562	diff=1.49065e-05
  0.145487	  0.145627	diff=0.000140061
  1.845915	  1.865950	diff=0.0200348
  0.006550	  0.006578	diff=2.81229e-05
  local_diff=0.0301775
# W_tgt{1}, [8 6]
 -0.000000	 -0.000000	diff=1.30654e-12
  0.000021	  0.000022	diff=2.78137e-07
  0.008457	  0.008348	diff=0.000109014
  0.000000	 -0.000000	diff=2.99343e-17
  0.000000	  0.000000	diff=8.27219e-13
 -0.000000	 -0.000000	diff=1.49694e-12
 -0.000000	 -0.000000	diff=1.06007e-10
  0.000000	  0.000000	diff=5.36662e-20
  0.000000	  0.000000	diff=1.0941e-13
 -0.000021	 -0.000021	diff=2.58047e-07
 -0.007876	 -0.007974	diff=9.78879e-05
  0.000000	  0.000000	diff=2.85921e-17
 -0.000000	 -0.000000	diff=6.30957e-13
  0.000000	  0.000000	diff=1.40573e-12
  0.000000	  0.000000	diff=9.3218e-11
  0.000000	 -0.000000	diff=5.12599e-20
  0.000000	  0.000000	diff=6.40149e-15
 -0.000012	 -0.000012	diff=1.54628e-08
  0.000010	  0.000010	diff=1.5639e-10
  0.000000	 -0.000000	diff=2.35214e-17
  0.000000	  0.000000	diff=1.87085e-15
 -0.000000	 -0.000000	diff=2.82549e-13
  0.000000	  0.000000	diff=8.78595e-13
  0.000000	 -0.000000	diff=6.99841e-21
  0.000000	  0.000000	diff=1.49288e-16
  0.000023	  0.000023	diff=1.60918e-08
 -0.000530	 -0.000531	diff=4.36512e-07
  0.000000	 -0.000000	diff=8.37201e-18
  0.000000	 -0.000000	diff=5.35507e-14
  0.000000	  0.000000	diff=8.84905e-13
 -0.000000	 -0.000000	diff=2.21974e-13
  0.000000	  0.000000	diff=3.42112e-21
  0.000000	  0.000000	diff=5.50464e-41
  0.000037	  0.000037	diff=1.40205e-07
 -0.002408	 -0.002417	diff=9.03936e-06
  0.000000	  0.000000	diff=1.60276e-23
  0.000000	 -0.000000	diff=2.43863e-13
  0.000000	  0.000000	diff=1.14525e-12
  0.000000	 -0.000000	diff=6.97213e-17
  0.000000	  0.000000	diff=5.16151e-61
  0.000000	 -0.000000	diff=1.17579e-28
  0.000000	  0.000000	diff=9.2265e-13
 -0.000000	 -0.000000	diff=6.92046e-13
  0.000000	  0.000000	diff=2.6898e-29
  0.000000	 -0.000000	diff=5.3467e-19
  0.000000	  0.000000	diff=4.92409e-17
  0.000000	 -0.000000	diff=1.58417e-22
  0.000000	  0.000000	diff=2.32445e-34
  local_diff=0.000217086
# W_tgt{2}, [8 4]
 -0.071362	 -0.071485	diff=0.00012359
  0.532100	  0.530731	diff=0.00136848
 -0.000101	 -0.000100	diff=3.81767e-07
 -0.000796	 -0.000776	diff=1.97807e-05
 -0.031510	 -0.031391	diff=0.000118883
  0.015004	  0.015053	diff=4.851e-05
  0.051053	  0.050694	diff=0.000358598
  3.638081	  3.595972	diff=0.0421091
 -0.000001	 -0.000001	diff=5.37918e-13
  0.000005	  0.000005	diff=6.04414e-13
 -0.000000	 -0.000000	diff=2.64499e-13
 -0.000000	 -0.000000	diff=1.00026e-12
 -0.000000	 -0.000000	diff=6.89229e-13
  0.000000	  0.000000	diff=1.19153e-13
  0.000002	  0.000002	diff=2.10882e-13
  0.000003	  0.000003	diff=7.21238e-12
  0.021535	  0.021699	diff=0.000164271
  0.069052	  0.069106	diff=5.32004e-05
 -0.028745	 -0.028754	diff=8.95531e-06
 -0.060642	 -0.060183	diff=0.000459439
  0.042561	  0.042483	diff=7.84445e-05
 -0.007008	 -0.007014	diff=5.64043e-06
  0.148883	  0.148898	diff=1.54763e-05
  0.428078	  0.418259	diff=0.00981886
  0.052168	  0.052767	diff=0.000599286
 -0.081964	 -0.081873	diff=9.04268e-05
  0.015287	  0.015313	diff=2.62133e-05
  0.322878	  0.325820	diff=0.00294157
 -0.051675	 -0.051693	diff=1.82975e-05
  0.023439	  0.023505	diff=6.56215e-05
 -0.066655	 -0.066689	diff=3.41197e-05
  1.828751	  1.858322	diff=0.0295713
  local_diff=0.0880985
# W_emb_src, [2 4]
 -0.000000	 -0.000000	diff=5.27539e-13
  0.000000	  0.000000	diff=2.27522e-12
-11.492646	-10.784612	diff=0.708034
-39.612802	-37.246982	diff=2.36582
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  local_diff=3.07385
# W_emb_tgt, [2 4]
  0.000000	  0.000000	diff=6.01745e-13
 -0.000000	 -0.000000	diff=4.59418e-13
  0.004332	  0.004304	diff=2.80815e-05
 -0.003162	 -0.003177	diff=1.53719e-05
 -0.000000	 -0.000000	diff=1.87861e-09
  0.000000	  0.000000	diff=1.00048e-09
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  local_diff=4.34562e-05
# W_soft, [4 2]
  0.163799	  0.163397	diff=0.000401677
 -0.361160	 -0.361564	diff=0.000404727
  0.006846	  0.006467	diff=0.000379625
  0.192091	  0.191701	diff=0.000389864
 -0.395371	 -0.395582	diff=0.000210191
 -0.150213	 -0.150430	diff=0.000217699
  0.520510	  0.520169	diff=0.000341006
  0.026195	  0.025843	diff=0.000352205
  local_diff=0.00269699
# Num params=168, abs_diff=4.19357
Elapsed time is 1.423059 seconds.
[?1l>
## trainLSTM('', '', '', '', '', '', '', '../output/gradcheck', 'isGradCheck', 1, 'initRange', 10, 'isResume', 0, 'feedInput', 1, 'numLayers', 2, 'dropout', 0.8)
[?1h=
                                                                   < M A T L A B (R) >
                                                         Copyright 1984-2013 The MathWorks, Inc.
                                                            R2013a (8.1.0.604) 64-bit (maci64)
                                                                    February 15, 2013

 
To get started, type one of these: helpwin, helpdesk, or demo.
For product information, visit www.mathworks.com.
 

  Student License -- for use in conjunction with courses offered at a
  degree-granting institution.  Professional and commercial use prohibited.

## Bilingual setting
# Init LSTM parameters using dataType=double, initRange=10
  Model size = 168, individual sizes:  W_src{1}=32 W_src{2}=32 W_tgt{1}=48 W_tgt{2}=32 W_emb_src=8 W_emb_tgt=8 W_soft=8
# assert = 0
# attnFunc = 0
# attnOpt = 0
# batchSize = 10
# dataType = double
# debug = 0
# decode = 1
# dropout = 0.8
# epochFraction = 1
# feedInput = 1
# finetuneEpoch = 5
# finetuneRate = 0.5
# gpuDevice = 0
# initRange = 10
# isBi = 1
# isClip = 1
# isGradCheck = 1
# isProfile = 0
# isResume = 0
# isReverse = 0
# learningRate = 1
# loadModel = 
# logFreq = 10
# lstmOpt = 0
# lstmSize = 2
# maxGradNorm = 5
# maxLenRatio = 1.5
# maxSentLen = 7
# minLenRatio = 0.5
# numEpoches = 10
# numLayers = 2
# onlyCPU = 0
# outDir = ../output/gradcheck
# posWin = 1
# seed = 0
# shuffle = 1
# sortBatch = 1
# srcLang = 
# srcVocabFile = 
# testPrefix = 
# tgtLang = 
# tgtVocabFile = 
# trainPrefix = 
# validPrefix = 
# chunkSize = 12800
# baseIndex = 0
# clipForward = 50
# clipBackward = 1000
# nonlinear_gate_f = sigmoid
# nonlinear_gate_f_prime = sigmoidPrime
# nonlinear_f = tanh
# nonlinear_f_prime = tanhPrime
# beamSize = 12
# stackSize = 100
# unkPenalty = 0
# lengthReward = 0
# isGPU = 0
# batchId = 1
# maxRelDist = 2
# logId = 3
# srcSos = 3
# srcEos = 4
# srcVocabSize = 4
# nullPosId = 0
# tgtSos = 3
# tgtEos = 4
# tgtVocabSize = 4
# modelFile = ../output/gradcheck/model.mat
# modelRecentFile = ../output/gradcheck/modelRecent.mat
# softmaxSize = 2
# lr = 1
# epoch = 1
# bestCostValid = 100000
# testPerplexity = 100000
# curTestPerpWord = 100000
# startIter = 0
# iter = 0
# epochBatchCount = 0
# finetuneCount = 0
# modelSize = 168
  src input 1: <s_sos> x y y
  src mask: 0  1  1  1  1
  tgt input 1: <t_sos> b b <t_eos> <t_eos>
  tgt output 1: b b <t_eos> <t_eos> <t_eos>
  tgt mask: 1  1  1  0  0
# W_src{1}, [8 4]
 -0.000067	 -0.000068	diff=1.23979e-06
  0.000000	  0.000000	diff=4.43392e-14
  0.000000	  0.000000	diff=6.32961e-09
  0.000000	  0.000000	diff=1.58524e-16
-14.719207	-16.059530	diff=1.34032
 -0.000000	 -0.000000	diff=3.13726e-12
  0.000000	  0.000000	diff=8.59821e-15
  0.000000	  0.000000	diff=6.45012e-13
  0.000023	  0.000023	diff=1.39051e-07
  0.000000	 -0.000000	diff=1.4728e-14
 -0.000000	 -0.000000	diff=6.87464e-10
  0.000000	 -0.000000	diff=5.26565e-17
  5.461578	  5.334443	diff=0.127136
  0.000000	  0.000000	diff=7.61555e-13
  0.000000	  0.000000	diff=1.39333e-14
  0.000000	 -0.000000	diff=2.1767e-14
 -0.000002	 -0.000002	diff=1.4699e-08
  0.000000	 -0.000000	diff=6.52657e-15
  0.000000	  0.000000	diff=2.75589e-10
  0.000000	 -0.000000	diff=6.3286e-18
  0.449855	  0.448817	diff=0.00103736
  0.000000	  0.000000	diff=3.37162e-13
  0.000000	 -0.000000	diff=2.04339e-16
  0.000000	 -0.000000	diff=9.69317e-15
  0.000000	 -0.000000	diff=9.31368e-19
  0.000000	  0.000000	diff=7.69033e-30
  0.000000	 -0.000000	diff=1.65854e-21
  0.000000	  0.000000	diff=5.74242e-30
  0.000000	 -0.000000	diff=3.25273e-13
  0.000000	 -0.000000	diff=4.82839e-25
  0.000000	  0.000000	diff=1.29125e-27
  0.000000	  0.000000	diff=3.05427e-31
  local_diff=1.4685
# W_src{2}, [8 4]
 -0.087050	 -0.086739	diff=0.000311079
 -0.104555	 -0.104607	diff=5.24231e-05
  0.029830	  0.029799	diff=3.03083e-05
  0.040328	  0.040535	diff=0.000207567
 -0.723474	 -0.726745	diff=0.00327087
  0.013565	  0.013999	diff=0.000433388
 -0.464075	 -0.465157	diff=0.00108233
  0.022380	  0.022321	diff=5.89542e-05
  0.000000	  0.000000	diff=7.6751e-14
  0.000000	  0.000000	diff=1.15572e-13
 -0.000000	 -0.000000	diff=1.76551e-13
  0.000000	  0.000000	diff=7.59365e-14
  0.000000	  0.000000	diff=4.50201e-13
  0.000000	  0.000000	diff=1.48587e-13
  0.000000	  0.000000	diff=4.98398e-13
  0.000000	 -0.000000	diff=8.61398e-15
  0.024853	  0.024873	diff=2.0336e-05
 -0.196043	 -0.196245	diff=0.00020265
  0.038320	  0.038283	diff=3.70244e-05
 -0.026892	 -0.026901	diff=8.44311e-06
  0.078484	  0.078434	diff=5.0346e-05
 -0.007768	 -0.007788	diff=2.02332e-05
 -0.728843	 -0.731675	diff=0.00283175
 -0.003156	 -0.003150	diff=5.82749e-06
 -0.001543	 -0.001507	diff=3.61331e-05
  0.455753	  0.454661	diff=0.00109135
 -0.152166	 -0.152594	diff=0.000427629
  0.061099	  0.061046	diff=5.21822e-05
 -0.168752	 -0.169013	diff=0.000260484
  0.121133	  0.120961	diff=0.000172497
  2.228265	  2.210119	diff=0.0181459
  0.007276	  0.007307	diff=3.12223e-05
  local_diff=0.0288409
# W_tgt{1}, [8 6]
 -0.000000	 -0.000000	diff=6.2292e-09
  0.000015	  0.000016	diff=2.5298e-07
  0.060884	  0.060119	diff=0.00076484
  0.000000	  0.000000	diff=1.86329e-12
  0.000002	  0.000002	diff=2.64145e-08
 -0.000000	 -0.000000	diff=1.0273e-10
  0.000000	  0.000000	diff=2.44072e-13
  0.000000	  0.000000	diff=0
  0.000000	 -0.000000	diff=1.81289e-35
 -0.000015	 -0.000015	diff=2.38185e-07
  0.000000	 -0.000000	diff=1.44824e-14
  0.000000	  0.000000	diff=5.58153e-20
  0.000000	 -0.000000	diff=2.44671e-32
 -0.000000	 -0.000000	diff=8.16683e-09
 -0.000000	 -0.000000	diff=3.92424e-13
  0.000000	 -0.000000	diff=7.3573e-16
  0.000000	  0.000000	diff=1.01422e-11
  0.000001	  0.000001	diff=8.19778e-10
  0.000023	  0.000023	diff=2.11826e-10
  0.000000	  0.000000	diff=1.41327e-13
  0.000000	  0.000000	diff=2.13369e-13
  0.000000	  0.000000	diff=2.54746e-12
  0.000000	 -0.000000	diff=2.11583e-13
  0.000000	  0.000000	diff=1.11694e-17
  0.000000	 -0.000000	diff=5.03651e-13
  0.000000	  0.000000	diff=2.10087e-11
 -0.005170	 -0.005175	diff=5.67355e-06
  0.000000	  0.000000	diff=1.66012e-21
 -0.000000	 -0.000000	diff=1.91877e-10
 -0.000000	 -0.000000	diff=6.3643e-13
  0.000000	  0.000000	diff=1.59317e-15
  0.000000	  0.000000	diff=2.82346e-21
 -0.000000	 -0.000000	diff=1.31473e-14
  0.000000	  0.000000	diff=1.74292e-10
 -0.013961	 -0.014003	diff=4.15161e-05
  0.000000	  0.000000	diff=2.59621e-21
 -0.000000	 -0.000000	diff=1.40528e-09
  0.000000	  0.000000	diff=4.86693e-13
  0.000000	  0.000000	diff=2.80162e-65
  0.000000	 -0.000000	diff=2.44665e-19
  0.000000	  0.000000	diff=3.31683e-19
  0.000000	  0.000000	diff=4.92952e-17
 -0.000000	 -0.000000	diff=1.42868e-13
  0.000000	  0.000000	diff=6.672e-21
  0.000000	 -0.000000	diff=4.16656e-16
  0.000000	  0.000000	diff=6.1473e-19
  0.000000	  0.000000	diff=1.79699e-30
  0.000000	  0.000000	diff=7.94063e-28
  local_diff=0.000812564
# W_tgt{2}, [8 4]
 -0.046807	 -0.046992	diff=0.00018554
  0.899926	  0.894289	diff=0.00563694
 -0.000032	 -0.000032	diff=1.52434e-07
 -0.024813	 -0.024701	diff=0.000111815
 -0.004882	 -0.004857	diff=2.4318e-05
  0.014023	  0.014087	diff=6.35016e-05
  0.021521	  0.021319	diff=0.000201599
  1.452706	  1.442812	diff=0.00989439
 -0.000000	 -0.000000	diff=8.13848e-13
  0.000001	  0.000001	diff=1.73597e-13
  0.000000	  0.000000	diff=6.23192e-14
 -0.000000	 -0.000000	diff=2.83224e-13
 -0.000000	 -0.000000	diff=5.7713e-13
  0.000000	  0.000000	diff=3.69756e-13
  0.000000	  0.000000	diff=3.60351e-13
  0.000001	  0.000001	diff=6.53623e-14
  0.113675	  0.113696	diff=2.13061e-05
 -0.046499	 -0.046607	diff=0.00010751
 -0.036259	 -0.036266	diff=6.35102e-06
 -0.007899	 -0.007889	diff=9.69445e-06
  0.032595	  0.032362	diff=0.000232558
 -0.003303	 -0.003312	diff=8.68865e-06
  0.223440	  0.223403	diff=3.74709e-05
  0.406097	  0.405019	diff=0.00107796
 -0.064350	 -0.064522	diff=0.000172566
 -0.081778	 -0.081804	diff=2.56724e-05
 -0.006464	 -0.006486	diff=2.25768e-05
  0.104609	  0.104460	diff=0.000148211
 -0.086935	 -0.086992	diff=5.66762e-05
 -0.007792	 -0.007793	diff=1.58223e-07
 -0.071641	 -0.071809	diff=0.000168241
  0.784444	  0.764323	diff=0.0201207
  local_diff=0.0383346
# W_emb_src, [2 4]
  0.000000	 -0.000000	diff=1.9143e-13
  0.000000	  0.000000	diff=1.17505e-14
-13.315305	-14.387187	diff=1.07188
-37.421782	-49.692857	diff=12.2711
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  local_diff=13.343
# W_emb_tgt, [2 4]
  0.000000	  0.000000	diff=1.58251e-10
  0.000000	  0.000000	diff=1.26032e-09
  0.031711	  0.031501	diff=0.000209903
 -0.000010	 -0.000010	diff=9.55223e-08
  0.000084	  0.000085	diff=7.22911e-07
  0.000000	  0.000000	diff=5.10378e-13
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  local_diff=0.000210723
# W_soft, [4 2]
 -0.026666	 -0.027010	diff=0.000344792
 -0.058779	 -0.059143	diff=0.000363627
 -0.041872	 -0.042203	diff=0.000330914
  0.128739	  0.128356	diff=0.000382858
 -0.299723	 -0.299879	diff=0.00015595
 -0.097695	 -0.097855	diff=0.00016007
  0.430156	  0.429869	diff=0.000287884
 -0.031845	 -0.032135	diff=0.000289784
  local_diff=0.00231588
# Num params=168, abs_diff=14.882
Elapsed time is 1.303049 seconds.
[?1l>
## trainLSTM('', '', '', '', '', '', '', '../output/gradcheck', 'isGradCheck', 1, 'initRange', 10, 'isResume', 0, 'feedInput', 1, 'numLayers', 2, 'dropout', 0.8, 'isReverse', 1, 'attnFunc', 1, 'attnOpt', 1)
[?1h=
                                                                   < M A T L A B (R) >
                                                         Copyright 1984-2013 The MathWorks, Inc.
                                                            R2013a (8.1.0.604) 64-bit (maci64)
                                                                    February 15, 2013

 
To get started, type one of these: helpwin, helpdesk, or demo.
For product information, visit www.mathworks.com.
 

  Student License -- for use in conjunction with courses offered at a
  degree-granting institution.  Professional and commercial use prohibited.

## Bilingual setting
# Init LSTM parameters using dataType=double, initRange=10
  Model size = 176, individual sizes:  W_src{1}=32 W_src{2}=32 W_tgt{1}=48 W_tgt{2}=32 W_emb_src=8 W_emb_tgt=8 W_h=8 W_soft=8
# assert = 0
# attnFunc = 1
# attnOpt = 1
# batchSize = 10
# dataType = double
# debug = 0
# decode = 1
# dropout = 0.8
# epochFraction = 1
# feedInput = 1
# finetuneEpoch = 5
# finetuneRate = 0.5
# gpuDevice = 0
# initRange = 10
# isBi = 1
# isClip = 1
# isGradCheck = 1
# isProfile = 0
# isResume = 0
# isReverse = 1
# learningRate = 1
# loadModel = 
# logFreq = 10
# lstmOpt = 0
# lstmSize = 2
# maxGradNorm = 5
# maxLenRatio = 1.5
# maxSentLen = 7
# minLenRatio = 0.5
# numEpoches = 10
# numLayers = 2
# onlyCPU = 0
# outDir = ../output/gradcheck
# posWin = 1
# seed = 0
# shuffle = 1
# sortBatch = 1
# srcLang = 
# srcVocabFile = 
# testPrefix = 
# tgtLang = 
# tgtVocabFile = 
# trainPrefix = 
# validPrefix = 
# chunkSize = 12800
# baseIndex = 0
# clipForward = 50
# clipBackward = 1000
# nonlinear_gate_f = sigmoid
# nonlinear_gate_f_prime = sigmoidPrime
# nonlinear_f = tanh
# nonlinear_f_prime = tanhPrime
# beamSize = 12
# stackSize = 100
# unkPenalty = 0
# lengthReward = 0
# isGPU = 0
# batchId = 1
# maxRelDist = 2
# logId = 3
# srcSos = 3
# srcEos = 4
# srcVocabSize = 4
# nullPosId = 0
# tgtSos = 3
# tgtEos = 4
# tgtVocabSize = 4
# modelFile = ../output/gradcheck/model.mat
# modelRecentFile = ../output/gradcheck/modelRecent.mat
# softmaxSize = 2
# lr = 1
# epoch = 1
# bestCostValid = 100000
# testPerplexity = 100000
# curTestPerpWord = 100000
# startIter = 0
# iter = 0
# epochBatchCount = 0
# finetuneCount = 0
# modelSize = 176
  src input 1: <s_sos> y x x
  src mask: 0  1  1  1  1
  tgt input 1: <t_sos> a b <t_eos> <t_eos>
  tgt output 1: a b <t_eos> <t_eos> <t_eos>
  tgt mask: 1  1  1  0  0
# W_src{1}, [8 4]
 -0.000417	 -0.000425	diff=7.94596e-06
  0.000000	 -0.000000	diff=8.63376e-19
  0.000009	  0.000009	diff=1.61329e-07
  0.000000	  0.000000	diff=5.83317e-19
-10.688819	-11.015800	diff=0.326982
 -0.000000	 -0.000000	diff=8.95056e-12
  0.000000	  0.000000	diff=3.9748e-12
  0.000000	  0.000000	diff=2.29572e-21
  0.000142	  0.000141	diff=8.0421e-07
  0.000000	  0.000000	diff=2.86785e-19
 -0.000003	 -0.000003	diff=1.93066e-08
  0.000000	 -0.000000	diff=1.93758e-19
  3.693816	  3.659078	diff=0.034738
 -0.000000	 -0.000000	diff=1.64674e-11
  0.000000	  0.000000	diff=7.13174e-12
  0.000000	  0.000000	diff=4.15715e-24
 -0.000087	 -0.000088	diff=3.73077e-07
  0.000000	 -0.000000	diff=1.7802e-19
  0.000002	  0.000002	diff=6.12189e-09
  0.000000	  0.000000	diff=1.20274e-19
 -2.256408	 -2.271347	diff=0.0149391
  0.000000	  0.000000	diff=3.49122e-13
 -0.000000	 -0.000000	diff=1.50104e-12
  0.000000	 -0.000000	diff=2.58052e-24
  0.000000	  0.000000	diff=1.28664e-18
  0.000000	 -0.000000	diff=5.93891e-36
  0.000000	 -0.000000	diff=2.67926e-20
  0.000000	  0.000000	diff=8.98286e-37
  0.000000	  0.000000	diff=3.33479e-14
  0.000000	 -0.000000	diff=2.62458e-31
  0.000000	 -0.000000	diff=2.89061e-29
  0.000000	 -0.000000	diff=8.60886e-41
  local_diff=0.376668
# W_src{2}, [8 4]
 -1.813149	 -1.911445	diff=0.0982961
 -0.108651	 -0.108147	diff=0.000504014
  0.029615	  0.029436	diff=0.000178485
  4.759760	  4.785103	diff=0.0253429
  0.033907	  0.035616	diff=0.00170905
  3.176557	  3.166330	diff=0.0102267
 -0.000001	 -0.000001	diff=6.03679e-08
 -0.000658	 -0.000653	diff=4.88175e-06
  0.000000	  0.000000	diff=8.31188e-13
  0.000000	  0.000000	diff=2.05896e-15
  0.000000	 -0.000000	diff=8.08868e-14
 -0.000000	 -0.000000	diff=2.70001e-13
  0.000000	  0.000000	diff=5.34387e-14
  0.000000	  0.000000	diff=1.7214e-13
  0.000000	  0.000000	diff=2.22651e-18
  0.000000	 -0.000000	diff=8.23585e-20
  0.087994	  0.086658	diff=0.00133623
  0.002102	  0.002102	diff=3.8204e-07
 -0.001560	 -0.001561	diff=9.6419e-07
 -0.251615	 -0.251545	diff=7.02038e-05
 -0.000032	 -0.000142	diff=0.000110464
 -0.061516	 -0.061628	diff=0.000111364
  0.000000	  0.000000	diff=4.7988e-10
  0.000013	  0.000013	diff=6.64773e-09
 -1.301019	 -1.304867	diff=0.00384826
 -0.000547	 -0.000545	diff=1.90256e-06
  0.021800	  0.021724	diff=7.5804e-05
  3.493928	  3.507554	diff=0.0136255
 -0.014342	 -0.014392	diff=4.99072e-05
 -0.046342	 -0.046182	diff=0.000160542
 -0.000001	 -0.000001	diff=4.15428e-09
  0.000000	  0.000000	diff=7.5445e-11
  local_diff=0.155654
# W_tgt{1}, [8 6]
  0.000000	 -0.000000	diff=1.76276e-13
  0.000128	  0.000131	diff=2.113e-06
 -0.020619	 -0.020281	diff=0.00033833
  0.000000	 -0.000000	diff=6.34092e-19
  0.000000	 -0.000000	diff=4.45544e-19
  0.000000	  0.000000	diff=1.08054e-12
 -0.000000	 -0.000000	diff=3.07824e-12
  0.000000	  0.000000	diff=8.48812e-92
  0.000000	  0.000000	diff=2.12431e-11
 -0.027462	 -0.030471	diff=0.00300868
  0.019098	  0.019401	diff=0.000302727
 -0.000000	 -0.000000	diff=1.40156e-11
 -0.000000	 -0.000000	diff=7.4753e-12
 -0.003947	 -0.004052	diff=0.000104986
 -0.000000	 -0.000000	diff=4.53969e-09
  2.012779	  1.654475	diff=0.358304
 -0.000000	 -0.000000	diff=6.7009e-13
  0.001622	  0.001614	diff=7.61688e-06
 -0.000000	 -0.000000	diff=4.56066e-11
 -0.000000	 -0.000000	diff=4.26651e-13
 -0.000000	 -0.000000	diff=1.25478e-12
  0.000207	  0.000206	diff=2.64438e-07
  0.000000	  0.000000	diff=9.53485e-12
  0.027006	  0.026916	diff=8.94354e-05
 -0.000000	 -0.000000	diff=3.72173e-13
  0.006999	  0.006855	diff=0.000143943
 -0.004644	 -0.004627	diff=1.74548e-05
 -0.000000	 -0.000000	diff=1.57491e-13
 -0.000000	 -0.000000	diff=4.79931e-13
  0.000892	  0.000887	diff=4.97041e-06
  0.000000	  0.000000	diff=2.19891e-10
  0.017234	  0.017197	diff=3.64972e-05
 -0.000000	 -0.000000	diff=1.25189e-13
 -0.000040	 -0.000040	diff=1.58025e-07
  0.004673	  0.004691	diff=1.79246e-05
  0.000000	  0.000000	diff=3.63931e-13
  0.000000	 -0.000000	diff=1.13629e-14
 -0.000000	 -0.000000	diff=4.10669e-10
  0.000000	  0.000000	diff=3.37463e-17
  0.000000	  0.000000	diff=1.13925e-09
  0.000000	  0.000000	diff=8.97245e-13
  0.000006	  0.000006	diff=3.6637e-09
  0.000001	  0.000001	diff=5.70024e-10
  0.000000	 -0.000000	diff=1.61224e-13
  0.000000	  0.000000	diff=1.73336e-15
  0.000000	  0.000000	diff=9.34631e-12
  0.000000	 -0.000000	diff=1.63666e-17
 -0.000000	 -0.000000	diff=2.63946e-11
  local_diff=0.362379
# W_tgt{2}, [8 4]
 -0.005609	 -0.005548	diff=6.12318e-05
 15.601504	 15.645669	diff=0.0441653
  0.000045	  0.000045	diff=2.10182e-07
  0.542629	  0.540448	diff=0.00218169
 -0.035162	 -0.034977	diff=0.000184637
  0.014483	  0.013987	diff=0.000495926
  0.001460	  0.001447	diff=1.27022e-05
 58.954702	 59.866335	diff=0.911632
 -0.011191	 -0.011138	diff=5.30032e-05
  0.000820	  0.000821	diff=8.17233e-08
 -0.000029	 -0.000029	diff=9.05399e-08
  0.000130	  0.000129	diff=4.99191e-07
 -0.011941	 -0.011886	diff=5.52257e-05
  0.004172	  0.004052	diff=0.000120223
 -0.000571	 -0.000566	diff=5.28759e-06
 -0.000738	 -0.000737	diff=1.03973e-06
  0.055186	  0.055013	diff=0.000172813
 -0.199543	 -0.199931	diff=0.000387869
 -0.011285	 -0.011251	diff=3.38641e-05
  0.237925	  0.239097	diff=0.00117201
  0.062794	  0.062800	diff=5.63728e-06
  0.008301	  0.008309	diff=7.73758e-06
  0.043058	  0.042678	diff=0.000380009
 20.094337	 20.122419	diff=0.0280819
 -0.176576	 -0.176200	diff=0.000375698
  0.721573	  0.721825	diff=0.000252575
 -0.018177	 -0.018111	diff=6.55712e-05
  1.320871	  1.317796	diff=0.00307505
 -0.001880	 -0.001908	diff=2.85862e-05
  0.004889	  0.004989	diff=0.0001001
  0.015874	  0.015775	diff=9.85287e-05
 16.337336	 15.641600	diff=0.695736
  local_diff=1.68894
# W_emb_src, [2 4]
 -0.000066	 -0.000068	diff=1.10601e-06
  0.000000	  0.000000	diff=1.41778e-11
 -9.605973	 -9.869441	diff=0.263468
-31.109855	-34.087230	diff=2.97738
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  local_diff=3.24084
# W_emb_tgt, [2 4]
  0.000000	 -0.000000	diff=5.77495e-15
 -0.000001	 -0.000001	diff=1.77457e-08
 -0.010975	 -0.010872	diff=0.000102879
  0.164485	  0.158929	diff=0.00555612
  0.000001	  0.000001	diff=1.08213e-08
 -0.000001	 -0.000001	diff=4.45282e-09
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  local_diff=0.00565903
# W_h, [2 4]
  1.481862	  1.481040	diff=0.000821426
  1.005735	  1.007247	diff=0.00151167
  7.182414	  7.179102	diff=0.0033123
  0.754340	  0.742068	diff=0.0122719
  3.660822	  3.580372	diff=0.08045
  1.307084	  1.263106	diff=0.0439777
  2.304253	  2.305433	diff=0.00118011
  6.071239	  6.071374	diff=0.000134804
  local_diff=0.14366
# W_soft, [4 2]
 -3.986266	 -3.989139	diff=0.0028725
  0.720294	  0.720191	diff=0.000103346
  3.904612	  3.903758	diff=0.000854314
 -0.632447	 -0.634810	diff=0.00236214
 -4.939213	 -4.943083	diff=0.00387017
  3.191459	  3.191370	diff=8.97239e-05
  1.158195	  1.157970	diff=0.000224802
  0.597443	  0.593744	diff=0.00369893
  local_diff=0.0140759
# Num params=176, abs_diff=5.98788
Elapsed time is 2.005500 seconds.
[?1l>
## trainLSTM('', '', '', '', '', '', '', '../output/gradcheck', 'isGradCheck', 1, 'initRange', 10, 'isResume', 0, 'feedInput', 1, 'numLayers', 2, 'dropout', 0.8, 'isReverse', 1, 'attnFunc', 2, 'attnOpt', 1)
[?1h=
                                                                   < M A T L A B (R) >
                                                         Copyright 1984-2013 The MathWorks, Inc.
                                                            R2013a (8.1.0.604) 64-bit (maci64)
                                                                    February 15, 2013

 
To get started, type one of these: helpwin, helpdesk, or demo.
For product information, visit www.mathworks.com.
 

  Student License -- for use in conjunction with courses offered at a
  degree-granting institution.  Professional and commercial use prohibited.

## Bilingual setting
# Init LSTM parameters using dataType=double, initRange=10
  Model size = 176, individual sizes:  W_src{1}=32 W_src{2}=32 W_tgt{1}=48 W_tgt{2}=32 W_emb_src=8 W_emb_tgt=8 W_h=8 W_soft=8
# assert = 0
# attnFunc = 2
# attnOpt = 1
# batchSize = 10
# dataType = double
# debug = 0
# decode = 1
# dropout = 0.8
# epochFraction = 1
# feedInput = 1
# finetuneEpoch = 5
# finetuneRate = 0.5
# gpuDevice = 0
# initRange = 10
# isBi = 1
# isClip = 1
# isGradCheck = 1
# isProfile = 0
# isResume = 0
# isReverse = 1
# learningRate = 1
# loadModel = 
# logFreq = 10
# lstmOpt = 0
# lstmSize = 2
# maxGradNorm = 5
# maxLenRatio = 1.5
# maxSentLen = 7
# minLenRatio = 0.5
# numEpoches = 10
# numLayers = 2
# onlyCPU = 0
# outDir = ../output/gradcheck
# posWin = 1
# seed = 0
# shuffle = 1
# sortBatch = 1
# srcLang = 
# srcVocabFile = 
# testPrefix = 
# tgtLang = 
# tgtVocabFile = 
# trainPrefix = 
# validPrefix = 
# chunkSize = 12800
# baseIndex = 0
# clipForward = 50
# clipBackward = 1000
# nonlinear_gate_f = sigmoid
# nonlinear_gate_f_prime = sigmoidPrime
# nonlinear_f = tanh
# nonlinear_f_prime = tanhPrime
# beamSize = 12
# stackSize = 100
# unkPenalty = 0
# lengthReward = 0
# isGPU = 0
# batchId = 1
# maxRelDist = 2
# logId = 3
# srcSos = 3
# srcEos = 4
# srcVocabSize = 4
# nullPosId = 0
# tgtSos = 3
# tgtEos = 4
# tgtVocabSize = 4
# modelFile = ../output/gradcheck/model.mat
# modelRecentFile = ../output/gradcheck/modelRecent.mat
# softmaxSize = 2
# lr = 1
# epoch = 1
# bestCostValid = 100000
# testPerplexity = 100000
# curTestPerpWord = 100000
# startIter = 0
# iter = 0
# epochBatchCount = 0
# finetuneCount = 0
# modelSize = 176
  src input 1: <s_sos> y x x
  src mask: 0  1  1  1  1
  tgt input 1: <t_sos> a b <t_eos> <t_eos>
  tgt output 1: a b <t_eos> <t_eos> <t_eos>
  tgt mask: 1  1  1  0  0
# W_src{1}, [8 4]
  0.000061	  0.000113	diff=5.1148e-05
  0.000000	  0.000000	diff=3.00703e-20
 -0.000001	 -0.000002	diff=1.017e-06
  0.000000	 -0.000000	diff=2.03751e-20
  1.598115	  2.916009	diff=1.31789
  0.000000	  0.000000	diff=9.90285e-13
 -0.000000	 -0.000000	diff=1.3359e-12
  0.000000	  0.000000	diff=3.47409e-23
 -0.000021	 -0.000037	diff=1.6483e-05
  0.000000	 -0.000000	diff=9.98835e-21
  0.000000	  0.000001	diff=3.4854e-07
  0.000000	  0.000000	diff=6.76793e-21
 -0.540478	 -0.968686	diff=0.428208
  0.000000	  0.000000	diff=1.9043e-12
 -0.000000	 -0.000000	diff=6.80735e-13
  0.000000	 -0.000000	diff=1.44788e-25
  0.000013	  0.000023	diff=1.036e-05
  0.000000	  0.000000	diff=6.2002e-21
 -0.000000	 -0.000000	diff=2.13693e-07
  0.000000	 -0.000000	diff=4.20114e-21
  0.333147	  0.601305	diff=0.268158
 -0.000000	 -0.000000	diff=3.91777e-13
  0.000000	  0.000000	diff=1.27038e-12
  0.000000	  0.000000	diff=8.98762e-26
  0.000000	 -0.000000	diff=1.1966e-19
  0.000000	  0.000000	diff=8.21273e-37
  0.000000	  0.000000	diff=2.49175e-21
  0.000000	 -0.000000	diff=2.3831e-37
  0.000000	 -0.000000	diff=3.10141e-15
  0.000000	  0.000000	diff=3.43949e-31
  0.000000	  0.000000	diff=2.68831e-30
  0.000000	  0.000000	diff=1.19049e-41
  local_diff=2.01434
# W_src{2}, [8 4]
 -0.617718	 -0.738213	diff=0.120495
  0.021043	  0.018956	diff=0.00208715
 -0.002601	 -0.002649	diff=4.73296e-05
 -0.485520	 -0.495027	diff=0.00950676
 -0.029864	  0.125486	diff=0.155349
 -0.163171	 -0.334279	diff=0.171108
 -0.000000	 -0.000002	diff=1.50135e-06
  0.000187	  0.000172	diff=1.48483e-05
 -0.000000	 -0.000000	diff=1.03646e-12
  0.000000	 -0.000000	diff=1.79019e-16
  0.000000	  0.000000	diff=6.48523e-15
  0.000000	  0.000000	diff=1.35094e-12
  0.000000	 -0.000000	diff=4.27738e-15
  0.000000	 -0.000000	diff=1.4091e-14
  0.000000	 -0.000000	diff=1.78283e-19
  0.000000	  0.000000	diff=1.863e-20
 -0.014843	 -0.016756	diff=0.00191293
 -0.000660	 -0.000614	diff=4.64542e-05
  0.000122	  0.000121	diff=6.16488e-07
  0.026047	  0.026021	diff=2.64204e-05
 -0.001424	 -0.004853	diff=0.00342901
  0.008857	  0.012502	diff=0.00364479
  0.000000	  0.000000	diff=3.11843e-08
 -0.000004	 -0.000004	diff=2.89489e-07
  0.104834	  0.104629	diff=0.000205438
  0.000057	  0.000057	diff=2.28448e-07
 -0.001748	 -0.001742	diff=5.93298e-06
 -0.357719	 -0.362831	diff=0.00511254
  0.001188	  0.001194	diff=6.41308e-06
  0.003774	  0.003757	diff=1.69209e-05
  0.000000	  0.000000	diff=3.11586e-10
  0.000000	  0.000000	diff=1.91446e-10
  local_diff=0.473017
# W_tgt{1}, [8 6]
  0.000000	 -0.000000	diff=3.64521e-13
  0.001202	  0.001222	diff=1.99971e-05
 -0.013150	 -0.012935	diff=0.000214829
  0.000000	 -0.000000	diff=1.89698e-18
  0.000000	 -0.000000	diff=4.02153e-16
  0.000000	  0.000000	diff=1.81398e-13
 -0.000000	 -0.000000	diff=6.9467e-13
  0.000000	  0.000000	diff=1.22419e-83
 -0.000000	 -0.000000	diff=5.37897e-13
  0.002945	  0.003028	diff=8.28415e-05
  0.012190	  0.012383	diff=0.000192325
 -0.000000	 -0.000000	diff=1.27508e-11
 -0.000000	 -0.000000	diff=3.27605e-11
  0.000248	  0.000252	diff=3.98536e-06
  0.000000	  0.000000	diff=1.64638e-09
  0.602204	  0.332424	diff=0.26978
 -0.000000	 -0.000000	diff=6.11596e-13
  0.000349	  0.000350	diff=1.03004e-06
 -0.000000	 -0.000000	diff=5.50529e-11
 -0.000000	 -0.000000	diff=6.38376e-13
 -0.000000	 -0.000000	diff=2.25832e-13
  0.000005	  0.000005	diff=1.51635e-09
  0.000000	  0.000000	diff=1.24275e-12
 -0.002118	 -0.002188	diff=6.93002e-05
 -0.000000	 -0.000000	diff=3.86756e-13
  0.000060	  0.000060	diff=1.62704e-08
 -0.004019	 -0.003999	diff=2.03726e-05
 -0.000000	 -0.000000	diff=1.01096e-12
 -0.000000	 -0.000000	diff=5.25207e-13
  0.000004	  0.000004	diff=9.84273e-10
  0.000000	  0.000000	diff=1.04065e-12
 -0.008173	 -0.008193	diff=2.05895e-05
 -0.000000	 -0.000000	diff=4.00593e-13
 -0.000040	 -0.000040	diff=1.59395e-07
  0.002978	  0.002990	diff=1.1395e-05
  0.000000	  0.000000	diff=3.51646e-13
  0.000000	 -0.000000	diff=1.09986e-14
 -0.000000	 -0.000000	diff=4.6098e-10
  0.000000	  0.000000	diff=3.16724e-17
  0.000000	  0.000000	diff=1.13804e-09
  0.000000	  0.000000	diff=7.27476e-13
  0.000006	  0.000006	diff=3.89407e-09
  0.000001	  0.000001	diff=6.06911e-10
  0.000000	 -0.000000	diff=1.67412e-13
  0.000000	  0.000000	diff=1.72219e-15
  0.000000	  0.000000	diff=1.09513e-11
  0.000000	 -0.000000	diff=1.88697e-17
 -0.000000	 -0.000000	diff=2.88829e-11
  local_diff=0.270417
# W_tgt{2}, [8 4]
 -0.175210	 -0.175110	diff=0.000100706
  9.597680	  9.485425	diff=0.112255
 -0.000035	 -0.000034	diff=1.64564e-07
  0.267411	  0.266341	diff=0.00106973
 -0.033202	 -0.033027	diff=0.000174488
 -0.189598	 -0.190844	diff=0.00124634
 -0.012200	 -0.012085	diff=0.000115511
 33.009175	 32.911640	diff=0.0975347
 -0.063497	 -0.063442	diff=5.55872e-05
 -0.004914	 -0.004906	diff=7.90412e-06
 -0.000011	 -0.000011	diff=1.51054e-08
  0.000092	  0.000092	diff=4.49994e-07
 -0.013243	 -0.013186	diff=5.71214e-05
 -0.043948	 -0.044045	diff=9.66596e-05
 -0.004409	 -0.004393	diff=1.59736e-05
 -0.006678	 -0.006660	diff=1.81012e-05
  0.097839	  0.097477	diff=0.000361606
 -0.234949	 -0.234980	diff=3.07385e-05
 -0.019669	 -0.019581	diff=8.8299e-05
  0.009509	  0.009371	diff=0.000137478
  0.039917	  0.039969	diff=5.14222e-05
  0.001465	  0.001314	diff=0.00015135
  0.008065	  0.007753	diff=0.0003117
 -1.858609	 -2.015357	diff=0.156747
 -0.175691	 -0.175771	diff=8.04325e-05
  0.214071	  0.214326	diff=0.000254952
  0.010279	  0.010258	diff=2.04712e-05
  0.082892	  0.083836	diff=0.000943297
 -0.025454	 -0.025494	diff=3.91905e-05
 -0.110080	 -0.110173	diff=9.31544e-05
  0.062606	  0.062556	diff=4.99121e-05
  4.774354	  4.777763	diff=0.00340948
  local_diff=0.375519
# W_emb_src, [2 4]
 -0.000013	 -0.000013	diff=2.16719e-07
 -0.000000	 -0.000000	diff=6.70342e-13
  1.433897	  2.612586	diff=1.17869
  4.797114	  9.024081	diff=4.22697
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  local_diff=5.40566
# W_emb_tgt, [2 4]
  0.000000	  0.000000	diff=6.97205e-16
 -0.000001	 -0.000001	diff=1.84173e-08
 -0.009699	 -0.009533	diff=0.000166086
  0.045445	  0.042139	diff=0.00330592
  0.000000	  0.000000	diff=2.67387e-09
 -0.000000	 -0.000000	diff=1.49441e-09
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  local_diff=0.00347202
# W_h, [2 4]
  0.109902	  0.108277	diff=0.00162507
  0.196290	  0.194484	diff=0.00180629
 11.726673	 11.712820	diff=0.0138525
 -2.404595	 -2.406832	diff=0.0022372
  6.535793	  6.388006	diff=0.147787
  6.748657	  6.600512	diff=0.148145
  1.739624	  1.740216	diff=0.000592443
  1.253463	  1.248681	diff=0.00478228
  local_diff=0.320828
# W_soft, [4 2]
 -2.177200	 -2.180135	diff=0.00293497
  1.585059	  1.584995	diff=6.40285e-05
  1.479411	  1.478612	diff=0.000799475
 -0.881152	 -0.883472	diff=0.00232
 -5.971407	 -5.975791	diff=0.00438437
  4.362419	  4.362344	diff=7.49875e-05
  0.248131	  0.247969	diff=0.000162132
  1.369741	  1.365478	diff=0.00426361
  local_diff=0.0150036
# Num params=176, abs_diff=8.87825
Elapsed time is 2.431986 seconds.
[?1l>
## trainLSTM('', '', '', '', '', '', '', '../output/gradcheck', 'isGradCheck', 1, 'initRange', 10, 'isResume', 0, 'feedInput', 1, 'numLayers', 2, 'dropout', 0.8, 'isReverse', 1, 'attnFunc', 4, 'attnOpt', 1)
[?1h=
                                                                   < M A T L A B (R) >
                                                         Copyright 1984-2013 The MathWorks, Inc.
                                                            R2013a (8.1.0.604) 64-bit (maci64)
                                                                    February 15, 2013

 
To get started, type one of these: helpwin, helpdesk, or demo.
For product information, visit www.mathworks.com.
 

  Student License -- for use in conjunction with courses offered at a
  degree-granting institution.  Professional and commercial use prohibited.

## Bilingual setting
# Init LSTM parameters using dataType=double, initRange=10
  Model size = 182, individual sizes:  W_src{1}=32 W_src{2}=32 W_tgt{1}=48 W_tgt{2}=32 W_emb_src=8 W_emb_tgt=8 W_pos=4 v_pos=2 W_h=8 W_soft=8
# assert = 0
# attnFunc = 4
# attnOpt = 1
# batchSize = 10
# dataType = double
# debug = 0
# decode = 1
# dropout = 0.8
# epochFraction = 1
# feedInput = 1
# finetuneEpoch = 5
# finetuneRate = 0.5
# gpuDevice = 0
# initRange = 10
# isBi = 1
# isClip = 1
# isGradCheck = 1
# isProfile = 0
# isResume = 0
# isReverse = 1
# learningRate = 1
# loadModel = 
# logFreq = 10
# lstmOpt = 0
# lstmSize = 2
# maxGradNorm = 5
# maxLenRatio = 1.5
# maxSentLen = 7
# minLenRatio = 0.5
# numEpoches = 10
# numLayers = 2
# onlyCPU = 0
# outDir = ../output/gradcheck
# posWin = 1
# seed = 0
# shuffle = 1
# sortBatch = 1
# srcLang = 
# srcVocabFile = 
# testPrefix = 
# tgtLang = 
# tgtVocabFile = 
# trainPrefix = 
# validPrefix = 
# chunkSize = 12800
# baseIndex = 0
# clipForward = 50
# clipBackward = 1000
# nonlinear_gate_f = sigmoid
# nonlinear_gate_f_prime = sigmoidPrime
# nonlinear_f = tanh
# nonlinear_f_prime = tanhPrime
# beamSize = 12
# stackSize = 100
# unkPenalty = 0
# lengthReward = 0
# isGPU = 0
# batchId = 1
# maxRelDist = 2
# distSigma = 0.5
# logId = 3
# srcSos = 3
# srcEos = 4
# srcVocabSize = 4
# nullPosId = 0
# tgtSos = 3
# tgtEos = 4
# tgtVocabSize = 4
# modelFile = ../output/gradcheck/model.mat
# modelRecentFile = ../output/gradcheck/modelRecent.mat
# softmaxSize = 2
# lr = 1
# epoch = 1
# bestCostValid = 100000
# testPerplexity = 100000
# curTestPerpWord = 100000
# startIter = 0
# iter = 0
# epochBatchCount = 0
# finetuneCount = 0
# modelSize = 182
  src input 1: x x x x
  src mask: 1  1  1  1  1
  tgt input 1: <t_sos> a b <t_eos> <t_eos>
  tgt output 1: a b <t_eos> <t_eos> <t_eos>
  tgt mask: 1  1  1  0  0
# W_src{1}, [8 4]
 -0.000295	 -0.000313	diff=1.76902e-05
  0.000000	  0.000000	diff=2.41807e-09
  0.000004	  0.000004	diff=4.36945e-08
  0.000000	 -0.000000	diff=3.92814e-21
-65.832249	-48.581511	diff=17.2507
 -0.000000	 -0.000000	diff=2.15657e-11
  0.000000	  0.000000	diff=7.5063e-14
  0.000000	  0.000000	diff=5.63736e-11
  0.000101	  0.000104	diff=3.43844e-06
 -0.000426	 -0.000430	diff=4.08788e-06
 -0.000001	 -0.000001	diff=1.51134e-08
 -0.000974	 -0.000974	diff=3.96997e-07
 13.856624	 16.137192	diff=2.28057
 29.523318	 23.941825	diff=5.58149
  0.000000	  0.000000	diff=1.08882e-13
 -0.000000	 -0.000000	diff=1.26495e-11
 -0.000033	 -0.000033	diff=3.51793e-07
 -0.000155	 -0.000154	diff=1.4422e-06
  0.000001	  0.000001	diff=2.02617e-09
  0.000610	  0.000605	diff=5.63081e-06
  1.065368	  1.080224	diff=0.0148559
 -0.002381	 -0.002390	diff=8.20129e-06
  0.000000	 -0.000000	diff=2.11296e-15
  0.000000	  0.000000	diff=2.34542e-12
  0.000000	  0.000000	diff=5.70628e-19
  0.000024	  0.000024	diff=1.18843e-07
  0.000000	 -0.000000	diff=1.18825e-20
 -0.000094	 -0.000094	diff=4.68919e-07
  0.000000	  0.000000	diff=1.48375e-14
  0.000368	  0.000371	diff=2.89684e-06
  0.000000	  0.000000	diff=2.6593e-27
  0.000000	  0.000000	diff=1.24849e-16
  local_diff=25.1277
# W_src{2}, [8 4]
 -0.693846	 -0.648255	diff=0.0455913
  0.259314	  0.283403	diff=0.024089
  0.060128	  0.060445	diff=0.000317335
  0.193660	  0.191226	diff=0.00243377
 -2.450907	 -2.330699	diff=0.120208
  1.471314	  1.623160	diff=0.151846
 -0.309191	 -0.337271	diff=0.0280803
  0.228672	  0.248441	diff=0.0197692
  0.010205	  0.009870	diff=0.000334836
 -0.298153	 -0.295459	diff=0.00269437
  0.000011	  0.000011	diff=8.39963e-08
  0.000096	  0.000096	diff=6.96066e-07
 -0.001343	 -0.001516	diff=0.000173468
 -0.295095	 -0.292690	diff=0.00240474
  0.140384	  0.133462	diff=0.00692225
  1.500928	  1.463867	diff=0.0370613
  0.364428	  0.364084	diff=0.000344671
  0.032760	  0.032192	diff=0.000567773
  0.049679	  0.050198	diff=0.000519711
 -0.017033	 -0.016711	diff=0.000321676
  0.050207	  0.050528	diff=0.000320865
  0.171611	  0.169709	diff=0.00190207
 -0.569016	 -0.572543	diff=0.00352704
 -0.192158	 -0.189530	diff=0.00262755
 -0.848576	 -0.847907	diff=0.000668799
 -0.044314	 -0.043236	diff=0.00107831
 -0.806002	 -0.814455	diff=0.00845366
  0.052380	  0.051688	diff=0.000691995
 -0.011992	 -0.011856	diff=0.000135635
 -0.262693	 -0.259526	diff=0.00316707
  6.884831	  6.986603	diff=0.101772
  0.371064	  0.363539	diff=0.00752535
  local_diff=0.575552
# W_tgt{1}, [8 6]
  0.000000	  0.000000	diff=1.11185e-12
  1.785661	  1.853746	diff=0.0680853
 -0.001260	 -0.001276	diff=1.51748e-05
  0.000000	 -0.000000	diff=2.39212e-18
  0.000000	  0.000000	diff=3.54792e-45
  0.000000	  0.000000	diff=4.74795e-12
 -0.000000	 -0.000000	diff=1.29387e-12
  0.000000	 -0.000000	diff=4.10622e-17
 -0.000000	 -0.000000	diff=4.22803e-13
 -1.735780	 -1.770629	diff=0.0348488
  0.001166	  0.001218	diff=5.25759e-05
  0.000000	  0.000000	diff=2.28486e-18
  0.000000	 -0.000000	diff=7.66277e-37
 -0.000000	 -0.000000	diff=5.40956e-10
  0.000000	  0.000000	diff=2.7281e-12
  0.000000	  0.000000	diff=3.72506e-17
 -0.000127	 -0.000125	diff=1.53538e-06
  0.811418	  0.852544	diff=0.0411262
  0.073130	  0.073462	diff=0.000331705
 -0.000037	 -0.000036	diff=9.45071e-07
  0.000602	  0.000594	diff=8.63926e-06
  0.367604	  0.389383	diff=0.0217792
  0.000185	  0.000182	diff=2.74732e-06
 -0.000000	 -0.000000	diff=1.2084e-11
 -0.000118	 -0.000116	diff=1.48392e-06
  0.087442	  0.104132	diff=0.01669
 -0.117261	 -0.111303	diff=0.00595752
 -0.000016	 -0.000016	diff=4.37966e-07
 -0.000903	 -0.000882	diff=2.11929e-05
  0.352498	  0.372466	diff=0.0199675
  0.000172	  0.000169	diff=2.60195e-06
 -0.000000	 -0.000000	diff=1.0197e-11
 -0.011240	 -0.010194	diff=0.00104598
  0.044856	  0.071187	diff=0.026331
 -0.093977	 -0.088854	diff=0.00512245
 -0.366369	 -0.344758	diff=0.0216113
  0.166146	  0.169339	diff=0.0031932
  0.029883	  0.032094	diff=0.00221141
  0.004986	  0.004540	diff=0.00044673
 -0.206328	 -0.199062	diff=0.00726552
  0.000026	  0.000026	diff=5.17404e-07
  0.001973	  0.001739	diff=0.000234732
  0.010783	  0.010623	diff=0.000159527
  0.000012	  0.000012	diff=1.69857e-07
  0.000084	  0.000083	diff=1.47892e-06
  0.257611	  0.264461	diff=0.00684977
 -0.000007	 -0.000007	diff=1.38437e-07
  0.000000	  0.000000	diff=1.48852e-13
  local_diff=0.283368
# W_tgt{2}, [8 4]
  0.334744	  0.463025	diff=0.128282
 -0.039668	 -0.090034	diff=0.0503658
 -0.007213	 -0.007700	diff=0.000487345
  0.163312	  0.164379	diff=0.00106746
  0.012933	  0.011936	diff=0.000996449
 -0.116344	 -0.116945	diff=0.000601158
 -0.208550	 -0.200525	diff=0.00802446
  5.278429	  7.888029	diff=2.6096
 -0.111843	 -0.104160	diff=0.00768329
  0.011780	  0.011982	diff=0.000202706
  0.000385	  0.000429	diff=4.40187e-05
  0.002211	  0.002252	diff=4.12249e-05
 -0.000681	 -0.000677	diff=3.85134e-06
  0.006938	  0.007078	diff=0.000140376
 -0.000375	 -0.000419	diff=4.41679e-05
  0.054505	  0.056237	diff=0.00173219
 -0.046825	  0.006321	diff=0.0531462
 -0.548776	 -0.549606	diff=0.000829279
 -0.315841	 -0.311299	diff=0.00454186
  0.237950	  0.195967	diff=0.0419826
 -0.771617	 -0.778357	diff=0.00674031
 -0.118394	 -0.121418	diff=0.00302462
 -0.503322	 -0.487602	diff=0.0157193
  3.987222	  3.759802	diff=0.22742
  0.873207	  0.816784	diff=0.0564221
 -0.030862	 -0.053974	diff=0.0231124
  0.335196	  0.316250	diff=0.0189456
  0.090177	  0.180623	diff=0.0904454
  0.690906	  0.668683	diff=0.0222239
  0.023078	  0.036770	diff=0.0136923
  0.334246	  0.321691	diff=0.0125542
  2.624292	  2.863435	diff=0.239143
  local_diff=3.63926
# W_emb_src, [2 4]
  0.000081	  0.000082	diff=1.12023e-06
 -1.507716	 -1.511069	diff=0.00335284
-56.761916	-43.522841	diff=13.2391
-326.235165	-150.312803	diff=175.922
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  local_diff=189.165
# W_emb_tgt, [2 4]
 -0.000000	 -0.000000	diff=2.51867e-10
  0.000000	  0.000000	diff=1.04277e-10
 -4.193958	 -4.230214	diff=0.0362556
 -1.097525	 -1.122975	diff=0.0254502
  0.000000	  0.000000	diff=7.06554e-10
 -0.000000	 -0.000000	diff=1.3833e-10
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  local_diff=0.0617058
# W_pos, [2 2]
  0.092907	  0.046638	diff=0.0462685
  0.061063	  0.030576	diff=0.0304864
  0.068308	  0.032549	diff=0.0357592
  0.061723	  0.029721	diff=0.032002
  local_diff=0.144516
# v_pos, [1 2]
 -0.586990	 -0.281622	diff=0.305368
 -2.038008	 -0.970324	diff=1.06768
  local_diff=1.37305
# W_h, [2 4]
 -1.511691	 -1.525228	diff=0.0135377
  0.677159	  0.697571	diff=0.0204125
  1.644364	  1.689296	diff=0.0449322
 -0.801119	 -0.822215	diff=0.021096
  1.561771	  1.562230	diff=0.000459142
 -0.072421	 -0.029684	diff=0.0427376
 -0.976677	 -0.949808	diff=0.0268695
  0.102608	  0.080962	diff=0.0216461
  local_diff=0.191691
# W_soft, [4 2]
  5.510857	  5.507985	diff=0.00287231
 -1.792821	 -1.801720	diff=0.00889952
 -0.958318	 -0.961724	diff=0.00340552
 -2.735817	 -2.744540	diff=0.00872315
 -1.500612	 -1.504354	diff=0.00374206
 -0.163399	 -0.172924	diff=0.00952439
  4.606341	  4.603320	diff=0.00302133
 -2.916310	 -2.926043	diff=0.00973229
  local_diff=0.0499206
# Num params=182, abs_diff=220.612
Elapsed time is 2.862221 seconds.
[?1l>
## trainLSTM('', '', '', '', '', '', '', '../output/gradcheck', 'isGradCheck', 1, 'initRange', 10, 'isResume', 0, 'feedInput', 1, 'numLayers', 2, 'dropout', 0.8, 'isReverse', 1, 'attnFunc', 1, 'attnOpt', 2)
[?1h=
                                                                   < M A T L A B (R) >
                                                         Copyright 1984-2013 The MathWorks, Inc.
                                                            R2013a (8.1.0.604) 64-bit (maci64)
                                                                    February 15, 2013

 
To get started, type one of these: helpwin, helpdesk, or demo.
For product information, visit www.mathworks.com.
 

  Student License -- for use in conjunction with courses offered at a
  degree-granting institution.  Professional and commercial use prohibited.

## Bilingual setting
# Init LSTM parameters using dataType=double, initRange=10
  Model size = 180, individual sizes:  W_src{1}=32 W_src{2}=32 W_tgt{1}=48 W_tgt{2}=32 W_emb_src=8 W_emb_tgt=8 W_a=4 W_h=8 W_soft=8
# assert = 0
# attnFunc = 1
# attnOpt = 2
# batchSize = 10
# dataType = double
# debug = 0
# decode = 1
# dropout = 0.8
# epochFraction = 1
# feedInput = 1
# finetuneEpoch = 5
# finetuneRate = 0.5
# gpuDevice = 0
# initRange = 10
# isBi = 1
# isClip = 1
# isGradCheck = 1
# isProfile = 0
# isResume = 0
# isReverse = 1
# learningRate = 1
# loadModel = 
# logFreq = 10
# lstmOpt = 0
# lstmSize = 2
# maxGradNorm = 5
# maxLenRatio = 1.5
# maxSentLen = 7
# minLenRatio = 0.5
# numEpoches = 10
# numLayers = 2
# onlyCPU = 0
# outDir = ../output/gradcheck
# posWin = 1
# seed = 0
# shuffle = 1
# sortBatch = 1
# srcLang = 
# srcVocabFile = 
# testPrefix = 
# tgtLang = 
# tgtVocabFile = 
# trainPrefix = 
# validPrefix = 
# chunkSize = 12800
# baseIndex = 0
# clipForward = 50
# clipBackward = 1000
# nonlinear_gate_f = sigmoid
# nonlinear_gate_f_prime = sigmoidPrime
# nonlinear_f = tanh
# nonlinear_f_prime = tanhPrime
# beamSize = 12
# stackSize = 100
# unkPenalty = 0
# lengthReward = 0
# isGPU = 0
# batchId = 1
# maxRelDist = 2
# logId = 3
# srcSos = 3
# srcEos = 4
# srcVocabSize = 4
# nullPosId = 0
# tgtSos = 3
# tgtEos = 4
# tgtVocabSize = 4
# modelFile = ../output/gradcheck/model.mat
# modelRecentFile = ../output/gradcheck/modelRecent.mat
# softmaxSize = 2
# lr = 1
# epoch = 1
# bestCostValid = 100000
# testPerplexity = 100000
# curTestPerpWord = 100000
# startIter = 0
# iter = 0
# epochBatchCount = 0
# finetuneCount = 0
# modelSize = 180
  src input 1: <s_sos> <s_sos> x y
  src mask: 0  0  1  1  1
  tgt input 1: <t_sos> a <t_eos> <t_eos> <t_eos>
  tgt output 1: a <t_eos> <t_eos> <t_eos> <t_eos>
  tgt mask: 1  1  0  0  0
# W_src{1}, [8 4]
  0.000272	  0.000283	diff=1.06418e-05
  0.000000	 -0.000000	diff=4.13753e-17
 -0.000004	 -0.000004	diff=1.15064e-08
  0.000000	  0.000000	diff=2.64595e-22
 28.883205	 31.170124	diff=2.28692
 -0.000000	 -0.000000	diff=1.66994e-11
  0.000000	 -0.000000	diff=5.34679e-16
  0.000000	  0.000000	diff=3.34628e-17
 -0.000093	 -0.000094	diff=1.28591e-06
 -0.003277	 -0.003266	diff=1.12642e-05
  0.000001	  0.000001	diff=2.52524e-08
  0.001515	  0.001490	diff=2.54309e-05
-10.288089	-10.353477	diff=0.0653885
-25.621868	-28.296116	diff=2.67425
  0.000000	  0.000000	diff=2.00596e-14
 -0.000000	 -0.000000	diff=3.77164e-12
  0.000035	  0.000036	diff=7.04696e-07
  0.000241	  0.000235	diff=6.35073e-06
 -0.000001	 -0.000001	diff=8.44987e-09
 -0.000950	 -0.000925	diff=2.49242e-05
  0.972150	  1.012143	diff=0.0399925
  0.003800	  0.003822	diff=2.28853e-05
  0.000000	 -0.000000	diff=7.97012e-16
  0.000000	  0.000000	diff=4.45621e-19
  0.000000	 -0.000000	diff=5.17242e-19
 -0.000037	 -0.000036	diff=8.22275e-07
  0.000000	  0.000000	diff=4.60084e-20
  0.000147	  0.000144	diff=3.23853e-06
  0.000000	 -0.000000	diff=1.33935e-14
 -0.000528	 -0.000531	diff=2.9432e-06
  0.000000	  0.000000	diff=1.16205e-29
  0.000000	  0.000000	diff=4.7654e-16
  local_diff=5.06666
# W_src{2}, [8 4]
  0.079207	  0.075301	diff=0.00390604
 -0.537364	 -0.546549	diff=0.00918486
  0.002018	  0.002535	diff=0.000516815
 -0.000332	 -0.000320	diff=1.20259e-05
  0.135552	  0.075858	diff=0.0596938
 -1.262863	 -1.351526	diff=0.0886623
 -0.069363	 -0.073418	diff=0.00405515
 -0.444869	 -0.451605	diff=0.00673593
 -0.017928	 -0.017438	diff=0.000489695
  0.371694	  0.358001	diff=0.0136933
 -0.000011	 -0.000012	diff=7.06056e-07
 -0.000163	 -0.000166	diff=3.13328e-06
 -0.006630	 -0.006473	diff=0.00015742
  0.333140	  0.321525	diff=0.011615
 -0.241129	 -0.235173	diff=0.00595614
 -1.821799	 -1.772699	diff=0.0490998
 -0.019479	 -0.019081	diff=0.000398053
 -0.075768	 -0.074081	diff=0.00168608
  0.006178	  0.006746	diff=0.000567262
 -0.037166	 -0.036363	diff=0.000802733
  0.010695	  0.012392	diff=0.00169684
 -0.155805	 -0.150888	diff=0.00491721
  0.030905	  0.023759	diff=0.00714658
  0.351456	  0.339250	diff=0.0122054
  0.041350	  0.039952	diff=0.00139826
  0.085687	  0.074742	diff=0.0109456
 -0.047954	 -0.056694	diff=0.00873925
  0.027855	  0.022887	diff=0.00496734
 -0.018211	 -0.023230	diff=0.00501875
  0.319890	  0.304269	diff=0.0156208
  0.202231	  0.289145	diff=0.0869142
 -0.503485	 -0.471570	diff=0.0319148
  local_diff=0.448721
# W_tgt{1}, [8 6]
  0.000000	  0.000000	diff=1.05102e-17
  0.004810	  0.004887	diff=7.70082e-05
 -0.000000	 -0.000000	diff=1.82492e-10
 -0.000000	 -0.000000	diff=5.95375e-09
  0.000000	 -0.000000	diff=8.56431e-15
 -0.000000	 -0.000000	diff=9.35597e-10
  0.000000	  0.000000	diff=2.15652e-10
 -0.000000	  0.000000	diff=3.19931e-12
  0.000000	 -0.000000	diff=3.65785e-31
 -0.004739	 -0.004667	diff=7.16789e-05
  0.000000	  0.000000	diff=4.08479e-10
  0.000000	  0.000000	diff=4.10837e-19
  0.000000	  0.000000	diff=8.18031e-15
  0.000000	 -0.000000	diff=1.86702e-14
 -0.000000	 -0.000000	diff=1.88924e-10
  0.000000	 -0.000000	diff=3.41122e-13
  0.000000	  0.000000	diff=3.16166e-18
 -0.000332	 -0.000332	diff=3.74864e-07
 -0.000000	 -0.000000	diff=4.76543e-13
 -0.000000	 -0.000000	diff=5.43512e-10
  0.000000	  0.000000	diff=2.81265e-15
  0.000000	  0.000000	diff=4.29332e-11
  0.000000	  0.000000	diff=3.73196e-11
  0.000000	 -0.000000	diff=1.10996e-13
  0.000000	  0.000000	diff=3.77874e-18
 -0.001712	 -0.001702	diff=9.81328e-06
 -0.000000	 -0.000000	diff=6.67506e-13
 -0.000000	 -0.000000	diff=7.78981e-10
  0.000000	  0.000000	diff=1.85391e-15
  0.000000	  0.000000	diff=1.82238e-11
 -0.000000	 -0.000000	diff=5.91326e-11
  0.000000	 -0.000000	diff=9.81633e-14
  0.000000	  0.000000	diff=6.42331e-28
  0.000000	 -0.000000	diff=5.81779e-24
 -0.000000	 -0.000000	diff=8.25708e-13
  0.000000	  0.000000	diff=2.56145e-22
  0.000000	 -0.000000	diff=1.21953e-34
  0.000000	  0.000000	diff=2.89195e-15
  0.000000	 -0.000000	diff=1.63515e-37
  0.000000	 -0.000000	diff=3.22361e-22
  0.000000	  0.000000	diff=3.73894e-29
  0.000000	  0.000000	diff=9.06386e-21
  0.000000	 -0.000000	diff=8.91204e-19
  0.000000	 -0.000000	diff=1.19667e-18
  0.000000	  0.000000	diff=1.47199e-38
  0.000000	  0.000000	diff=1.10112e-20
  0.000000	  0.000000	diff=1.02341e-29
  0.000000	  0.000000	diff=8.66732e-30
  local_diff=0.000158885
# W_tgt{2}, [8 4]
  0.357365	  0.357746	diff=0.000381023
  2.645935	  2.660832	diff=0.0148978
 -0.000470	 -0.000467	diff=2.22554e-06
  0.029441	  0.029327	diff=0.000114047
  0.001377	  0.001371	diff=6.1002e-06
  0.007711	  0.007754	diff=4.28322e-05
  0.773355	  0.769464	diff=0.0038911
 10.493143	 10.782561	diff=0.289418
 -0.000092	 -0.000092	diff=1.17656e-09
 -0.000092	 -0.000092	diff=4.45994e-09
 -0.000001	 -0.000001	diff=1.7197e-11
  0.000005	  0.000005	diff=5.76231e-11
  0.000006	  0.000006	diff=7.86181e-11
 -0.000023	 -0.000023	diff=2.64127e-10
  0.002226	  0.002226	diff=3.26405e-08
 -0.000043	 -0.000043	diff=1.37761e-10
  0.225014	  0.225718	diff=0.000704596
  0.404226	  0.404261	diff=3.49193e-05
  0.052094	  0.052269	diff=0.000175166
 -0.757498	 -0.756140	diff=0.00135807
 -0.378937	 -0.378706	diff=0.000231338
 -0.046685	 -0.046603	diff=8.20642e-05
 -0.393799	 -0.393911	diff=0.000112124
 -2.696061	 -2.668222	diff=0.0278383
 -0.326875	 -0.325678	diff=0.00119673
 -0.421587	 -0.421928	diff=0.000340423
  0.054916	  0.055184	diff=0.000268396
  1.536283	  1.542757	diff=0.00647399
 -2.036506	 -2.038147	diff=0.00164156
  0.392653	  0.393108	diff=0.000454797
 -0.364980	 -0.364854	diff=0.00012549
  3.448578	  3.505414	diff=0.0568365
  local_diff=0.406628
# W_emb_src, [2 4]
 -0.000004	 -0.000005	diff=1.0423e-06
  1.866035	  1.785806	diff=0.0802295
 26.030273	 27.924589	diff=1.89432
 77.773492	 96.464637	diff=18.6911
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  local_diff=20.6657
# W_emb_tgt, [2 4]
  0.000000	  0.000000	diff=1.44674e-09
 -0.000000	 -0.000000	diff=2.43583e-12
 -0.011563	 -0.011148	diff=0.000414572
 -0.002990	 -0.002961	diff=2.87533e-05
  0.000000	  0.000000	diff=1.20997e-11
 -0.000000	 -0.000000	diff=4.27696e-12
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  local_diff=0.000443327
# W_a, [2 2]
 -0.004721	 -0.004680	diff=4.07531e-05
 -1.254132	 -1.254877	diff=0.000745548
 -0.015406	 -0.015455	diff=4.85679e-05
  0.690386	  0.689181	diff=0.00120428
  local_diff=0.00203915
# W_h, [2 4]
 -1.408648	 -1.408411	diff=0.000237302
 -0.206574	 -0.206544	diff=3.01564e-05
  1.746481	  1.742977	diff=0.0035039
  3.143445	  3.145787	diff=0.00234195
  0.490966	  0.485882	diff=0.00508377
 -3.077260	 -3.074909	diff=0.00235113
  2.034908	  2.034710	diff=0.000198736
  1.239457	  1.238674	diff=0.000782835
  local_diff=0.0145298
# W_soft, [4 2]
 -3.344625	 -3.349193	diff=0.00456765
  9.821286	  9.820724	diff=0.00056256
 -0.268849	 -0.269902	diff=0.00105339
 -6.196670	 -6.201628	diff=0.00495854
  0.167489	  0.164506	diff=0.0029831
  9.543944	  9.543133	diff=0.000811295
 -2.059847	 -2.063128	diff=0.00328104
 -7.639216	 -7.644510	diff=0.0052939
  local_diff=0.0235115
# Num params=180, abs_diff=26.6284
Elapsed time is 2.075561 seconds.
[?1l>
## trainLSTM('', '', '', '', '', '', '', '../output/gradcheck', 'isGradCheck', 1, 'initRange', 10, 'isResume', 0, 'feedInput', 1, 'numLayers', 2, 'dropout', 0.8, 'isReverse', 1, 'attnFunc', 1, 'attnOpt', 3)
[?1h=
                                                                   < M A T L A B (R) >
                                                         Copyright 1984-2013 The MathWorks, Inc.
                                                            R2013a (8.1.0.604) 64-bit (maci64)
                                                                    February 15, 2013

 
To get started, type one of these: helpwin, helpdesk, or demo.
For product information, visit www.mathworks.com.
 

  Student License -- for use in conjunction with courses offered at a
  degree-granting institution.  Professional and commercial use prohibited.

## Bilingual setting
# Init LSTM parameters using dataType=double, initRange=10
  Model size = 182, individual sizes:  W_src{1}=32 W_src{2}=32 W_tgt{1}=48 W_tgt{2}=32 W_emb_src=8 W_emb_tgt=8 W_a=4 v_a=2 W_h=8 W_soft=8
# assert = 0
# attnFunc = 1
# attnOpt = 3
# batchSize = 10
# dataType = double
# debug = 0
# decode = 1
# dropout = 0.8
# epochFraction = 1
# feedInput = 1
# finetuneEpoch = 5
# finetuneRate = 0.5
# gpuDevice = 0
# initRange = 10
# isBi = 1
# isClip = 1
# isGradCheck = 1
# isProfile = 0
# isResume = 0
# isReverse = 1
# learningRate = 1
# loadModel = 
# logFreq = 10
# lstmOpt = 0
# lstmSize = 2
# maxGradNorm = 5
# maxLenRatio = 1.5
# maxSentLen = 7
# minLenRatio = 0.5
# numEpoches = 10
# numLayers = 2
# onlyCPU = 0
# outDir = ../output/gradcheck
# posWin = 1
# seed = 0
# shuffle = 1
# sortBatch = 1
# srcLang = 
# srcVocabFile = 
# testPrefix = 
# tgtLang = 
# tgtVocabFile = 
# trainPrefix = 
# validPrefix = 
# chunkSize = 12800
# baseIndex = 0
# clipForward = 50
# clipBackward = 1000
# nonlinear_gate_f = sigmoid
# nonlinear_gate_f_prime = sigmoidPrime
# nonlinear_f = tanh
# nonlinear_f_prime = tanhPrime
# beamSize = 12
# stackSize = 100
# unkPenalty = 0
# lengthReward = 0
# isGPU = 0
# batchId = 1
# maxRelDist = 2
# logId = 3
# srcSos = 3
# srcEos = 4
# srcVocabSize = 4
# nullPosId = 0
# tgtSos = 3
# tgtEos = 4
# tgtVocabSize = 4
# modelFile = ../output/gradcheck/model.mat
# modelRecentFile = ../output/gradcheck/modelRecent.mat
# softmaxSize = 2
# lr = 1
# epoch = 1
# bestCostValid = 100000
# testPerplexity = 100000
# curTestPerpWord = 100000
# startIter = 0
# iter = 0
# epochBatchCount = 0
# finetuneCount = 0
# modelSize = 182
  src input 1: x x x x
  src mask: 1  1  1  1  1
  tgt input 1: <t_sos> a b <t_eos> <t_eos>
  tgt output 1: a b <t_eos> <t_eos> <t_eos>
  tgt mask: 1  1  1  0  0
# W_src{1}, [8 4]
 -0.000101	 -0.000108	diff=6.92588e-06
  0.000000	  0.000000	diff=7.17333e-09
  0.000003	  0.000003	diff=5.93543e-08
  0.000000	  0.000000	diff=1.36153e-20
-95.294114	-96.077788	diff=0.783674
 -0.000000	 -0.000000	diff=3.41495e-11
  0.000000	  0.000000	diff=6.33883e-14
  0.000000	  0.000000	diff=1.46622e-10
  0.000034	  0.000036	diff=1.46932e-06
 -0.000321	 -0.000323	diff=2.06663e-06
 -0.000001	 -0.000001	diff=6.44337e-09
 -0.001221	 -0.001228	diff=7.30285e-06
 31.375155	 31.913847	diff=0.538692
 33.488917	 29.358378	diff=4.13054
  0.000000	  0.000000	diff=9.20387e-14
 -0.000000	 -0.000000	diff=1.63102e-11
 -0.000027	 -0.000027	diff=1.30644e-07
 -0.000194	 -0.000194	diff=7.34206e-07
  0.000001	  0.000001	diff=2.49983e-09
  0.000765	  0.000762	diff=2.83163e-06
  1.958730	  1.963665	diff=0.00493464
 -0.002177	 -0.002168	diff=8.29882e-06
  0.000000	 -0.000000	diff=1.82629e-15
  0.000000	  0.000000	diff=2.4156e-13
  0.000000	  0.000000	diff=7.19186e-19
  0.000030	  0.000030	diff=1.76338e-08
  0.000000	 -0.000000	diff=1.49761e-20
 -0.000118	 -0.000118	diff=6.80153e-08
  0.000000	  0.000000	diff=1.85713e-14
  0.000336	  0.000337	diff=1.99177e-07
  0.000000	  0.000000	diff=2.30299e-27
  0.000000	  0.000000	diff=1.07028e-16
  local_diff=5.45787
# W_src{2}, [8 4]
 -0.750064	 -0.859684	diff=0.109619
  1.016640	  1.023761	diff=0.00712077
  0.075885	  0.075826	diff=5.91397e-05
  0.235593	  0.236606	diff=0.00101255
 -3.276354	 -3.578702	diff=0.302347
  1.988893	  2.005917	diff=0.0170246
 -0.265070	 -0.213489	diff=0.0515805
  0.858973	  0.863819	diff=0.00484645
  0.004881	  0.004877	diff=3.79334e-06
 -0.351459	 -0.352080	diff=0.000621256
  0.000028	  0.000028	diff=6.98956e-12
  0.000088	  0.000088	diff=9.31676e-11
 -0.012155	 -0.012153	diff=1.77177e-06
 -0.327165	 -0.327739	diff=0.000574518
  0.068224	  0.067598	diff=0.000626236
  1.759693	  1.744379	diff=0.0153139
  0.296520	  0.296491	diff=2.87846e-05
  0.047703	  0.047696	diff=7.37516e-06
  0.066793	  0.066779	diff=1.42397e-05
 -0.016511	 -0.016473	diff=3.83588e-05
  0.068107	  0.068101	diff=6.35819e-06
  0.218336	  0.218257	diff=7.96825e-05
 -0.841687	 -0.841562	diff=0.000124658
 -0.280751	 -0.281082	diff=0.000330713
 -0.762188	 -0.761568	diff=0.000619991
 -0.062484	 -0.062612	diff=0.000127252
 -1.120573	 -1.123134	diff=0.0025607
  0.056595	  0.056774	diff=0.000179826
 -0.036050	 -0.036086	diff=3.62278e-05
 -0.357805	 -0.358203	diff=0.000397729
  9.735552	  9.767388	diff=0.0318364
  0.540352	  0.539142	diff=0.0012097
  local_diff=0.54835
# W_tgt{1}, [8 6]
  0.000000	 -0.000000	diff=2.49433e-15
 -0.054318	 -0.052178	diff=0.00213988
 -0.000008	 -0.000008	diff=1.36895e-07
  0.000000	 -0.000000	diff=3.49922e-18
  0.000000	  0.000000	diff=8.251e-46
  0.000000	  0.000000	diff=1.17792e-12
 -0.000000	 -0.000000	diff=3.78204e-13
  0.000000	  0.000000	diff=1.36122e-18
  0.000000	  0.000000	diff=2.38249e-15
  0.047969	  0.049838	diff=0.00186927
  0.000008	  0.000008	diff=1.35477e-07
  0.000000	  0.000000	diff=3.34232e-18
  0.000000	  0.000000	diff=6.30218e-35
  0.000000	  0.000000	diff=2.04959e-10
  0.000000	  0.000000	diff=1.16213e-12
  0.000000	  0.000000	diff=7.61318e-19
 -0.015872	 -0.015973	diff=0.000100655
  0.670630	  0.666410	diff=0.00422067
  1.474112	  1.489339	diff=0.015227
  0.000705	  0.000703	diff=2.06122e-06
  0.109918	  0.110426	diff=0.000508391
 -3.455075	 -3.419008	diff=0.0360662
  0.005593	  0.005643	diff=5.07339e-05
 -0.000002	 -0.000002	diff=2.56249e-08
 -0.012120	 -0.012175	diff=5.43276e-05
  0.539777	  0.537648	diff=0.00212931
 -1.037774	 -1.030444	diff=0.00733064
 -0.001015	 -0.001016	diff=7.41815e-07
 -0.073832	 -0.073605	diff=0.000227084
 -2.360754	 -2.345359	diff=0.0153953
  0.004215	  0.004244	diff=2.87127e-05
 -0.000001	 -0.000001	diff=1.31661e-08
 -0.669408	 -0.666941	diff=0.00246734
  0.946555	  0.946398	diff=0.00015711
 -2.165799	 -2.135614	diff=0.0301851
 -0.777650	 -0.776989	diff=0.000661074
  0.406437	  0.406389	diff=4.78595e-05
 -2.591235	 -2.564184	diff=0.027051
  0.291241	  0.292128	diff=0.000886923
 -0.447576	 -0.447949	diff=0.000373562
  0.004887	  0.004871	diff=1.61823e-05
  0.136304	  0.136028	diff=0.000276452
  0.157114	  0.157330	diff=0.000216315
  0.000353	  0.000354	diff=8.61857e-07
  0.010449	  0.010454	diff=4.56886e-06
  3.628626	  3.691649	diff=0.0630231
 -0.000221	 -0.000221	diff=7.80667e-08
  0.000001	  0.000001	diff=8.41922e-09
  local_diff=0.210719
# W_tgt{2}, [8 4]
  8.494684	  8.985383	diff=0.4907
  6.179603	  6.192508	diff=0.012905
 -0.112556	 -0.112242	diff=0.000314339
  0.090456	  0.089872	diff=0.000584056
  0.000794	  0.000647	diff=0.000146304
 -0.655878	 -0.655541	diff=0.000336663
 -0.681690	 -0.679139	diff=0.00255051
 24.154853	 24.748232	diff=0.593379
 -0.989626	 -0.983875	diff=0.00575124
  0.067416	  0.067418	diff=1.57681e-06
  0.006879	  0.006881	diff=1.54958e-06
 -0.010318	 -0.010303	diff=1.48064e-05
 -0.005202	 -0.005218	diff=1.59567e-05
 -0.028526	 -0.028164	diff=0.000361592
 -0.006357	 -0.006359	diff=2.4435e-06
 -0.094758	 -0.094664	diff=9.39138e-05
  4.524625	  4.677943	diff=0.153318
 -0.569370	 -0.569236	diff=0.000134624
 -0.687226	 -0.688073	diff=0.000846681
  0.824548	  0.826077	diff=0.00152902
 -1.307933	 -1.307910	diff=2.32437e-05
 -0.156638	 -0.156810	diff=0.000172099
 -0.966378	 -0.964446	diff=0.00193212
 10.130082	 10.159178	diff=0.0290961
  1.917010	  1.916654	diff=0.00035584
  0.417504	  0.419682	diff=0.00217812
  0.801766	  0.800848	diff=0.000917739
 -1.320295	 -1.312750	diff=0.00754418
  0.943196	  0.941933	diff=0.00126286
  0.235513	  0.235293	diff=0.000220456
  0.535323	  0.535507	diff=0.000184103
 -6.823694	 -6.679595	diff=0.144099
  local_diff=1.45097
# W_emb_src, [2 4]
  0.000005	  0.000005	diff=8.25054e-08
 -1.836916	 -1.852927	diff=0.0160112
-85.365779	-86.072194	diff=0.706416
-297.657204	-297.277172	diff=0.380032
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  local_diff=1.10246
# W_emb_tgt, [2 4]
 -0.000000	 -0.000000	diff=1.00732e-11
 -0.000000	 -0.000000	diff=2.63642e-11
  0.108673	  0.119046	diff=0.010373
  0.030867	  0.031625	diff=0.000758197
  0.000000	  0.000000	diff=6.37593e-10
 -0.000000	 -0.000000	diff=3.39385e-10
  0.000000	  0.000000	diff=0
  0.000000	  0.000000	diff=0
  local_diff=0.0111312
# W_a, [2 2]
  0.022171	  0.022642	diff=0.000470579
 -0.197344	 -0.196958	diff=0.000386375
 -0.008175	 -0.008180	diff=5.41642e-06
 -0.042423	 -0.042473	diff=4.97812e-05
  local_diff=0.000912152
# v_a, [1 2]
  0.910934	  0.913519	diff=0.00258447
 -2.714058	 -2.713844	diff=0.000214556
  local_diff=0.00279903
# W_h, [2 4]
  0.130139	  0.130787	diff=0.000648153
 -0.832062	 -0.831459	diff=0.000603363
 -0.296298	 -0.295301	diff=0.000996755
  2.285211	  2.286888	diff=0.00167692
  1.576224	  1.580116	diff=0.00389136
 -6.814995	 -6.735038	diff=0.0799569
 -0.828555	 -0.825923	diff=0.00263141
  1.464849	  1.465929	diff=0.00107953
  local_diff=0.0914844
# W_soft, [4 2]
  7.175324	  7.172881	diff=0.00244324
 -1.759878	 -1.770534	diff=0.0106561
 -0.736914	 -0.739895	diff=0.00298072
 -4.652072	 -4.662452	diff=0.0103797
 -0.879300	 -0.882546	diff=0.00324528
 -0.573407	 -0.584769	diff=0.0113625
  3.509876	  3.507175	diff=0.00270026
 -2.028308	 -2.039860	diff=0.0115521
  local_diff=0.0553199
# Num params=182, abs_diff=8.93202
Elapsed time is 2.114490 seconds.
[?1l>